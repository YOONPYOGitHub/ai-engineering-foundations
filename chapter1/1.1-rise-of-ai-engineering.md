# 1.1 The Rise of AI Engineering

> **"Scaleì´ ëª¨ë“  ê²ƒì„ ë°”ê¿¨ë‹¤"**  
> AI post-2020ì„ í•œ ë‹¨ì–´ë¡œ: Scale

[â† ëª©ì°¨ë¡œ ëŒì•„ê°€ê¸°](./README.md)

---

## ğŸ“‹ ëª©ì°¨

- [Language Modelsì˜ ì—­ì‚¬](#language-modelsì˜-ì—­ì‚¬)
- [Self-Supervision: ê²Œì„ ì²´ì¸ì €](#self-supervision-ê²Œì„-ì²´ì¸ì €)
- [Transformerì˜ í˜ëª…](#transformerì˜-í˜ëª…)
- [Scaling Lawsì™€ Emergent Abilities](#scaling-lawsì™€-emergent-abilities)
- [Multimodalë¡œì˜ í™•ì¥](#multimodalë¡œì˜-í™•ì¥)
- [Foundation Modelsì˜ ì •ì˜](#foundation-modelsì˜-ì •ì˜)
- [AI Engineeringì˜ íƒ„ìƒ](#ai-engineeringì˜-íƒ„ìƒ)
- [í•µì‹¬ ìš”ì•½](#í•µì‹¬-ìš”ì•½)

---

## Language Modelsì˜ ì—­ì‚¬

### ê¸°ë³¸ ê°œë…

**Language Modelì´ë€?**
> í•˜ë‚˜ ì´ìƒì˜ ì–¸ì–´ì— ëŒ€í•œ í†µê³„ì  ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ëª¨ë¸

ì§ê´€ì ìœ¼ë¡œ, ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.

**ì˜ˆì‹œ:**
```
ë¬¸ë§¥: "My favorite color is __"
Language Model ì˜ˆì¸¡:
- "blue" (ë†’ì€ í™•ë¥ ) âœ“
- "car" (ë‚®ì€ í™•ë¥ ) âœ—
```

### ì—­ì‚¬ì  ë°°ê²½

**1905ë…„: Sherlock Holmes**
- "The Adventure of the Dancing Men" ì´ì•¼ê¸°
- ì˜ì–´ì—ì„œ ê°€ì¥ í”í•œ ê¸€ìëŠ” E
- ê°€ì¥ í”í•œ ë§‰ëŒ€ê¸° ê·¸ë¦¼ = E
- ë‹¨ìˆœí•œ í†µê³„ì  ì •ë³´ í™œìš©

**1951ë…„: Claude Shannon**
- 2ì°¨ ì„¸ê³„ëŒ€ì „ ì¤‘ ì ì˜ ë©”ì‹œì§€ í•´ë…
- "Prediction and Entropy of Printed English" ë…¼ë¬¸
- Entropy ê°œë… ë„ì… (í˜„ì¬ë„ ì‚¬ìš©ë¨)

### Token: Language Modelì˜ ê¸°ë³¸ ë‹¨ìœ„

**Tokenì´ë€?**
- ë¬¸ì(character), ë‹¨ì–´(word), ë˜ëŠ” ë‹¨ì–´ì˜ ì¼ë¶€
- ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„

**GPT-4 ì˜ˆì‹œ:**
```
"I can't wait to build AI applications"
â†“
["I", " can", "'t", " wait", " to", " build", " AI", " applications"]
9 tokens

ì°¸ê³ : "can't" â†’ ["can", "'t"] (2 tokens)
```

**Tokenization**
- ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •
- GPT-4: í‰ê·  1 token â‰ˆ 0.75 words
- 100 tokens â‰ˆ 75 words

**Vocabulary**
- ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“  í† í°ì˜ ì§‘í•©
- Mixtral 8x7B: 32,000 tokens
- GPT-4: 100,256 tokens

**ì™œ Tokenì„ ì‚¬ìš©í•˜ëŠ”ê°€?**

1. **ì˜ë¯¸ìˆëŠ” êµ¬ì„± ìš”ì†Œ**
   ```
   "cooking" â†’ ["cook", "ing"]
   ë‘˜ ë‹¤ ì˜ë¯¸ ë³´ì¡´
   ```

2. **íš¨ìœ¨ì„±**
   ```
   Character < Token < Word
   
   Tokenì´ ìµœì :
   - Wordë³´ë‹¤ ì ì€ vocabulary
   - Characterë³´ë‹¤ ë§ì€ ì˜ë¯¸
   ```

3. **ë¯¸ì§€ì˜ ë‹¨ì–´ ì²˜ë¦¬**
   ```
   "chatgpting" â†’ ["chatgpt", "ing"]
   êµ¬ì¡° ì´í•´ ê°€ëŠ¥
   ```

### Language Modelì˜ ë‘ ê°€ì§€ íƒ€ì…

**1. Masked Language Model**

íŠ¹ì§•:
- ë¬¸ì¥ ì–´ë””ì„œë“  ë¹ ì§„ í† í° ì˜ˆì¸¡
- ì•ë’¤ ë¬¸ë§¥ ëª¨ë‘ ì‚¬ìš© (bidirectional)
- Fill in the blank íƒœìŠ¤í¬

ì˜ˆì‹œ:
```
"My favorite __ is blue"
â†’ "color" ì˜ˆì¸¡
```

ëŒ€í‘œ ëª¨ë¸:
- **BERT** (Bidirectional Encoder Representations from Transformers)

ì‚¬ìš© ì‚¬ë¡€:
- ê°ì„± ë¶„ì„
- í…ìŠ¤íŠ¸ ë¶„ë¥˜
- ì½”ë“œ ë””ë²„ê¹… (ì•ë’¤ ì½”ë“œ ì´í•´ í•„ìš”)

**2. Autoregressive Language Model**

íŠ¹ì§•:
- ë‹¤ìŒ í† í° ì˜ˆì¸¡
- ì´ì „ í† í°ë§Œ ì‚¬ìš©
- ê³„ì†í•´ì„œ í† í° ìƒì„± ê°€ëŠ¥

ì˜ˆì‹œ:
```
"My favorite color is __"
â†’ "blue" ì˜ˆì¸¡
â†’ ê³„ì† ìƒì„±: "blue and I"
```

í˜„ì¬:
- **í…ìŠ¤íŠ¸ ìƒì„±ì˜ í‘œì¤€**
- ì´ ì±…ì—ì„œ "language model" = autoregressive model

ë¹„êµ:
```
Masked LM:  [context] __ [context] â†’ token
Autoregressive: [previous tokens] â†’ next token
```

**Generative AI**

Language modelì˜ ì¶œë ¥:
- ê³ ì •ëœ vocabulary
- ë¬´í•œí•œ ê°€ëŠ¥í•œ ì¶œë ¥
- **Open-ended**: ì—´ë¦° ê²°ê³¼

â†’ ì´ê²ƒì´ "Generative AI"

### Completion Machine

**í•µì‹¬ ê°œë…:**
> Language Model = Completion Machine  
> ì£¼ì–´ì§„ í…ìŠ¤íŠ¸(prompt)ë¥¼ ì™„ì„±í•˜ë ¤ê³  ì‹œë„

**ì˜ˆì‹œ:**
```
Prompt: "To be or not to be"
Completion: ", that is the question."
```

**ì¤‘ìš”:**
- Completion = Prediction (í™•ë¥  ê¸°ë°˜)
- ì •ë‹µ ë³´ì¥ âœ—
- Probabilistic nature â†’ í¥ë¯¸ë¡­ê³ ë„ ì¢Œì ˆìŠ¤ëŸ¬ì›€

**Completionì˜ í˜**

ë§ì€ íƒœìŠ¤í¬ë¥¼ completionìœ¼ë¡œ í”„ë ˆì´ë° ê°€ëŠ¥:
- ë²ˆì—­
- ìš”ì•½
- ì½”ë”©
- ìˆ˜í•™ ë¬¸ì œ í’€ì´

**ë²ˆì—­ ì˜ˆì‹œ:**
```
Prompt: "How are you in French is â€¦"
Completion: "Comment Ã§a va"
```

**ë¶„ë¥˜ ì˜ˆì‹œ:**
```
Prompt: 
"Question: Is this email likely spam? 
Here's the email: <email content>
Answer:"

Completion: "Likely spam"
â†’ Spam classifier!
```

**í•œê³„:**
- Completion â‰  Conversation
- ì§ˆë¬¸ì— ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ë‹µí•  ìˆ˜ë„ ìˆìŒ
- Post-training í•„ìš” (Chapter 2ì—ì„œ ë‹¤ë£¸)

---

## Self-Supervision: ê²Œì„ ì²´ì¸ì €

### ê¸°ì¡´ ë°©ì‹: Supervised Learning

**í”„ë¡œì„¸ìŠ¤:**
```
1. ë°ì´í„° ìˆ˜ì§‘
2. ì‚¬ëŒì´ ë ˆì´ë¸”ë§
3. ëª¨ë¸ í•™ìŠµ
4. ìƒˆ ë°ì´í„°ì— ì ìš©
```

**ì˜ˆì‹œ: Fraud Detection**
```
Input: Transaction
Label: "Fraud" or "Not Fraud"
â†’ ëª¨ë¸ í•™ìŠµ
â†’ ìƒˆ ê±°ë˜ ì˜ˆì¸¡
```

**2010ë…„ëŒ€ ì„±ê³µ: ImageNet + AlexNet**
- 1ë°±ë§Œ ì´ë¯¸ì§€
- 1,000 ì¹´í…Œê³ ë¦¬ ("car", "balloon", "monkey")
- Deep Learning í˜ëª…ì˜ ì‹œì‘

### Supervisionì˜ ë¬¸ì œì 

**1. ë¹„ìš©**
```
ì´ë¯¸ì§€ 1ê°œ ë ˆì´ë¸”: $0.05
1ë°±ë§Œ ì´ë¯¸ì§€: $50,000

2ëª…ì´ cross-check: $100,000
1ë°±ë§Œ ì¹´í…Œê³ ë¦¬ë¡œ í™•ì¥: $50,000,000!
```

**2. ë³µì¡ë„**
```
ì¼ìƒ ë¬¼ì²´ ë ˆì´ë¸”: ëˆ„êµ¬ë‚˜ ê°€ëŠ¥, ì €ë ´
Latin ë²ˆì—­: ë” ë¹„ìŒˆ
CT ìŠ¤ìº” ì•” ì§„ë‹¨: ì²œë¬¸í•™ì  ë¹„ìš©
```

**3. í™•ì¥ì„±**
```
ìƒˆ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ â†’ ìƒˆ ë ˆì´ë¸” í•„ìš”
ìƒˆ íƒœìŠ¤í¬ â†’ ì²˜ìŒë¶€í„° ë‹¤ì‹œ
```

### Self-Supervision: í˜ëª…

**í•µì‹¬ ì•„ì´ë””ì–´:**
> ë°ì´í„°ê°€ ìŠ¤ìŠ¤ë¡œ ë ˆì´ë¸” ì œê³µ

**Language Modeling ì˜ˆì‹œ:**

ë¬¸ì¥: "I love street food."

ìƒì„±ë˜ëŠ” í•™ìŠµ ìƒ˜í”Œ:
```
Input (context)              â†’ Output (next token)
<BOS>                        â†’ I
<BOS>, I                     â†’ love
<BOS>, I, love               â†’ street
<BOS>, I, love, street       â†’ food
<BOS>, I, love, street, food â†’ .
<BOS>, ..., food, .          â†’ <EOS>

6ê°œì˜ í•™ìŠµ ìƒ˜í”Œ!
ë ˆì´ë¸”ë§ ë¹„ìš©: $0
```

**Special Tokens:**
- `<BOS>`: Beginning of Sequence
- `<EOS>`: End of Sequence (ì–¸ì œ ë©ˆì¶œì§€ ì•Œë ¤ì¤Œ)

**ìˆ˜í•™ì  í‘œí˜„:**
```
Supervised:
P(label | input) â† ëª…ì‹œì  ë ˆì´ë¸” í•„ìš”

Self-supervised:
P(next_word | previous_words) â† ë°ì´í„°ê°€ ë ˆì´ë¸”
```

**Self-supervised vs Unsupervised:**
- Self-supervised: ë ˆì´ë¸”ì„ ë°ì´í„°ì—ì„œ ì¶”ë¡ 
- Unsupervised: ë ˆì´ë¸” ì „í˜€ ë¶ˆí•„ìš”

### ì™œ í˜ëª…ì ì¸ê°€?

**1. ë¬´í•œí•œ ë°ì´í„°**
```
ë ˆì´ë¸”ëœ ë°ì´í„°: ìˆ˜ë°±ë§Œ ~ ìˆ˜ì–µ ê°œ
ì¸í„°ë„· í…ìŠ¤íŠ¸: ìˆ˜ì¡° ê°œ í† í°

ê¸°ì¡´: "ë°ì´í„°ê°€ ë¶€ì¡±í•´ìš”" ğŸ˜¢
Self-supervision: "ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ìš”" ğŸ‰
```

**2. ë¹„ìš© ì ˆê°**
```
Supervised:
$1/ë ˆì´ë¸” Ã— 1M ë ˆì´ë¸” = $1,000,000

Self-supervised:
$0 Ã— âˆ ë°ì´í„° = $0
```

**3. ë²”ìš© í•™ìŠµ**
```
ê°ì„± ë¶„ì„ ë°ì´í„°:
â†’ ê°ì„± ë¶„ì„ë§Œ ê°€ëŠ¥

ì¸í„°ë„· ì „ì²´ í…ìŠ¤íŠ¸:
â†’ ëª¨ë“  ê²ƒ í•™ìŠµ ê°€ëŠ¥
```

**4. Transfer Learning**
```
Pre-training (self-supervised):
- ì¸í„°ë„· ì „ì²´ì—ì„œ í•™ìŠµ
- ë²”ìš© ëŠ¥ë ¥ íšë“

    â†“ (Transfer)

Finetuning (task-specific):
- ì ì€ ë°ì´í„°ë¡œ íŠ¹í™”
- ì „ë¬¸ ëŠ¥ë ¥ ê°œë°œ
```

**ê²°ê³¼:**
> Self-supervisionì´ Language Modelsë¥¼ Large Language Modelsë¡œ í™•ì¥ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦

---

## Transformerì˜ í˜ëª…

### 2017ë…„: "Attention Is All You Need"

**ë°œí‘œ:** Google

**í•µì‹¬ ê¸°ìˆ :** Attention Mechanism

**ì¥ì :**
- ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ (RNNê³¼ ë‹¬ë¦¬)
- ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ê°€ëŠ¥
- íš¨ìœ¨ì  í•™ìŠµ

**ì˜í–¥:**
```
2017: Transformer ë°œí‘œ
2018: BERT (Masked LM), GPT-1 (Autoregressive LM)
2019: GPT-2
2020: GPT-3
â†’ Large Language Models ì‹œëŒ€ ì‹œì‘
```

### ì´ì „ ëª¨ë¸ë“¤ì˜ í•œê³„

**Word2Vec (2013)**
- ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„
- ì˜ˆ: king - man + woman â‰ˆ queen
- í•˜ì§€ë§Œ ë¬¸ë§¥ ì´í•´ ì œí•œì 

**RNN/LSTM**
- ìˆœì°¨ì  ì •ë³´ ì²˜ë¦¬
- ê¸´ ì˜ì¡´ì„± í•™ìŠµ ê°€ëŠ¥
- í•˜ì§€ë§Œ ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆê°€, ì—¬ì „íˆ í•œê³„

**N-gram ëª¨ë¸ (1990s-2000s)**
- í†µê³„ì  ë°©ë²•
- ê·œì¹™ ê¸°ë°˜
- ê¸´ ë¬¸ë§¥ ë¶ˆê°€, ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ì–´ë ¤ì›€

---

## Scaling Lawsì™€ Emergent Abilities

### ëª¨ë¸ í¬ê¸°: Parameters

**Parameterë€?**
> í•™ìŠµ ê³¼ì •ì—ì„œ ì—…ë°ì´íŠ¸ë˜ëŠ” ëª¨ë¸ ë‚´ë¶€ ë³€ìˆ˜  
> ì¼ë°˜ì ìœ¼ë¡œ parameters â†‘ â†’ í•™ìŠµ ëŠ¥ë ¥ â†‘

**"Large"ì˜ ë³€í™”:**
```
2018ë…„ GPT-1:   117M parameters â†’ "large"
2019ë…„ GPT-2:   1.5B parameters â†’ 117Mì€ ì´ì œ "small"
2020ë…„ GPT-3:   175B parameters â†’ 1.5Bë„ "small"
2024ë…„ í˜„ì¬:    100B+ â†’ "large"
ë¯¸ë˜:           ???
```

**ì„±ì¥ ë¹„ìœ¨:**
```
GPT-1 â†’ GPT-2:  13ë°°
GPT-1 â†’ GPT-3:  1,500ë°°
GPT-1 â†’ GPT-4:  14,500ë°° (ì¶”ì •)
```

### Scaling Lawsì˜ ë°œê²¬

**3ê°€ì§€ Scale:**
1. **Model Size** (Parameters)
2. **Data Size** (Tokens)  
3. **Compute** (FLOPs)

**í•µì‹¬ ë°œê²¬:**
```
ë” í° ëª¨ë¸ + ë” ë§ì€ ë°ì´í„° + ë” ë§ì€ ê³„ì‚°
= ë” ë‚˜ì€ ì„±ëŠ¥ (ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê²Œ Predictable)
```

**AlexNet ë…¼ë¬¸ (2012)ì—ì„œ ì´ë¯¸:**
> "ìš°ë¦¬ì˜ ëª¨ë“  ì‹¤í—˜ì€ ë” ë¹ ë¥¸ GPUì™€ ë” í° ë°ì´í„°ì…‹ì´ ë‚˜ì˜¤ê¸°ë§Œì„ ê¸°ë‹¤ë¦¬ë©´ ê²°ê³¼ê°€ ê°œì„ ë  ê²ƒì„ì„ ì‹œì‚¬í•œë‹¤"

â†’ Ilya Sutskever (AlexNet ì €ì) â†’ OpenAI ê³µë™ ì°½ë¦½ â†’ GPTë¡œ í˜„ì‹¤í™”

**ì™œ í° ëª¨ë¸ì´ ë” ë§ì€ ë°ì´í„° í•„ìš”?**
```
í° ëª¨ë¸ = ë” í° í•™ìŠµ ëŠ¥ë ¥
â†’ ì„±ëŠ¥ ìµœëŒ€í™”í•˜ë ¤ë©´ ë” ë§ì€ ë°ì´í„° í•„ìš”

ì‘ì€ ë°ì´í„°ë¡œ í° ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥í•˜ì§€ë§Œ:
â†’ ë‚­ë¹„ (ì‘ì€ ëª¨ë¸ë¡œë„ ë¹„ìŠ·í•œ ì„±ëŠ¥)
```

### Emergent Abilities (ì°½ë°œì  ëŠ¥ë ¥)

**ì •ì˜:**
> ëª¨ë¸ì´ íŠ¹ì • í¬ê¸°ë¥¼ ë„˜ìœ¼ë©´ ê°‘ìê¸° ë‚˜íƒ€ë‚˜ëŠ” ìƒˆë¡œìš´ ëŠ¥ë ¥

**1. Few-Shot Learning**

Small Model:
```
Q: "Translate to French: Hello"
A: "Bonjour"
Q: "Translate to French: Goodbye"
A: [ì‹¤íŒ¨] âœ—
```

Large Model (GPT-3):
```
Q: "Translate to French: Hello"
A: "Bonjour"
Q: "Translate to French: Goodbye"  
A: "Au revoir" âœ“
Q: "Translate to French: How are you?"
A: "Comment allez-vous?" âœ“

â†’ ì˜ˆì‹œë§Œ ë³´ê³  íŒ¨í„´ í•™ìŠµ!
```

**2. Chain-of-Thought Reasoning**

Small Model:
```
Q: "What is 15% of 80?"
A: "12" (ë‹µë§Œ)
```

Large Model:
```
Q: "What is 15% of 80?"
A: "Let me think step by step:
    - 15% = 0.15
    - 0.15 Ã— 80 = 12
    Therefore, 15% of 80 is 12."
    
â†’ ì¶”ë¡  ê³¼ì •ê¹Œì§€ í‘œí˜„!
```

**3. Instruction Following**

Small Model:
```
Input: "Write a poem about AI"
Output: [ì„ì˜ì˜ í…ìŠ¤íŠ¸ ìƒì„±]
```

Large Model:
```
Input: "Write a poem about AI"
Output: [ì‹¤ì œë¡œ AIì— ëŒ€í•œ ì‹œ ì‘ì„±]

â†’ ì§€ì‹œì‚¬í•­ì„ ì´í•´í•˜ê³  ë”°ë¦„!
```

### Phase Transitions (ì„ê³„ì )

**í˜„ìƒ:**
```
ëª¨ë¸ í¬ê¸°: 0 â†’ 1B â†’ 10B â†’ 100B â†’ 1T

ì„±ëŠ¥:
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  â† ê°‘ìê¸° ì í”„!
                            â”‚
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 

íŠ¹ì • í¬ê¸°ë¥¼ ë„˜ìœ¼ë©´ ê¸‰ê²©í•œ ì„±ëŠ¥ í–¥ìƒ
```

**ì‹œì‚¬ì :**
- "ì¶©ë¶„íˆ í¬ë©´" ìƒˆë¡œìš´ ëŠ¥ë ¥ ì¶œí˜„
- ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ì›€
- ë” í¬ë©´ ë˜ ë¬´ìŠ¨ ëŠ¥ë ¥ì´?

**Chip Huyenì˜ ê²½í—˜:**
> "GPT-3ê°€ ë†€ë¼ìš´ ê²ƒì€ í¬ê¸°ë‚˜ ëŠ¥ë ¥ì´ ì•„ë‹ˆì—ˆë‹¤.  
> ì‘ì€ í’ˆì§ˆ í–¥ìƒì´ ì—„ì²­ë‚œ ìˆ˜ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤ëŠ” ê²ƒì´ ë†€ë¼ì› ë‹¤."

---

## Multimodalë¡œì˜ í™•ì¥

### í…ìŠ¤íŠ¸ë¥¼ ë„˜ì–´ì„œ

**ì¸ê°„ì˜ ì¸ì‹:**
- ì–¸ì–´ë¿ë§Œ ì•„ë‹ˆë¼
- ì‹œê°, ì²­ê°, ì´‰ê° ë“±

**AIì˜ í•„ìš”:**
> ì‹¤ì œ ì„¸ê³„ì—ì„œ ì‘ë™í•˜ë ¤ë©´ í…ìŠ¤íŠ¸ ì´ìƒ í•„ìš”

**ì§„í™”:**
```
Language Models (2020):
Input:  í…ìŠ¤íŠ¸
Output: í…ìŠ¤íŠ¸

Large Multimodal Models (2021~):
Input:  í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ + ë¹„ë””ì˜¤
Output: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ + ë¹„ë””ì˜¤
```

### Multimodal Modelì˜ ì‘ë™

**Language Model:**
```
ë‹¤ìŒ í† í° ì˜ˆì¸¡:
â† í…ìŠ¤íŠ¸ í† í°ë§Œ ì‚¬ìš©
```

**Multimodal Model:**
```
ë‹¤ìŒ í† í° ì˜ˆì¸¡:
â† í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ... í† í° ì‚¬ìš©
```

### CLIP: Vision + Language (2021)

**ê°œë°œ:** OpenAI

**í•™ìŠµ ë°©ë²•:** Natural Language Supervision
- Self-supervisionì˜ ë³€í˜•
- (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒì´ ì¸í„°ë„·ì— í•¨ê»˜ ë‚˜íƒ€ë‚¨
- 4ì–µ ê°œì˜ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ
- ImageNetì˜ 400ë°° í¬ê¸°
- ë ˆì´ë¸”ë§ ë¹„ìš© ì œë¡œ

**ëŠ¥ë ¥:**
- Zero-shot image classification
- Image-text similarity

**ì˜ˆì‹œ:**
```
Query: "a photo of a dog"
â†’ CLIPì´ ê°œ ì‚¬ì§„ë“¤ì„ ì°¾ì•„ëƒ„
(í•™ìŠµ ì¤‘ì— "ê°œ" ì¹´í…Œê³ ë¦¬ ëª…ì‹œì  í•™ìŠµ ì—†ì´!)
```

**ì¤‘ìš”:**
- CLIPì€ ìƒì„± ëª¨ë¸ ì•„ë‹˜
- Embedding ëª¨ë¸ (Chapter 3ì—ì„œ ìì„¸íˆ)
- Multimodal ìƒì„± ëª¨ë¸ì˜ ë°±ë³¸

### ì£¼ìš” Multimodal Models

**Vision + Language:**
- **Flamingo** (DeepMind, 2022): ì´ë¯¸ì§€ + ì§ˆë¬¸ â†’ ë‹µë³€
- **LLaVA**: ì˜¤í”ˆì†ŒìŠ¤ ë©€í‹°ëª¨ë‹¬
- **GPT-4V**: GPT-4 + Vision
- **Claude 3**: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì´í•´
- **Gemini**: ë‹¤ì–‘í•œ modality

**Text â†’ Image:**
- **DALL-E 3**: í…ìŠ¤íŠ¸ ì„¤ëª… â†’ ì´ë¯¸ì§€
  ```
  "An astronaut riding a horse on Mars"
  â†’ [í•´ë‹¹ ì´ë¯¸ì§€ ìƒì„±]
  ```
- **Midjourney**: ì˜ˆìˆ ì  ì´ë¯¸ì§€
- **Stable Diffusion**: ì˜¤í”ˆì†ŒìŠ¤

**Audio:**
- **Whisper** (OpenAI, 2022):
  - ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸ (transcription)
  - ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹
  - ë²ˆì—­
  - ì‹œë„ëŸ¬ìš´ í™˜ê²½ì—ì„œë„ ì‘ë™
- **Text-to-Speech**: í…ìŠ¤íŠ¸ â†’ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„±

**Video:**
- **Runway Gen-2**: í…ìŠ¤íŠ¸ â†’ 4ì´ˆ ë¹„ë””ì˜¤
- **Pika Labs**: í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ â†’ ë¹„ë””ì˜¤
- **Sora** (OpenAI, 2024):
  - í…ìŠ¤íŠ¸ â†’ ìµœëŒ€ 60ì´ˆ ë¹„ë””ì˜¤
  - ì˜í™” ìˆ˜ì¤€ í’ˆì§ˆ
  - ë¬¼ë¦¬ì  ì¼ê´€ì„±
  - ë³µì¡í•œ ì¥ë©´

---

## Foundation Modelsì˜ ì •ì˜

### Stanford HAI ì •ì˜ (2021)

> **Foundation Model:**  
> "ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ í•™ìŠµë˜ì–´ ë‹¤ì–‘í•œ downstream íƒœìŠ¤í¬ì— ì ì‘ë  ìˆ˜ ìˆëŠ” ëª¨ë¸"

### í•µì‹¬ íŠ¹ì§•

**1. Scale**
```
- ìˆ˜ì‹­ì–µ ~ ìˆ˜ì¡° parameters
- ìˆ˜ì¡° í† í°ì˜ í•™ìŠµ ë°ì´í„°
- ë§‰ëŒ€í•œ ê³„ì‚°ëŸ‰
```

**2. Generality (ë²”ìš©ì„±)**
```
- ë‹¨ì¼ íƒœìŠ¤í¬ ì•„ë‹˜
- ë‹¤ì–‘í•œ ë„ë©”ì¸
- ì—¬ëŸ¬ modality
```

**3. Adaptability (ì ì‘ì„±)**
```
- Prompt engineering
- Few-shot learning
- Finetuning
```

### ì£¼ìš” Foundation Models

**Text (LLMs):**
- GPT-4 (OpenAI)
- Claude (Anthropic)
- Gemini (Google)
- Llama 3 (Meta)

**Vision:**
- CLIP (OpenAI)
- DALL-E 3 (OpenAI)
- Midjourney
- Stable Diffusion

**Audio:**
- Whisper (OpenAI)
- AudioLM (Google)

**Video:**
- Sora (OpenAI)
- Runway Gen-2

**Multimodal:**
- GPT-4V (Vision)
- Gemini 1.5 (Text, Image, Video, Audio)
- Claude 3

### ì™œ "Foundation"ì¸ê°€?

**Foundation = ê¸°ì´ˆ, í† ëŒ€**

ì „í†µì  AI:
```
App 1 â†’ Model 1
App 2 â†’ Model 2
App 3 â†’ Model 3

ê° ì• í”Œë¦¬ì¼€ì´ì…˜ë§ˆë‹¤ ë³„ë„ ëª¨ë¸
```

Foundation Models:
```
         â”Œâ†’ App 1
         â”‚
Foundation â”¼â†’ App 2
  Model   â”‚
         â””â†’ App 3

í•˜ë‚˜ì˜ ëª¨ë¸ â†’ ì—¬ëŸ¬ ì• í”Œë¦¬ì¼€ì´ì…˜
```

**ê±´ì¶• ë¹„ìœ :**
```
ê±´ì¶•:
Foundation (ê¸°ì´ˆ) â†’ ê·¸ ìœ„ì— ë‹¤ì–‘í•œ ê±´ë¬¼

AI:
Foundation Model â†’ ê·¸ ìœ„ì— ë‹¤ì–‘í•œ ì•±
```

### Task-Specific â†’ General-Purpose

**ì´ì „:**
- ê°ì„± ë¶„ì„ ëª¨ë¸ â†’ ê°ì„± ë¶„ì„ë§Œ
- ë²ˆì—­ ëª¨ë¸ â†’ ë²ˆì—­ë§Œ

**Foundation Models:**
- Out-of-the-boxë¡œ ì—¬ëŸ¬ íƒœìŠ¤í¬ ê°€ëŠ¥
- Sentiment analysis + Translation + Coding + ...

**í•˜ì§€ë§Œ:**
- íŠ¹ì • íƒœìŠ¤í¬ì— ìµœì í™” ê°€ëŠ¥
- Prompt engineering, RAG, Finetuning

**ì˜ˆì‹œ: ì œí’ˆ ì„¤ëª… ìƒì„±**

Out-of-the-box:
```
âœ“ ì •í™•í•œ ì„¤ëª… ìƒì„±
âœ— ë¸Œëœë“œ voice ëˆ„ë½
âœ— í•µì‹¬ ë©”ì‹œì§€ ë†“ì¹¨
âœ— ë§ˆì¼€íŒ… cliche ë§ìŒ
```

ì ì‘ í›„:
```
âœ“ ë¸Œëœë“œ voice
âœ“ í•µì‹¬ ë©”ì‹œì§€
âœ“ ê³ í’ˆì§ˆ
```

ì ì‘ ë°©ë²•:
- **Prompt Engineering**: ìƒì„¸í•œ ì§€ì‹œ + ì˜ˆì‹œ
- **RAG**: ê³ ê° ë¦¬ë·° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
- **Finetuning**: ê³ í’ˆì§ˆ ì œí’ˆ ì„¤ëª…ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµ

---

## AI Engineeringì˜ íƒ„ìƒ

### AI Engineeringì´ë€?

**ì •ì˜:**
> Foundation Models ìœ„ì— ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì •

**ì°¨ì´:**
```
ì „í†µì  ML Engineering:
- ML ëª¨ë¸ ê°œë°œ

AI Engineering:
- ê¸°ì¡´ Foundation Models í™œìš©
```

### ì™œ ì§€ê¸ˆì¸ê°€?

**3ê°€ì§€ ìš”ì¸ì´ í•¨ê»˜ ì‘ìš©:**

**Factor 1: General-Purpose AI ëŠ¥ë ¥**

ì´ì „:
```
íŠ¹ì • íƒœìŠ¤í¬ë§Œ ê°€ëŠ¥
ìƒˆ ì• í”Œë¦¬ì¼€ì´ì…˜ = ìƒˆ ëª¨ë¸ í•„ìš”
```

í˜„ì¬:
```
ê±°ì˜ ëª¨ë“  íƒœìŠ¤í¬ ê°€ëŠ¥
ì´ì „ì— ë¶ˆê°€ëŠ¥í–ˆë˜ ê²ƒë„ ê°€ëŠ¥
ë¯¸ë˜ì—ëŠ” ë” ë§ì€ ê²ƒ ê°€ëŠ¥

â†’ ìˆ˜ìš” í­ì¦
```

ì˜ˆì‹œ:
- AIê°€ ì¸ê°„ì²˜ëŸ¼ ê¸€ ì“°ê¸° â†’ ëª¨ë“  ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìë™í™”
- ì¦‰ì‹œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ìƒì„± â†’ ì°½ì‘ ë¯¼ì£¼í™”
- í•™ìŠµ ë°ì´í„° í•©ì„±, ì•Œê³ ë¦¬ì¦˜ ê°œë°œ, ì½”ë“œ ì‘ì„± â†’ AIê°€ AI ë§Œë“¤ê¸°

**Factor 2: ì¦ê°€í•œ íˆ¬ì**

ChatGPT ì„±ê³µ â†’ íˆ¬ì ê¸‰ì¦

Goldman Sachs ì¶”ì •:
```
2025ë…„ AI íˆ¬ì:
- ë¯¸êµ­: $100B
- ê¸€ë¡œë²Œ: $200B

ì°¸ê³ : ë¯¸êµ­ ê³µêµìœ¡ ì˜ˆì‚° $900Bì˜ 1/9
```

S&P 500 ê¸°ì—…:
```
2022 Q2: 11% AI ì–¸ê¸‰
2023 Q2: 33% AI ì–¸ê¸‰ (3ë°°)

AI ì–¸ê¸‰ ê¸°ì—… ì£¼ê°€: +4.6%
ì¼ë°˜ ê¸°ì—… ì£¼ê°€: +2.4%
```

Matt Ross (Scribd):
> "2022ë…„ 4ì›” â†’ 2023ë…„ 4ì›”  
> AI ë¹„ìš© ì¶”ì •ì¹˜ê°€ 2 orders of magnitude ê°ì†Œ (100ë°°)"

**Factor 3: ë‚®ì€ ì§„ì… ì¥ë²½**

ì´ì „:
```
ëª¨ë¸ ì‚¬ìš©:
- ì¸í”„ë¼ í•„ìš”
- í˜¸ìŠ¤íŒ… í•„ìš”
- ì „ë¬¸ê°€ íŒ€ í•„ìš”
- ìˆ˜ë°±ë§Œ ë‹¬ëŸ¬
```

í˜„ì¬:
```
Model as a Service:
- API í˜¸ì¶œë§Œ
- ì¸í”„ë¼ ë¶ˆí•„ìš”
- ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
- ì €ë ´í•œ ë¹„ìš©
```

AIê°€ ì½”ë“œ ì‘ì„±:
```
- ì½”ë”© ê²½í—˜ ì—†ì–´ë„ ì•± ê°œë°œ
- Plain Englishë¡œ ì‘ì—…
- ëˆ„êµ¬ë‚˜ AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ê°€ëŠ¥
```

### ê°€ì¥ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” ë¶„ì•¼

**Sam Altman (OpenAI CEO, 2022ë…„ 9ì›”):**
> "ëŒ€ë‹¤ìˆ˜ ì‚¬ëŒë“¤ì—ê²Œ ê°€ì¥ í° ê¸°íšŒëŠ”  
> ì´ ëª¨ë¸ë“¤ì„ íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì ì‘ì‹œí‚¤ëŠ” ê²ƒ"

**GitHub Stars ì„±ì¥:**
```
2ë…„ ë‚´ì— AI Engineering ë„êµ¬ë“¤:
- AutoGPT, Stable Diffusion WebUI
- LangChain, Ollama

â†’ Bitcoin ì´ˆê³¼
â†’ React, Vue ì¶”ê²© ì¤‘

ì´ì „ ì–´ë–¤ ì†Œí”„íŠ¸ì›¨ì–´ ë„êµ¬ë³´ë‹¤ ë¹ ë¥¸ ì„±ì¥
```

**LinkedIn ì¡°ì‚¬ (2023ë…„ 8ì›”):**
- "Generative AI", "ChatGPT", "Prompt Engineering" í”„ë¡œí•„ ì¶”ê°€
- ë§¤ì›” í‰ê·  75% ì¦ê°€

**ComputerWorld:**
> "AIì—ê²Œ í–‰ë™ì„ ê°€ë¥´ì¹˜ëŠ” ê²ƒì´  
> ê°€ì¥ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” ì»¤ë¦¬ì–´ ìŠ¤í‚¬"

### ì™œ "AI Engineering"ì´ë¼ëŠ” ìš©ì–´?

**ê³ ë ¤í•œ ìš©ì–´ë“¤:**
- ML Engineering
- MLOps
- AIOps
- LLMOps

**ML Engineering ì„ íƒ ì•ˆ í•¨:**
- Foundation Models â‰  ì „í†µì  ML
- ì¤‘ìš”í•œ ì°¨ì´ì  ì¡´ì¬
- ML Engineering = ë‘˜ì„ í¬ê´„í•˜ëŠ” ìš©ì–´ë¡œëŠ” ì¢‹ìŒ

**~Ops ì„ íƒ ì•ˆ í•¨:**
- ìš´ì˜ë¿ë§Œ ì•„ë‹ˆë¼ ì—”ì§€ë‹ˆì–´ë§
- ì´ˆì ì€ ëª¨ë¸ ì ì‘(engineering)

**AI Engineering ì„ íƒ:**
- 20ëª… ì„¤ë¬¸: ëŒ€ë¶€ë¶„ ì„ í˜¸
- ì»¤ë®¤ë‹ˆí‹° ì„ íƒ ì¡´ì¤‘

---

## í•µì‹¬ ìš”ì•½

### Language Models â†’ LLMs

**Self-Supervisionì˜ í˜ëª…:**
```
ë ˆì´ë¸” ì—†ì´ í•™ìŠµ â†’ ë¬´í•œí•œ ë°ì´í„°
ë¹„ìš© ì œë¡œ â†’ ëŒ€ê·œëª¨ ëª¨ë¸ ê°€ëŠ¥
```

**Scaling Laws:**
```
ë” í¬ë©´ ë” ì¢‹ìŒ (ì˜ˆì¸¡ ê°€ëŠ¥)
GPT-1 (117M) â†’ GPT-4 (1.7T): 14,500ë°°
```

**Emergent Abilities:**
```
ì„ê³„ì  ë„˜ìœ¼ë©´ ìƒˆ ëŠ¥ë ¥:
- Few-shot learning
- Chain-of-thought
- Instruction following
```

### LLMs â†’ Foundation Models

**Multimodal í™•ì¥:**
```
í…ìŠ¤íŠ¸ only â†’ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ + ë¹„ë””ì˜¤
CLIP, DALL-E, Whisper, Sora
```

**Generality:**
```
Task-specific â†’ General-purpose
í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì—¬ëŸ¬ íƒœìŠ¤í¬
```

**Adaptability:**
```
ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì ì‘:
- Prompt engineering
- RAG
- Finetuning
```

### Foundation Models â†’ AI Engineering

**3ê°€ì§€ ìš”ì¸:**
```
1. ê°•ë ¥í•œ ë²”ìš© ëŠ¥ë ¥ â†’ ë” ë§ì€ ì• í”Œë¦¬ì¼€ì´ì…˜
2. ë‚®ì€ ì§„ì… ì¥ë²½ â†’ ëˆ„êµ¬ë‚˜ ê°œë°œ ê°€ëŠ¥
3. í­ë°œì  íˆ¬ì â†’ $200B by 2025
```

**ê²°ê³¼:**
```
ê°€ì¥ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” ì—”ì§€ë‹ˆì–´ë§ ë¶„ì•¼
Build â†’ Integrate
Code â†’ Prompt
Train â†’ Adapt
```

### Timeline

```
1950s:  ì²« Language Model (Shannon)
2013:   Word2Vec
2017:   Transformer â† ì „í™˜ì 
2018:   BERT, GPT-1 (117M)
2019:   GPT-2 (1.5B)
2020:   GPT-3 (175B), Few-shot
2021:   CLIP, "Foundation Models" ìš©ì–´
2022:   ChatGPT â† AI Engineering í­ë°œ
2023:   GPT-4, Claude, Gemini, Sora
2024:   AI Engineer = ê°€ì¥ ë¹ ë¥¸ ì„±ì¥ ì§êµ°
```

---

**ë‹¤ìŒ**: [1.2 Foundation Model Use Cases â†’](./1.2-foundation-model-use-cases.md)

[â† ëª©ì°¨ë¡œ ëŒì•„ê°€ê¸°](./README.md)
