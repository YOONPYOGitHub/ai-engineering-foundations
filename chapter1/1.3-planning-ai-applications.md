# 1.3 Planning AI Applications

> **"모든 AI 애플리케이션을 만들 필요는 없다. 만들어야 할 것을 신중히 선택하라."**

[← 목차로 돌아가기](./README.md)

---

## 📋 목차

- [개요](#개요)
- [Use Case Evaluation](#use-case-evaluation)
- [Setting Expectations](#setting-expectations)
- [Milestone Planning](#milestone-planning)
- [Maintenance](#maintenance)
- [핵심 요약](#핵심-요약)

---

## 개요

### AI 애플리케이션을 만들기 전에

**3가지 핵심 질문:**
```
1. 이 애플리케이션을 만들어야 하는가?
2. AI가 필요한가?
3. 내가 직접 만들어야 하는가?
```

**현실:**
- AI 가능 ≠ AI 필요
- 데모 성공 ≠ 프로덕션 성공
- 빠른 시작 ≠ 쉬운 유지보수

### Planning의 중요성

```
Planning 없이:
"AI로 뭔가 만들자!" 
→ 6개월 후: 사용 안됨, 유지보수 어려움, 비용 초과

Planning 후:
명확한 목표 + 현실적 기대 + 단계적 접근
→ 6개월 후: 사용자 만족, 지속적 개선, ROI 달성
```

**이 섹션의 목표:**
- 올바른 use case 선택
- 현실적인 기대 설정
- 체계적인 계획 수립
- 장기적 유지보수 전략

---

## Use Case Evaluation

> **"AI로 *할 수 있다*고 해서 *해야 한다*는 것은 아니다"**

### 1. 핵심 질문들

**Q1: 이 문제가 중요한가?**

평가 기준:
```
✅ 좋은 신호:
- 사용자가 반복적으로 겪는 문제
- 현재 해결책이 불만족스러움
- 해결 시 명확한 가치 (시간 절약, 비용 절감, 매출 증가)
- 많은 사람/팀이 영향받음

❌ 나쁜 신호:
- "있으면 좋겠다" 수준
- 실제 사용자 요청 없음
- 가치 측정 불가능
- 이미 좋은 해결책 존재
```

**Q2: AI가 필요한가?**

AI vs 전통적 방법:
```
AI가 적합한 경우:
✓ 규칙 기반으로 정의하기 어려움
✓ 패턴 인식 필요
✓ 자연어 이해/생성 필요
✓ 개인화 필요
✓ 창의성 필요

전통적 방법이 더 나은 경우:
✗ 명확한 규칙 정의 가능
✗ 100% 정확성 필요 (은행 거래, 의료 진단)
✗ 설명 가능성 필수
✗ 예측 가능성 필수
✗ 간단한 if-else로 충분
```

**예시:**

나쁜 선택:
```
Use Case: 사용자 나이 계산
현재: 생년월일 → 나이 계산 (규칙 명확)
AI로?: LLM에게 "이 사람 나이는?" 물어보기

문제:
- Overkill
- 비용 높음 (API 호출)
- 느림
- 불필요한 복잡도
- Hallucination 위험
```

좋은 선택:
```
Use Case: 고객 이메일 분류
현재: 수동 분류 (시간 소모, 일관성 부족)
AI로: 자동 분류 (긴급/일반/스팸)

이유:
- 규칙 정의 어려움 (언어의 미묘함)
- 반복 작업 (하루 수백 통)
- AI가 잘하는 태스크 (분류)
- 명확한 가치 (시간 절약)
```

**Q3: Foundation Model이 이 문제를 잘 푸는가?**

평가 방법:
```
1. 작은 샘플로 테스트
   - 10-20개 예시
   - 다양한 시나리오
   - Edge cases 포함

2. 기존 연구 확인
   - 유사한 use case
   - 벤치마크 성능
   - 알려진 한계

3. 경쟁사 분석
   - 다른 회사가 유사한 것을 만들었나?
   - 성공했나? 실패했나?
```

**Foundation Models의 강점과 약점:**

강점:
```
✓ 자연어 이해 (대화, 질문 답변)
✓ 텍스트 생성 (초안, 요약, 변환)
✓ 패턴 인식 (분류, 추출)
✓ 코드 생성/이해
✓ 창의적 작업 (아이디어, 콘텐츠)
✓ Few-shot learning (예시 몇 개로 학습)
```

약점:
```
✗ 수학 계산 (산술 오류)
✗ 정확한 사실 기억 (hallucination)
✗ 최신 정보 (training cutoff)
✗ 긴 문서 처리 (context limit)
✗ 일관성 (같은 질문, 다른 답)
✗ 비용 (대량 처리 시)
```

**Q4: 내가 만들어야 하는가?**

Build vs Buy:
```
직접 만들기 (Build):
적합한 경우:
- 핵심 차별화 요소
- 특수한 도메인 지식 필요
- 기존 솔루션 없음
- 데이터 프라이버시 중요
- 장기적 투자 가능

장점: 완전한 통제, 맞춤형
단점: 시간, 비용, 전문성 필요

기존 솔루션 사용 (Buy):
적합한 경우:
- 범용 use case
- 빠른 시장 진입 필요
- 리소스 제한적
- 전문성 부족
- 실험 단계

장점: 빠름, 검증됨, 유지보수 용이
단점: 제한적 맞춤화, 종속성
```

**의사결정 프레임워크:**

```
┌─────────────────────────────────────┐
│ 1. 문제 중요도?                      │
│    낮음 → STOP                       │
│    높음 → 2번으로                    │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│ 2. AI 필요?                          │
│    불필요 → 전통적 방법 사용         │
│    필요 → 3번으로                    │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│ 3. Foundation Model이 잘 푸나?      │
│    아니오 → 다른 접근 고려           │
│    예 → 4번으로                      │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│ 4. Build vs Buy?                    │
│    Buy → 기존 솔루션 탐색            │
│    Build → 5번으로                   │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│ 5. 리소스 있나?                      │
│    없음 → 재평가                     │
│    있음 → GO!                        │
└─────────────────────────────────────┘
```

### 2. 리스크 평가

**카테고리:**

**1. 기술적 리스크**
```
Hallucination:
- 심각도: 높음
- 완화: 검증 메커니즘, RAG, 사람 검토

Context Limit:
- 심각도: 중간
- 완화: 청킹, 요약, 긴 context 모델

비용:
- 심각도: 중간-높음
- 완화: Caching, 작은 모델, 최적화

Latency:
- 심각도: 중간
- 완화: Streaming, 비동기, 캐싱
```

**2. 비즈니스 리스크**
```
사용자 수용:
- 사람들이 실제 사용할까?
- 완화: 사용자 연구, 파일럿 테스트

ROI:
- 투자 대비 수익?
- 완화: 작게 시작, 단계적 확장

경쟁:
- 다른 회사가 더 나은 것을 만들면?
- 완화: 차별화 요소, 빠른 반복
```

**3. 윤리/법적 리스크**
```
편향성:
- AI 출력이 편향되면?
- 완화: 다양한 테스트, 모니터링

프라이버시:
- 민감한 데이터 처리?
- 완화: 데이터 최소화, 암호화

저작권:
- AI 생성 콘텐츠의 소유권?
- 완화: 법률 자문, 라이선스 확인

규제:
- 산업별 규제 준수?
- 완화: 컴플라이언스 팀과 협력
```

**리스크 매트릭스:**
```
            낮은 영향      높은 영향
높은 가능성  [주의]        [위험]
낮은 가능성  [무시 가능]   [모니터]

각 리스크를 매트릭스에 위치시키고
우선순위 결정
```

### 3. 성공 지표 정의

**SMART 목표:**
```
Specific:   구체적 (무엇을 달성?)
Measurable: 측정 가능 (어떻게 측정?)
Achievable: 달성 가능 (현실적?)
Relevant:   관련성 (비즈니스 목표와 연결?)
Time-bound: 기한 (언제까지?)
```

**예시:**

나쁜 목표:
```
❌ "AI로 고객 서비스를 개선한다"

문제:
- 너무 모호함
- 측정 불가능
- 기한 없음
```

좋은 목표:
```
✅ "3개월 내에 AI 챗봇을 도입하여
   일반적인 문의의 70%를 자동 응답하고
   평균 응답 시간을 5분에서 30초로 단축"

SMART:
- Specific: AI 챗봇, 일반 문의 자동 응답
- Measurable: 70% 자동 응답, 5분→30초
- Achievable: 3개월 (현실적)
- Relevant: 고객 만족도, 비용 절감
- Time-bound: 3개월
```

**지표 예시:**

태스크별:
```
분류:
- 정확도 (85%+)
- F1 score
- 사용자 수정 빈도

생성:
- Hallucination 비율 (5% 이하)
- 사용자 만족도 (4/5+)
- 수락률 (사용자가 AI 제안 수락)

검색:
- Precision@K
- MRR (Mean Reciprocal Rank)
- 사용자가 찾는 정보 발견 시간
```

비즈니스:
```
효율성:
- 시간 절약 (시간/주)
- 비용 절감 ($/월)
- 처리량 증가 (태스크/일)

사용자:
- 만족도 (NPS, CSAT)
- 참여도 (DAU, 세션 시간)
- 재사용률

재정:
- ROI
- 매출 증가
- 비용 절감
```

---

## Setting Expectations

> **"AI는 마법이 아니다. 확률적 시스템이다."**

### 1. AI의 한계 이해하기

**Foundation Models ≠ Perfect**

```
AI가 할 수 없는 것:
✗ 100% 정확하지 않음
✗ 설명 불가능한 경우 많음
✗ 예측 불가능 (같은 입력, 다른 출력)
✗ 편향성 존재
✗ 최신 정보 모름 (training cutoff)
✗ 진짜 "이해" 아님 (패턴 매칭)
```

**Hallucination 현실**

정의: AI가 사실이 아닌 것을 사실처럼 생성

예시:
```
질문: "2024년 노벨 물리학상 수상자는?"
AI: "Jane Doe가 양자 얽힘 연구로 수상" 
     ↑ 완전히 지어낸 이름과 연구

질문: "우리 회사의 Q3 매출은?"
AI: "$5.2M" (실제 데이터 없는데도 답변)
```

빈도:
```
연구 결과:
- GPT-3.5: 15-20% hallucination
- GPT-4: 5-10% (개선됐지만 여전히)
- 도메인별 차이:
  ├─ 역사/사실: 더 많음
  └─ 창작/아이디어: 상관 없음
```

완화책:
```
1. RAG (사실 확인용 데이터 제공)
2. 검증 메커니즘 (후처리)
3. 사용자 교육 ("AI는 때때로 틀림")
4. 사람 검토 (중요한 결정)
5. 확실성 표시 ("확실하지 않습니다")
```

### 2. 이해관계자와의 소통

**기대 조정:**

**경영진:**
```
그들의 기대:
"AI로 모든 것을 자동화하자!
 6개월 내에 비용 90% 절감!"

현실적 기대:
"AI로 반복 작업 50% 자동화
 첫 6개월: 파일럿 + 학습
 1년 내: 20-30% 비용 절감"

소통 방법:
- 단계적 로드맵 제시
- 빠른 승리 (quick wins) 먼저
- 지속적인 업데이트
- 리스크 명확히
```

**엔지니어:**
```
그들의 우려:
"AI는 불확실하고 디버깅 어려움
 기술 부채 될 것 같음"

해결:
- 명확한 평가 파이프라인
- 버전 관리
- 모니터링 시스템
- 기술 문서화
- 실험 프레임워크
```

**사용자:**
```
그들의 기대:
"AI가 완벽하게 이해하고 답변"

현실:
"AI는 보조 도구, 때때로 틀림"

교육:
- Onboarding에서 한계 설명
- 피드백 메커니즘 제공
- 투명성 (AI 생성 표시)
- 점진적 신뢰 구축
```

### 3. 데모 vs 프로덕션

**흔한 실수:**
```
데모 성공 → "프로덕션 준비됐어!"

현실:
데모: 10개 cherry-picked 예시
프로덕션: 10,000개 실제 사용자 쿼리

차이:
┌──────────────┬─────────────┬─────────────┐
│              │ 데모         │ 프로덕션     │
├──────────────┼─────────────┼─────────────┤
│ 데이터       │ 선별됨       │ 지저분함     │
│ 에러         │ 숨김         │ 처리 필수    │
│ Edge case    │ 무시         │ 자주 발생    │
│ 성능         │ 느려도 OK    │ 빨라야 함    │
│ 비용         │ 무시         │ 중요함       │
│ 모니터링     │ 없음         │ 필수         │
└──────────────┴─────────────┴─────────────┘
```

**프로덕션 준비 체크리스트:**
```
평가:
□ 다양한 실제 데이터로 테스트
□ Edge case 처리
□ 실패 시나리오 정의
□ 성공 지표 측정

성능:
□ Latency < 목표
□ 비용 예산 내
□ 동시 사용자 처리

안정성:
□ 에러 처리
□ Fallback 메커니즘
□ 로깅/모니터링
□ 알림 시스템

사용자 경험:
□ 명확한 UI/UX
□ 피드백 수집
□ 도움말/문서
□ Graceful degradation
```

### 4. 반복의 중요성

**첫 번째 버전은 완벽하지 않다:**
```
V1: 60% 정확도, 느림, 비싸
→ 사용자 피드백 수집
→ 문제 파악

V2: 75% 정확도, 빨라짐
→ 더 많은 사용자
→ 새로운 문제 발견

V3: 85% 정확도, 최적화
→ 안정화
→ 새로운 기능

지속적 개선...
```

**Agile 접근:**
```
Big Bang ✗
- 6개월 개발 → 출시 → 문제 발견 → 처음부터

Iterative ✓
- 2주 스프린트
- 작은 개선
- 빠른 피드백
- 방향 조정
```

---

## Milestone Planning

> **"큰 목표를 작은 단계로 나누라"**

### 1. 단계별 접근

**Phase 0: 검증 (1-2주)**

목표: "이게 가능한가?"

활동:
```
1. 수동 테스트
   - 10-20개 예시
   - ChatGPT/Claude로 직접 테스트
   - 결과 기록

2. 기술 검증
   - API 테스트
   - 비용 추정
   - Latency 측정

3. Go/No-Go 결정
   ✓ 가능성 확인됨 → Phase 1
   ✗ 안 됨 → 다른 접근 탐색
```

예시:
```
Use Case: 고객 이메일 자동 분류

Phase 0:
- 20개 이메일 수집
- GPT-4에 수동 분류 요청
- 결과: 18/20 정확 (90%)
- 비용: $0.001/이메일
- 결정: GO! → Phase 1
```

**Phase 1: 프로토타입 (2-4주)**

목표: "빠르게 작동하는 것 만들기"

활동:
```
1. 최소 기능 구현
   - 간단한 프롬프트
   - 기본 API 통합
   - 단순 UI

2. 내부 테스트
   - 팀원 10명
   - 실제 작업에 사용
   - 피드백 수집

3. 초기 평가
   - 정확도 측정
   - 문제점 파악
   - 개선 방향 도출
```

**Phase 2: 파일럿 (1-2개월)**

목표: "실제 사용자와 검증"

활동:
```
1. 선택된 사용자 그룹
   - 10-50명
   - 다양한 프로필
   - 적극적 피드백 가능

2. 핵심 기능 안정화
   - 에러 처리
   - 로깅
   - 기본 모니터링

3. 데이터 수집
   - 사용 패턴
   - 성공/실패 케이스
   - 사용자 만족도

4. 평가 및 반복
   - 주간 리뷰
   - 빠른 개선
   - KPI 추적
```

**Phase 3: 베타 (2-3개월)**

목표: "스케일 준비"

활동:
```
1. 더 넓은 사용자
   - 100-1000명
   - 다양한 사용 사례

2. 인프라 강화
   - 부하 테스트
   - 비용 최적화
   - 성능 개선

3. 운영 프로세스
   - 모니터링 대시보드
   - 알림 시스템
   - On-call 로테이션
   - 문서화

4. 론칭 준비
   - 마케팅 자료
   - 교육 자료
   - 지원 프로세스
```

**Phase 4: 일반 공개 (GA)**

목표: "모든 사용자에게"

활동:
```
1. 점진적 롤아웃
   - 10% → 25% → 50% → 100%
   - 각 단계에서 모니터링
   - 문제 발생 시 롤백 가능

2. 지속적 개선
   - 사용자 피드백 분석
   - A/B 테스트
   - 새로운 기능 추가

3. 스케일링
   - 인프라 확장
   - 비용 최적화
   - 성능 튜닝
```

### 2. 리소스 계획

**팀 구성:**

최소 (Startup/Small Team):
```
- 1 AI Engineer (개발 + 평가)
- 1 Product Manager (요구사항 + 조정)
- 1 Designer (UX)

파트타임:
- Data Scientist (평가)
- DevOps (인프라)
```

중간 (Growing Company):
```
- 2-3 AI Engineers
- 1 ML Engineer (최적화)
- 1 Product Manager
- 1 Designer
- 1 Data Analyst (분석)
- 0.5 DevOps
```

대규모 (Enterprise):
```
- 팀 lead
- 3-5 AI Engineers
- 2 ML Engineers
- 1-2 Data Scientists
- 1 Product Manager
- 1 Designer
- 1 QA Engineer
- 1 DevOps
```

**시간 배분:**

Phase별 시간:
```
Phase 0 (검증):       5-10%
Phase 1 (프로토타입): 15-20%
Phase 2 (파일럿):     25-30%
Phase 3 (베타):       25-30%
Phase 4 (GA):         10-15%

+ 지속적 개선: 20-30%
```

활동별 시간 (평균):
```
개발:       30%
평가:       25%
데이터:     15%
최적화:     10%
회의/조정:  10%
문서화:     5%
기타:       5%
```

**예산:**

비용 항목:
```
1. 인건비
   - 엔지니어 급여
   - 계약직/컨설턴트

2. 인프라
   - API 비용 (OpenAI, Anthropic)
   - 클라우드 (AWS, GCP)
   - GPU (학습/추론)

3. 도구/서비스
   - 평가 플랫폼
   - 모니터링 도구
   - 버전 관리

4. 데이터
   - 데이터 수집
   - 레이블링
   - 합성 데이터
```

예산 예시 (중간 규모 프로젝트, 6개월):
```
인건비:        $300K (3명 × $50K/월 × 6개월 × 33%)
API 비용:      $50K (평균 $8K/월)
클라우드:      $30K
도구:          $10K
예비:          $60K (약 10%)
─────────────────
총:            $450K
```

### 3. 의존성 관리

**기술 의존성:**
```
Critical (필수):
- LLM API (OpenAI, Anthropic)
- 클라우드 인프라
- 데이터베이스

Important (중요):
- 평가 도구
- 모니터링 시스템
- CI/CD

Nice-to-have:
- 실험 플랫폼
- 고급 분석 도구
```

**팀 의존성:**
```
Blocker:
- Legal (데이터 프라이버시)
- Security (보안 검토)
- Infra (배포 환경)

Coordinated:
- Product (요구사항)
- Design (UX)
- Marketing (출시)

Informed:
- Executive (업데이트)
- Support (준비)
```

**리스크 완화:**
```
단일 실패점 (SPOF) 제거:
- API: 여러 제공자, fallback
- 클라우드: Multi-region
- 팀: 지식 공유, 문서화

의존성 추적:
- RAID log (Risks, Assumptions, Issues, Dependencies)
- 주간 체크인
- 조기 경고 시스템
```

---

## Maintenance

> **"출시는 끝이 아니라 시작이다"**

### 1. 지속적 모니터링

**무엇을 모니터링?**

**성능 메트릭:**
```
Latency:
- p50, p95, p99 응답 시간
- 목표: < 2초 (대부분 애플리케이션)
- 알림: p95 > 5초

Error Rate:
- 4xx 에러 (클라이언트)
- 5xx 에러 (서버)
- 목표: < 1%

Throughput:
- 초당 요청 수 (RPS)
- 동시 사용자 수
```

**품질 메트릭:**
```
Accuracy/F1:
- 자동 평가 (가능한 경우)
- 샘플 기반 (수동)

사용자 피드백:
- 👍 / 👎 비율
- 리포트 빈도
- 만족도 설문

Hallucination:
- 검출률 (자동/수동)
- 심각도 분류
```

**비용 메트릭:**
```
- API 호출 수
- 토큰 사용량
- 인프라 비용
- $/사용자, $/쿼리
```

**대시보드 예시:**
```
┌─────────────────────────────────────┐
│ 🎯 핵심 지표 (실시간)                │
│ Latency: 1.2s ✓                     │
│ Error Rate: 0.5% ✓                  │
│ 비용: $234/day ⚠️ (예산 초과 추세)  │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│ 📊 사용자 피드백 (지난 7일)          │
│ 👍 87% 👎 13%                        │
│ 리포트: 5건 (이슈 티켓 생성됨)       │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│ 🔍 품질 (주간 샘플 100개)            │
│ Accuracy: 89% ✓                     │
│ Hallucination: 7% ⚠️                │
└─────────────────────────────────────┘
```

### 2. 모델 업데이트

**왜 업데이트?**
```
1. 새 모델 출시
   - GPT-4 → GPT-4-turbo
   - 더 빠름, 더 싸거나 더 나음

2. 성능 저하
   - 사용자 행동 변화
   - 새로운 edge case

3. 비용 최적화
   - 더 작은 모델로 충분?
   - 더 효율적인 프롬프트?

4. 새로운 기능
   - Vision, function calling
```

**업데이트 프로세스:**
```
1. 평가 (Offline)
   - 테스트 세트로 새 모델 평가
   - 기존 모델과 비교
   - Trade-off 분석

2. Canary (5-10%)
   - 소수 사용자에게 배포
   - 면밀히 모니터링
   - 1-2일

3. 확장 (50%)
   - 문제 없으면 50%로
   - 비교 분석 (A/B)
   - 1주

4. 완전 롤아웃 (100%)
   - 모든 사용자
   - 계속 모니터링
```

**롤백 계획:**
```
문제 발생 시:
1. 즉시 이전 버전 복원
2. 영향받은 사용자 파악
3. 원인 분석
4. 수정 후 재시도
```

### 3. 데이터 드리프트

**정의:**
```
Input 분포 변화:
- 사용자 행동 변화
- 새로운 제품/기능
- 계절성
- 외부 이벤트

예시:
모델 학습: 2023년 데이터
현재: 2024년 → 새로운 트렌드, 언어 변화
```

**감지 방법:**
```
1. 통계적 테스트
   - 입력 분포 비교
   - KL divergence

2. 성능 모니터링
   - Accuracy 하락?
   - 에러 증가?

3. 사용자 피드백
   - 불만 증가?
   - 특정 패턴의 실패?

4. 정기 평가
   - 월간 테스트 세트
   - 트렌드 추적
```

**대응:**
```
데이터 추가:
- 새로운 예시 수집
- 프롬프트에 예시 추가 (few-shot)

모델 업데이트:
- Finetuning (새 데이터)
- 더 강력한 모델

프롬프트 수정:
- 지시사항 개선
- 컨텍스트 추가
```

### 4. 기술 부채 관리

**흔한 기술 부채:**
```
1. 임시 해결책 (Workarounds)
   - "일단 이렇게..."
   - 나중에 정리 안 됨
   - 쌓임

2. 문서화 부족
   - 프롬프트가 왜 이렇게?
   - 어떤 모델? 어떤 설정?

3. 테스트 부족
   - "수동으로 확인함"
   - 회귀 위험

4. 모니터링 구멍
   - 일부만 추적
   - 알림 없음
```

**예방/해결:**
```
정기 리팩토링:
- 분기마다 1-2주
- 코드 정리
- 테스트 추가

문서화 습관:
- PR에 Why 설명
- ADR (Architecture Decision Records)
- 온보딩 문서

자동화:
- CI/CD
- 자동 테스트
- 자동 평가
```

### 5. 보안 & 컴플라이언스

**보안 고려사항:**
```
Prompt Injection:
- 악의적 프롬프트
- Input validation
- Output filtering

데이터 프라이버시:
- PII (개인 식별 정보) 마스킹
- GDPR/CCPA 준수
- 로그 보안

API 키 관리:
- 비밀 관리 (Vault, KMS)
- 로테이션
- 접근 제어

모델 출력:
- 유해 콘텐츠 필터
- 편향 검사
```

**컴플라이언스:**
```
산업별:
- 헬스케어: HIPAA
- 금융: PCI DSS
- 교육: FERPA

정기 감사:
- 분기별 보안 리뷰
- 접근 로그 분석
- 취약점 스캔

문서화:
- 데이터 흐름도
- 보안 정책
- 인시던트 대응 계획
```

### 6. 팀 & 프로세스

**On-call 로테이션:**
```
누가:
- AI Engineer (주)
- ML Engineer (백업)

언제:
- 24/7 (중요 시스템)
- Business hours (덜 중요)

어떻게:
- Runbook (장애 대응 매뉴얼)
- 에스컬레이션 경로
- 포스트모템
```

**지식 공유:**
```
Weekly:
- 팀 sync
- 이슈 리뷰
- 우선순위 조정

Monthly:
- Retrospective
- 학습 공유
- 프로세스 개선

Quarterly:
- 로드맵 리뷰
- 기술 부채 sprint
```

**사용자 지원:**
```
Tier 1: 일반 지원팀
- FAQ
- 알려진 이슈
- 간단한 문제 해결

Tier 2: AI 팀
- 복잡한 이슈
- 버그 수정
- 기능 개선
```

---

## 핵심 요약

### Planning 프레임워크

**1. Use Case Evaluation**
```
┌─ 중요한 문제?
├─ AI 필요?
├─ FM이 잘 푸나?
├─ Build vs Buy?
└─ 리스크 관리 가능?
   → YES: 진행
   → NO: 재검토
```

**2. Setting Expectations**
```
Reality Check:
- AI는 완벽하지 않음
- Hallucination 존재
- 지속적 개선 필요
- 비용 고려

커뮤니케이션:
- 경영진: 현실적 ROI
- 엔지니어: 명확한 평가
- 사용자: 한계 교육
```

**3. Milestone Planning**
```
Phase 0: 검증 (1-2주)
Phase 1: 프로토타입 (2-4주)
Phase 2: 파일럿 (1-2개월)
Phase 3: 베타 (2-3개월)
Phase 4: GA + 지속 개선

작게 시작 → 빠르게 반복 → 점진적 확장
```

**4. Maintenance**
```
모니터링:
- 성능 (latency, error)
- 품질 (accuracy, hallucination)
- 비용 ($/query)
- 사용자 만족도

지속적 개선:
- 모델 업데이트
- 데이터 드리프트 대응
- 기술 부채 관리
- 보안 & 컴플라이언스
```

### 성공의 열쇠

**1. 현실적 기대**
```
✓ AI는 도구, 마법 아님
✓ 완벽한 첫 버전 없음
✓ 지속적 투자 필요
```

**2. 체계적 접근**
```
✓ 단계적 계획
✓ 명확한 지표
✓ 빠른 피드백
```

**3. 장기적 관점**
```
✓ 유지보수 고려
✓ 팀 구축
✓ 프로세스 확립
```

---

**다음**: [1.4 AI Engineering Stack →](./1.4-ai-engineering-stack.md)

[← 목차로 돌아가기](./README.md)
