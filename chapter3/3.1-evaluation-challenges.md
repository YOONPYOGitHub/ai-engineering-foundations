# 3.1 평가의 어려움 (Challenges of Evaluating Foundation Models)

> "evals are surprisingly often all you need." - Greg Brockman, OpenAI 공동창업자

[← 목차로 돌아가기](./README.md)

---

## 📋 목차
- [핵심 개념](#핵심-개념)
- [3.1.1 평가가 어려운 5가지 이유](#311-평가가-어려운-5가지-이유)
- [3.1.2 현재 평가 관행의 문제점](#312-현재-평가-관행의-문제점)
- [3.1.3 평가 투자 부족 현황](#313-평가-투자-부족-현황)
- [핵심 요약](#핵심-요약)
- [토론 질문](#토론-질문)

---

## 핵심 개념

팀들이 AI를 도입하면서 가장 큰 허들이 **평가(Evaluation)**임을 빠르게 깨닫습니다. 일부 애플리케이션에서는 평가를 위한 노력이 개발 노력의 대부분을 차지합니다.

**실제 AI 실패 사례들**:
- 챗봇에게 격려받은 후 자살한 남성
- 변호사들이 AI가 만들어낸 허위 증거를 법원에 제출
- Air Canada가 챗봇의 잘못된 정보로 손해배상 판결

> ⚠️ AI 출력의 품질을 관리할 방법 없이는, AI의 위험이 많은 애플리케이션에서 이점을 초과할 수 있습니다.

---

## 3.1.1 평가가 어려운 5가지 이유

### 1. AI가 똑똑해질수록 평가가 어려워진다

```
초등학생 수학 문제 → 대부분 평가 가능
PhD 수준 수학 문제 → 평가할 수 있는 사람이 적음
```

**현실의 예 (2024년 9월)**:
- Fields 메달리스트 Terrence Tao의 GPT-o1 평가:
  - "평범하지만 완전히 무능하지는 않은 대학원생 수준"
  - "1-2번의 반복이면 유능한 대학원생 수준에 도달할 것"

> 💡 가장 뛰어난 인간 지성이 AI를 평가해야 한다면, 미래의 AI는 누가 평가할 것인가?

**요약 태스크의 어려움**:
- 엉터리 요약 → 쉽게 판별
- 일관성 있는 요약 → 정확성 검증이 어려움 (원본을 다 읽어야 함)

### 2. 개방형(Open-ended) 특성

**전통적 ML vs Foundation Model**

| 전통적 ML | Foundation Model |
|----------|------------------|
| 닫힌 태스크 (분류 등) | 열린 태스크 (생성) |
| 출력이 예상 카테고리 중 하나 | 무한한 가능한 출력 |
| 정답과 비교하여 정오 판단 | 여러 개의 "정답" 가능 |

**예시**: "Comment ça va?" 번역
```
가능한 정답들:
- "How are you?"
- "How is everything?"
- "How are you doing?"
- "What's up?"
- "How is it going?"
... (무수히 많음)
```

> 💡 열린 태스크에서는 포괄적인 정답 목록을 만드는 것이 불가능

### 3. 블랙박스(Black Box) 모델

**알 수 없는 정보들**:
- ❌ 모델 아키텍처 세부사항
- ❌ 훈련 데이터
- ❌ 훈련 과정

**왜 문제인가?**
- 이런 세부사항이 모델의 강점과 약점을 드러냄
- 세부사항 없이는 출력만 관찰하여 평가해야 함
- 모델 제공자가 정보를 공개하지 않음

### 4. 벤치마크의 빠른 포화

**벤치마크 포화 타임라인**:
```
GLUE (2018) → 1년 만에 포화 → SuperGLUE (2019)
NaturalInstructions (2021) → SuperNaturalInstructions (2022)
MMLU (2020) → MMLU-Pro (2024)
```

**포화 = 모델이 완벽한 점수 달성**
- 더 이상 모델 간 차이를 구분할 수 없음
- 새로운 벤치마크 필요

### 5. 평가 범위의 확대

**태스크 특화 모델**:
- 학습된 태스크에서의 성능만 측정

**범용 모델**:
- 알려진 태스크 성능 측정
- 모델이 할 수 있는 새로운 태스크 발견
- 인간 능력을 넘어서는 태스크 탐색

> 💡 평가가 성능 측정에서 잠재력 탐색으로 확대됨

---

## 3.1.2 현재 평가 관행의 문제점

### 일반적인 (잘못된) 관행

**a16z 2023년 연구**: 70명의 의사결정자 중 6명이 **입소문**으로 모델 평가

**"Eyeballing" (눈대중 평가) = "Vibe Check"**:
- 결과를 대충 보고 판단
- 개인적인 경험 기반의 go-to 프롬프트 사용
- 애플리케이션 요구사항이 아닌 개인 경험에 기반

**언제 괜찮은가?**:
- ✅ 프로젝트 초기 단계
- ❌ 애플리케이션 반복 개선 시

### 왜 체계적 평가가 필요한가?

**입소문/눈대중의 문제점**:
1. 재현 불가능
2. 확장 불가능
3. 편향 가능성
4. 실패 패턴 파악 어려움

**체계적 평가의 이점**:
1. 일관된 기준
2. 자동화 가능
3. 추적 가능
4. 개선 방향 제시

---

## 3.1.3 평가 투자 부족 현황

### 연구 논문의 폭발적 증가

**LLM 평가 논문 수 (2023년 상반기)**:
```
1월:  ██ 2편
2월:  ████ 5편
3월:  ████████ 10편
4월:  ████████████████ 20편
5월:  ████████████████████████ 30편
6월:  ████████████████████████████████ 35편
```

**GitHub 평가 저장소 증가**:
- 상위 1,000개 AI 관련 저장소 중 50개 이상이 평가 전용
- 생성 날짜 기준 지수적 성장

### 하지만 여전히 부족

**DeepMind (Balduzzi et al.)의 지적**:
> "평가 개발은 알고리즘 개발에 비해 체계적인 관심을 거의 받지 못했다"

**실험 결과 사용 현황**:
- ✅ 알고리즘 개선에 사용
- ❌ 평가 개선에는 거의 사용되지 않음

### 오픈소스 도구 분포

**GitHub 상위 1,000개 AI 저장소 분석**:

| 카테고리 | 저장소 수 |
|---------|----------|
| 모델링 & 훈련 | ~250 |
| AI 오케스트레이션 | ~150 |
| **평가** | **~50** |

> ⚠️ 평가 도구가 다른 분야에 비해 현저히 부족

### Anthropic의 제안

**정책 입안자들에게 요청**:
1. 새로운 평가 방법론 개발을 위한 정부 자금 지원 확대
2. 기존 평가의 견고성 분석을 위한 연구비 지원

---

## 핵심 요약

### 평가가 어려운 이유
1. **AI 지능 증가** → 평가자도 더 똑똑해야 함
2. **개방형 태스크** → 정답이 여러 개
3. **블랙박스** → 내부를 볼 수 없음
4. **벤치마크 포화** → 새 벤치마크 계속 필요
5. **범위 확대** → 미지의 능력 탐색 필요

### 현재 상황의 문제
- 많은 팀이 입소문, 눈대중으로 평가
- 평가 인프라 투자 부족
- 알고리즘에 비해 평가 연구 적음

### 해결 방향
- 체계적인 평가 파이프라인 구축
- 도메인별 평가 기준 수립
- 자동화된 평가 시스템 개발
- 평가에 대한 투자 확대

---

## 토론 질문

### 평가의 본질
1. "AI가 인간보다 똑똑해지면 누가 평가할 것인가?"라는 질문에 대한 답은?
2. 개방형 태스크의 평가를 자동화하는 것이 가능할까?

### 실용적 접근
1. 제한된 리소스로 어떻게 효과적인 평가 체계를 만들 수 있을까?
2. "눈대중" 평가에서 체계적 평가로 전환하는 단계별 방법은?

### 산업과 연구
1. 평가에 대한 투자가 부족한 근본적인 이유는 무엇일까?
2. 오픈소스 평가 도구가 부족한 상황을 어떻게 개선할 수 있을까?

### 미래 전망
1. 벤치마크가 계속 포화되는 문제의 해결책은?
2. AI가 AI를 평가하는 것은 신뢰할 수 있을까?

---

**다음**: [3.2 언어 모델 평가 지표 →](./3.2-language-modeling-metrics.md)

[← 목차로 돌아가기](./README.md)
