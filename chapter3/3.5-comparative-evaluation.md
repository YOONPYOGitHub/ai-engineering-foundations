# 3.5 비교 평가로 모델 순위 매기기 (Ranking Models with Comparative Evaluation)

> "비교 평가는 스포츠에서 거의 한 세기 동안 사용되어 왔으며, 이제 AI 평가에도 적용되고 있다"

[← 목차로 돌아가기](./README.md)

---

## 📋 목차
- [핵심 개념](#핵심-개념)
- [3.5.1 Pointwise vs Comparative Evaluation](#351-pointwise-vs-comparative-evaluation)
- [3.5.2 Comparative Evaluation의 도전 과제](#352-comparative-evaluation의-도전-과제)
- [3.5.3 Comparative Evaluation의 미래](#353-comparative-evaluation의-미래)
- [핵심 요약](#핵심-요약)
- [토론 질문](#토론-질문)

---

## 핵심 개념

### 왜 순위가 필요한가?

> 모델을 평가하는 이유는 점수 자체가 아니라 **어떤 모델이 최고인지** 알기 위함

**목표**: 모델들의 순위 결정

### 두 가지 순위 결정 방법

| 방법 | 설명 |
|------|------|
| **Pointwise** | 각 모델을 독립적으로 평가 → 점수로 순위 |
| **Comparative** | 모델들을 서로 비교 → 비교 결과로 순위 |

---

## 3.5.1 Pointwise vs Comparative Evaluation

### Pointwise Evaluation

**방법**: 
1. 각 모델을 독립적으로 평가 (예: 1-5 Likert 척도)
2. 점수를 기준으로 순위 매기기

**예시: 춤 경연대회**
```
모든 댄서에게 개별 점수 부여
→ 점수가 가장 높은 댄서가 우승
```

### Comparative Evaluation

**방법**:
1. 모델들을 서로 비교
2. 비교 결과에서 순위 계산

**예시: 춤 경연대회**
```
모든 댄서가 나란히 춤추게 함
→ 심사위원이 가장 좋아하는 댄서 선택
→ 가장 많이 선택된 댄서가 우승
```

### 왜 Comparative가 유용한가?

> 💡 주관적 품질은 비교가 점수 매기기보다 쉬움

**예시**:
- 두 노래 중 어느 것이 더 좋은지 말하기 → 쉬움
- 각 노래에 구체적인 점수 부여 → 어려움

### AI에서의 Comparative Evaluation

**역사**:
- 2021년 Anthropic이 처음 사용
- LMSYS Chatbot Arena: 커뮤니티 비교로 모델 순위

**실제 적용 (ChatGPT 예시)**:

```
┌────────────────────────────────────────────────┐
│ 두 응답 중 더 좋은 것을 선택해주세요             │
├────────────────────────────────────────────────┤
│                                                │
│  응답 A                    응답 B               │
│  ┌─────────┐              ┌─────────┐          │
│  │ ...     │              │ ...     │          │
│  └─────────┘              └─────────┘          │
│                                                │
│     [A 선택]    [동점]    [B 선택]              │
│                                                │
└────────────────────────────────────────────────┘
```

### Match (대결)와 Win Rate

**Match**: 두 모델 간의 한 번의 비교

**비교 이력 예시**:

| Match # | Model A | Model B | Winner |
|---------|---------|---------|--------|
| 1 | Model 1 | Model 2 | Model 1 |
| 2 | Model 3 | Model 10 | Model 10 |
| 3 | Model 7 | Model 4 | Model 4 |
| ... | ... | ... | ... |

**Win Rate**: A가 B를 이기는 확률
- 모든 A vs B 매치 중 A가 이긴 비율

### 순위 알고리즘

**5개 모델 Win Rate 예시**:

| Model A | Model B | # Matches | A >> B |
|---------|---------|-----------|--------|
| Model 1 | Model 2 | 1000 | 90% |
| Model 1 | Model 3 | 1000 | 40% |
| Model 1 | Model 4 | 1000 | 15% |
| Model 1 | Model 5 | 1000 | 10% |
| Model 2 | Model 3 | 1000 | 60% |
| Model 2 | Model 4 | 1000 | 80% |
| Model 2 | Model 5 | 1000 | 80% |
| Model 3 | Model 4 | 1000 | 70% |
| Model 3 | Model 5 | 1000 | 10% |
| Model 4 | Model 5 | 1000 | 20% |

→ 데이터만 보고 순위를 직관적으로 결정하기 어려움

### Rating 알고리즘

| 알고리즘 | 유래 | 특징 |
|---------|------|------|
| **Elo** | 체스 | 순서에 민감 |
| **Bradley-Terry** | 통계학 | LMSYS 현재 사용 |
| **TrueSkill** | Xbox | 팀 게임 지원 |

**LMSYS Chatbot Arena**:
- 처음에는 Elo 사용
- 평가자와 프롬프트 순서에 민감함 발견
- **Bradley-Terry로 전환**

### 순위의 품질 평가

> 순위가 정확하면: 높은 순위 모델이 낮은 순위 모델을 더 자주 이김

**예측 문제로 접근**:
- 과거 매치 결과 → 순위 계산
- 순위로 미래 매치 결과 예측
- 예측 정확도 = 순위 품질

---

## 3.5.2 Comparative Evaluation의 도전 과제

### 1. 확장성 병목 (Scalability Bottlenecks)

**문제**: 모델 쌍의 수가 **이차적으로** 증가

```
LMSYS (2024년 1월):
- 57개 모델
- 244,000개 비교
- 평균: 모델 쌍당 153개 비교

57개 모델 → 1,596개 모델 쌍
(57 × 56 / 2 = 1,596)
```

**완화: Transitivity 가정**
```
A > B 이고 B > C 이면 → A > C
→ A와 C를 직접 비교할 필요 없음
```

**⚠️ 한계**:
- 인간 선호가 항상 전이적이지 않음
- 다른 평가자, 다른 프롬프트로 인한 비전이성

**새 모델 평가 문제**:
- Pointwise: 새 모델만 평가
- Comparative: 새 모델을 기존 모델들과 비교해야 함 → 기존 순위도 변경될 수 있음

### 2. 표준화와 품질 관리 부족

**LMSYS Chatbot Arena 방식**:
```
1. 누구나 웹사이트 방문
2. 프롬프트 입력
3. 두 익명 모델의 응답 받음
4. 더 좋은 것 투표
5. 모델 이름 공개
```

**장점**:
- 다양한 신호 포착
- 게임하기 어려움

**문제점**:

#### 표준화 부재
- 누구나 어떤 프롬프트든 사용 가능
- 더 좋은 응답의 기준이 불명확
- 사실 확인 기대하기 어려움

```
일부 사용자: 예의 바르고 중립적인 응답 선호
다른 사용자: 필터 없는 응답 선호

→ 부적절한 농담 거부가 오히려 다운보트?
→ 독성 응답을 의도적으로 선호하는 악의적 사용자?
```

#### 실제 사용 환경과 차이
- 테스트 프롬프트 ≠ 실제 사용 프롬프트
- 고급 프롬프팅 기법 사용 안 함

**LMSYS 프롬프트 분석 (33,000개)**:
```
"hello" / "hi": 180개 (0.55%)
(다른 변형: "hello!", "hola", "hey" 등 미포함)

"X has 3 sisters, each has a brother. How many brothers does X have?"
→ 44번 질문됨
```

> 간단한 프롬프트로는 모델 간 차이를 구분하기 어려움

#### RAG 평가 한계
- 공개 리더보드는 RAG 지원 안 함
- RAG 성능 = 생성 능력 + 검색 능력

**해결 시도**:
- 사전 정의된 프롬프트 세트로 제한 (다양성 감소)
- 어려운 프롬프트만 필터링 (LMSYS)
- 신뢰할 수 있는 평가자만 사용 (Scale, 비용 증가)
- 제품에 통합 (코드 에디터에서 두 스니펫 비교)
- AI 평가자 사용 (인간보다 일관적일 수 있음)

### 3. 비교 성능 → 절대 성능 변환

**문제**: Win rate ≠ 실제 성능

```
상황:
- Model A: 고객 지원 티켓 70% 해결
- Model B: A를 51%로 이김

질문: B는 티켓을 몇 % 해결하나?
→ 알 수 없음!
```

**가능한 시나리오**:
1. B는 좋지만 A는 나쁨
2. A와 B 모두 나쁨
3. A와 B 모두 좋음

**비용-편익 분석 어려움**:
```
B가 A보다 2배 비쌈
51% win rate로 인한 성능 향상이 비용을 정당화하는가?
→ Comparative evaluation만으로는 답할 수 없음
```

> 💡 1% win rate 변화가 어떤 앱에서는 큰 성능 향상, 어떤 앱에서는 미미한 효과

---

## 3.5.3 Comparative Evaluation의 미래

### 왜 여전히 가치 있는가?

#### 1. 비교가 점수 매기기보다 쉬움
- 모델이 인간 능력을 넘어서면?
- 구체적 점수 불가, 하지만 차이 감지는 가능

**Llama 2 논문**:
> "최고의 인간 주석자 능력을 넘어서는 글쓰기에서도, 
> 인간은 두 답변을 비교할 때 여전히 가치 있는 피드백을 제공할 수 있다"

#### 2. 포화되지 않음
- 벤치마크: 완벽한 점수 달성 → 무용화
- Comparative: 더 강한 모델이 나오면 계속 유효

```
벤치마크 포화 문제:
GLUE → SuperGLUE → ... (계속 새 벤치마크 필요)

Comparative 평가:
새 모델이 나오면 기존 모델과 비교 (무한히 확장 가능)
```

#### 3. 게임하기 어려움
- 벤치마크: 참조 데이터로 학습 → 점수 부풀리기 가능
- Comparative: 쉬운 속임수 없음

> 💡 많은 사람들이 공개 벤치마크보다 Chatbot Arena를 더 신뢰

### 현재 한계에도 불구하고

**Comparative Evaluation의 장점**:
- 오프라인 평가: 벤치마크에 추가적인 신호 제공
- 온라인 평가: A/B 테스팅과 보완적

**⚠️ Comparative ≠ A/B Testing**:
- A/B Testing: 사용자가 한 모델의 출력만 봄
- Comparative: 사용자가 여러 모델의 출력을 동시에 봄

---

## 핵심 요약

### Comparative Evaluation 요약

| 측면 | 내용 |
|------|------|
| **정의** | 모델을 서로 비교하여 순위 결정 |
| **방법** | 매치 → Win Rate → Rating 알고리즘 |
| **알고리즘** | Elo, Bradley-Terry, TrueSkill |
| **장점** | 비교가 쉬움, 포화 없음, 게임 어려움 |
| **한계** | 확장성, 표준화, 절대 성능 변환 |

### Pointwise vs Comparative

| 측면 | Pointwise | Comparative |
|------|-----------|-------------|
| 평가 방식 | 독립적 점수 | 상호 비교 |
| 새 모델 추가 | 쉬움 | 어려움 |
| 포화 문제 | 있음 | 없음 |
| 절대 성능 | 알 수 있음 | 알 수 없음 |
| 게임하기 | 쉬움 | 어려움 |

### 주의사항

1. **선호도 ≠ 정확도**
   - 수학 문제: "어떤 답이 더 좋아?"가 아니라 "어떤 답이 맞아?"
   - 선호도 투표가 적절한 질문 vs 아닌 질문 구분 필요

2. **투표자의 전문성**
   - 투표자가 주제를 알아야 유효한 선호도 신호
   - AI가 인턴/어시스턴트 역할일 때 적합

---

## 토론 질문

### 이해도 확인
1. Elo와 Bradley-Terry 알고리즘의 차이점은?
2. Transitivity 가정이 깨지면 어떤 문제가 발생하는가?

### 실용적 적용
1. 우리 제품에 Comparative Evaluation을 어떻게 통합할 수 있을까?
2. Win rate를 실제 비즈니스 메트릭으로 어떻게 변환할 수 있을까?

### 심화 토론
1. Chatbot Arena 순위를 얼마나 신뢰할 수 있는가?
2. AI가 인간보다 훨씬 똑똑해지면 Comparative Evaluation도 한계에 도달할까?

---

## Summary (Chapter 3 전체 요약)

AI 모델이 강해질수록 치명적 실패의 잠재성도 높아져 평가가 더욱 중요해집니다. 동시에 개방형의 강력한 모델을 평가하는 것은 도전적입니다.

이 챕터에서 다룬 내용:

1. **Foundation Model 평가가 어려운 이유**
   - 투자 부족
   - 벤치마크 포화

2. **언어 모델 메트릭**
   - Perplexity, Cross Entropy
   - 평가와 데이터 처리에 활용

3. **개방형 응답 평가 접근법**
   - Functional Correctness
   - Similarity Scores
   - AI as a Judge

4. **AI as a Judge**
   - Subjective evaluation
   - Judge에 따라 점수가 달라짐
   - Exact evaluation, Human evaluation으로 보완 필요

5. **Comparative Evaluation**
   - Preference signals 기반
   - Preference model로 자동화 시도

> 다음 챕터: 평가 파이프라인 구축 방법

---

[← 이전: 3.4 AI as a Judge](./3.4-ai-as-judge.md) | [목차로 돌아가기](./README.md)
