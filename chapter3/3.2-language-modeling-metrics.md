# 3.2 언어 모델 평가 지표 (Understanding Language Modeling Metrics)

> "언어 모델의 성능은 다운스트림 애플리케이션 성능과 높은 상관관계가 있다"

[← 목차로 돌아가기](./README.md)

---

## 📋 목차
- [핵심 개념](#핵심-개념)
- [3.2.1 Entropy (엔트로피)](#321-entropy-엔트로피)
- [3.2.2 Cross Entropy (교차 엔트로피)](#322-cross-entropy-교차-엔트로피)
- [3.2.3 Bits-per-Character와 Bits-per-Byte](#323-bits-per-character와-bits-per-byte)
- [3.2.4 Perplexity (혼란도)](#324-perplexity-혼란도)
- [3.2.5 Perplexity 해석과 활용](#325-perplexity-해석과-활용)
- [핵심 요약](#핵심-요약)
- [토론 질문](#토론-질문)

---

## 핵심 개념

Foundation Model은 언어 모델에서 발전했습니다. 많은 Foundation Model이 여전히 언어 모델을 핵심 구성 요소로 가지고 있습니다.

**언어 모델이란?**
- 언어에 대한 통계적 정보를 인코딩
- "I like drinking __" 다음에 "tea"가 "charcoal"보다 더 가능성 높음
- 더 많은 통계 정보를 캡처할수록 다음 토큰 예측을 더 잘함

**4가지 핵심 메트릭** (서로 변환 가능):
1. Cross Entropy
2. Perplexity
3. Bits-per-Character (BPC)
4. Bits-per-Byte (BPB)

---

## 3.2.1 Entropy (엔트로피)

### 정의

**Entropy**: 토큰이 평균적으로 운반하는 정보량

- 엔트로피가 높을수록 → 각 토큰이 더 많은 정보를 운반
- 더 많은 비트가 토큰을 표현하는 데 필요

### 직관적 이해

**예시: 사각형 내 위치를 설명하는 언어**

```
언어 A: 2개 토큰                  언어 B: 4개 토큰
┌─────────────┐                   ┌──────┬──────┐
│    상       │                   │ 좌상 │ 우상 │
├─────────────┤                   ├──────┼──────┤
│    하       │                   │ 좌하 │ 우하 │
└─────────────┘                   └──────┴──────┘

1 비트 필요                        2 비트 필요
Entropy = 1                        Entropy = 2
```

**언어 A**: 
- 2개 토큰 → 1비트로 충분
- Entropy = 1

**언어 B**:
- 4개 토큰 → 2비트 필요
- Entropy = 2
- 더 구체적인 정보, 하지만 더 많은 비트 필요

### 엔트로피와 예측 가능성

> 💡 엔트로피가 낮을수록 → 언어가 더 예측 가능

- 2개 토큰 언어: 2개 중 하나 예측 (더 쉬움)
- 4개 토큰 언어: 4개 중 하나 예측 (더 어려움)

---

## 3.2.2 Cross Entropy (교차 엔트로피)

### 정의

**Cross Entropy**: 언어 모델이 데이터셋에서 다음에 올 것을 예측하는 어려움

언어 모델을 훈련할 때:
- 목표: 모델이 훈련 데이터의 분포를 학습
- 모델이 더 잘 학습할수록 → Cross Entropy가 낮아짐

### 수학적 표현

- **P**: 훈련 데이터의 실제 분포
- **Q**: 언어 모델이 학습한 분포

```
H(P) = 훈련 데이터의 엔트로피
DKL(P||Q) = P와 Q 사이의 KL Divergence

Cross Entropy: H(P, Q) = H(P) + DKL(P||Q)
```

### 핵심 특성

1. **비대칭성**: H(P, Q) ≠ H(Q, P)

2. **완벽한 학습 시**:
   - 모델이 완벽히 학습하면 → Q = P
   - KL Divergence = 0
   - Cross Entropy = 데이터의 Entropy

3. **훈련 목표**: Cross Entropy 최소화

---

## 3.2.3 Bits-per-Character와 Bits-per-Byte

### 필요성

**문제**: 모델마다 토큰화 방법이 다름
- 모델 A: 단어를 토큰으로
- 모델 B: 문자를 토큰으로

→ 토큰당 비트 수로는 모델 간 비교 불가

### Bits-per-Character (BPC)

**정의**: 한 문자를 표현하는 데 필요한 비트 수

```
예시:
- 토큰당 비트: 6
- 토큰당 평균 문자 수: 2
- BPC = 6 / 2 = 3
```

### Bits-per-Byte (BPB)

**문제**: 문자 인코딩도 다름
- ASCII: 7비트/문자
- UTF-8: 8-32비트/문자

**해결**: Bits-per-Byte 사용

```
예시:
- BPC = 3
- 문자당 7비트 (ASCII)
- BPB = 3 / (7/8) = 3.43
```

### 압축과의 관계

BPB가 3.43이면:
- 원본 1바이트(8비트)를 3.43비트로 표현 가능
- **절반 이하로 압축 가능!**

---

## 3.2.4 Perplexity (혼란도)

### 정의

**Perplexity**: Entropy/Cross Entropy의 지수 형태

```
데이터 Perplexity: PPL(P) = 2^H(P)
모델 Perplexity:   PPL(P,Q) = 2^H(P,Q)
```

### 직관적 이해

> Perplexity = 다음 토큰 예측 시 모델의 불확실성

**예시**:
- 4개 위치 토큰을 완벽히 학습한 모델
- Cross Entropy = 2비트
- Perplexity = 2² = 4

→ 모델이 4개의 선택지 중에서 고민

### Bit vs Nat

**Bit (base 2)**:
```
PPL(P, Q) = 2^H(P,Q)
```

**Nat (자연로그, TensorFlow/PyTorch 사용)**:
```
PPL(P, Q) = e^H(P,Q)
```

> 💡 혼란을 피하기 위해 많은 연구자들이 Cross Entropy 대신 Perplexity 보고

---

## 3.2.5 Perplexity 해석과 활용

### 좋은 Perplexity 값이란?

**데이터와 계산 방법에 따라 다름**

| 규칙 | 설명 | 예시 |
|------|------|------|
| 구조화된 데이터 | 더 낮은 PPL | HTML > 일상 텍스트 |
| 어휘 크기 | 클수록 높은 PPL | 전쟁과 평화 > 동화책 |
| 컨텍스트 길이 | 길수록 낮은 PPL | 1951년: 10토큰, 현재: 500~10,000 토큰 |

**참고**: Perplexity 3 이하도 흔함
- 3의 PPL = 1/3 확률로 다음 토큰 정확히 예측
- 어휘가 10,000~100,000인 것 고려하면 놀라운 수치!

### GPT-2 모델별 Perplexity

| 모델 크기 | WikiText2 PPL | PTB PPL |
|----------|---------------|---------|
| 117M | 29.41 | 65.85 |
| 345M | 22.76 | 47.33 |
| 762M | 19.93 | 40.31 |
| 1542M | 18.34 | 35.76 |

> 💡 큰 모델일수록 Perplexity가 낮음 (더 좋음)

### 활용 사례

#### 1. 모델 능력 프록시
- 다음 토큰 예측을 못하면 → 다운스트림 태스크도 안 좋을 가능성
- 많은 회사들이 모델 Perplexity 비공개화 추세

#### 2. 데이터 오염 탐지
- 모델이 본 텍스트에서 Perplexity가 가장 낮음
- 벤치마크 데이터의 PPL이 낮으면 → 훈련 데이터에 포함되었을 가능성
- 벤치마크 신뢰도 하락

#### 3. 데이터 중복 제거
- 새 데이터의 PPL이 높으면 → 훈련셋에 없는 새로운 데이터
- 새 데이터의 PPL이 낮으면 → 이미 있는 데이터와 유사

#### 4. 비정상 텍스트 탐지
- PPL이 높은 텍스트:
  - 이상한 아이디어: "my dog teaches quantum physics in his free time"
  - 의미 없는 텍스트: "home cat go eye"

### ⚠️ 주의사항

```
Post-training (SFT, RLHF) 후에는 Perplexity가 
좋은 프록시가 아닐 수 있음!

- Post-training: 태스크 완수 방법 학습
- 태스크 완수를 잘하면서 다음 토큰 예측은 더 못할 수 있음
- "Post-training이 엔트로피를 붕괴시킨다"

Quantization도 Perplexity를 예상치 못하게 변화시킬 수 있음
```

### Perplexity 계산 방법

```python
# 언어 모델 X, 토큰 시퀀스 x1, x2, ..., xn

PPL = P(x1, x2, ..., xn)^(-1/n)
    = (1 / P(x1, x2, ..., xn))^(1/n)
    = (∏ 1/P(xi | x1, ..., xi-1))^(1/n)
```

**필요 조건**: 모델이 logprobs를 노출해야 함
- 모든 상용 모델이 logprobs를 노출하는 것은 아님
- 보안상의 이유로 제한하는 경우가 많음

---

## 핵심 요약

### 4가지 메트릭의 관계

```
Entropy → Cross Entropy → Perplexity
   H(P)      H(P,Q)          2^H(P,Q)
         ↓
   Bits (BPC, BPB)
```

### 핵심 포인트

| 메트릭 | 의미 | 좋은 값 |
|--------|------|---------|
| Entropy | 토큰의 정보량 | - |
| Cross Entropy | 예측 어려움 | 낮을수록 좋음 |
| Perplexity | 불확실성 (선택지 수) | 낮을수록 좋음 |
| BPC/BPB | 압축 효율성 | 낮을수록 좋음 |

### 활용 분야

1. **모델 훈련 가이드**
2. **모델 능력 비교**
3. **데이터 오염 탐지**
4. **데이터 중복 제거**
5. **비정상 텍스트 탐지**

---

## 토론 질문

### 이해도 확인
1. Perplexity 5와 Perplexity 50의 실질적 차이는 무엇인가?
2. 왜 Post-training 후에 Perplexity가 나빠질 수 있는가?

### 실용적 적용
1. 우리 프로젝트에서 Perplexity를 어떻게 활용할 수 있을까?
2. 데이터 오염을 탐지하기 위해 Perplexity를 어떻게 사용할 수 있을까?

### 심화 토론
1. 모델 회사들이 Perplexity를 비공개하는 이유는 무엇일까?
2. Perplexity 외에 언어 모델 성능을 측정하는 다른 방법은?

---

**다음**: [3.3 정확한 평가 →](./3.3-exact-evaluation.md)

[← 이전: 3.1 평가의 어려움](./3.1-evaluation-challenges.md) | [목차로 돌아가기](./README.md)
