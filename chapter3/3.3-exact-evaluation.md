# 3.3 정확한 평가 (Exact Evaluation)

> "정확한 평가는 모호함 없이 판단을 제공한다"

[← 목차로 돌아가기](./README.md)

---

## 📋 목차
- [핵심 개념](#핵심-개념)
- [3.3.1 Functional Correctness (기능적 정확성)](#331-functional-correctness-기능적-정확성)
- [3.3.2 Similarity Measurements (유사도 측정)](#332-similarity-measurements-유사도-측정)
- [3.3.3 Introduction to Embedding](#333-introduction-to-embedding)
- [핵심 요약](#핵심-요약)
- [토론 질문](#토론-질문)

---

## 핵심 개념

### Exact vs Subjective Evaluation

| 유형 | 설명 | 예시 |
|------|------|------|
| **Exact Evaluation** | 모호함 없는 판단 | 객관식 정답 확인 |
| **Subjective Evaluation** | 평가자에 따라 달라짐 | 에세이 채점 |

**예시**:
- 객관식 정답 A, 당신의 답 B → **틀림** (모호함 없음)
- 에세이 점수 → 같은 사람도 시간차를 두고 다른 점수 가능

### 두 가지 Exact Evaluation 접근법

1. **Functional Correctness**: 의도한 기능을 수행하는가?
2. **Similarity Measurements**: 참조 데이터와 얼마나 유사한가?

---

## 3.3.1 Functional Correctness (기능적 정확성)

### 정의

> 시스템이 의도한 기능을 수행하는지 평가

**예시**:
- 웹사이트 생성 요청 → 생성된 웹사이트가 요구사항을 충족하는가?
- 레스토랑 예약 요청 → 예약이 성공했는가?

### 왜 궁극적 메트릭인가?

- 애플리케이션이 **해야 할 일을 하는지** 직접 측정
- 하지만 항상 측정하기 쉽지 않음
- 자동화가 어려운 경우가 많음

### 코드 생성에서의 Functional Correctness

**자동화 가능한 대표적 사례**

```python
# 예시: 최대공약수 함수 생성 요청
def gcd(num1, num2):
    # AI가 생성한 코드
    ...

# 테스트
assert gcd(15, 20) == 5  # 정답은 5
```

**검증 방법**:
1. Python 인터프리터에 코드 입력
2. 코드가 유효한지 확인
3. 주어진 입력에 대해 올바른 결과 반환하는지 확인

### pass@k 메트릭

**주요 벤치마크**:
- **HumanEval** (OpenAI)
- **MBPP** (Google) - Mostly Basic Python Problems Dataset
- **Spider**, **BIRD-SQL**, **WikiSQL** (Text-to-SQL)

**HumanEval 예시**:

```python
# 문제
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ 주어진 숫자 리스트에서 두 숫자가 threshold보다 
        가까이 있는지 확인
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True 
    """

# 테스트 케이스
def check(candidate):
    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True
    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False
    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True
    # ... 더 많은 테스트
```

**pass@k 계산**:
- 각 문제에 대해 k개의 코드 샘플 생성
- k개 중 하나라도 모든 테스트 통과하면 → 문제 해결
- **pass@k = 해결된 문제 / 전체 문제**

```
예시: 10개 문제, k=3일 때 5개 해결
pass@3 = 5/10 = 50%

일반적으로:
pass@1 < pass@3 < pass@10
```

### 게임 봇과 측정 가능한 목표

**자동 평가 가능한 다른 태스크**:
- 게임 봇: 테트리스 점수
- 에너지 최적화: 절감된 에너지량
- 예약 시스템: 성공적인 예약 수

> 💡 측정 가능한 목표가 있는 태스크 = Functional Correctness로 평가 가능

---

## 3.3.2 Similarity Measurements (유사도 측정)

Functional Correctness로 자동 평가할 수 없는 경우, **참조 데이터와 비교**

### 참조 데이터(Reference Data)

**형식**: (입력, 참조 응답들)
- 하나의 입력에 여러 개의 참조 응답 가능
- 예: 프랑스어 문장의 여러 영어 번역

**용어**:
- Reference responses = Ground truths = Canonical responses
- Reference-based metrics vs Reference-free metrics

### 유사도 측정의 4가지 방법

| 방법 | 설명 | 점수 유형 |
|------|------|----------|
| 평가자 판단 | 인간/AI가 동일 여부 판단 | - |
| Exact Match | 정확히 일치 여부 | 이진 (O/X) |
| Lexical Similarity | 외형적 유사도 | 0~1 스케일 |
| Semantic Similarity | 의미적 유사도 | -1~1 스케일 |

---

### Exact Match (정확 매칭)

**정의**: 생성된 응답이 참조 응답과 **정확히 일치**하는가?

**적합한 경우**:
- 짧고 정확한 답변을 기대하는 태스크
- 수학 문제, 상식 질문, 퀴즈

**예시**:
```
✓ "2 + 3은?" → "5"
✓ "최초의 여성 노벨상 수상자는?" → "마리 퀴리"
✓ "현재 계좌 잔액은?" → "$1,234.56"
```

**변형: Contains Match**
- 출력에 참조 응답이 **포함**되면 일치로 처리
- "2 + 3은?" → "정답은 5입니다" (5 포함)

**⚠️ 한계**:
```
질문: "Anne Frank는 언제 태어났나?"
정답: 1929
모델 출력: "September 12, 1929"

→ 1929가 포함되어 있지만, 9월 12일이 아님 (6월 12일이 맞음)
→ 잘못된 정보가 정답으로 처리될 수 있음
```

**복잡한 태스크에서의 한계**:
```
프랑스어: "Comment ça va?"

참조 번역:
- "How are you?"
- "How is everything?"
- "How are you doing?"

모델 출력: "How is it going?"
→ 올바른 번역이지만 참조에 없어서 오답 처리
```

---

### Lexical Similarity (어휘 유사도)

**정의**: 두 텍스트가 얼마나 **겹치는지** 측정

#### Fuzzy Matching (근사 문자열 매칭)

**Edit Distance**: 한 텍스트를 다른 텍스트로 변환하는 데 필요한 편집 수

```
편집 연산:
1. 삭제: "brad" → "bad"
2. 삽입: "bad" → "bard"
3. 치환: "bad" → "bed"
(4. 전치: "mats" → "mast")

예시:
"bad" → "bard": 1 편집
"bad" → "cash": 3 편집
→ "bad"는 "bard"에 더 유사
```

#### N-gram Similarity

**N-gram**: 연속된 N개의 토큰

```
"My cats scare the mice"

Unigrams (1-gram): My, cats, scare, the, mice
Bigrams (2-gram): "my cats", "cats scare", "scare the", "the mice"
```

**비교 예시**:
```
참조: "My cats scare the mice"

A: "My cats eat the mice"
   → 5개 중 4개 일치 (80%)

B: "Cats and mice fight all the time"
   → 5개 중 3개 일치 (60%)

→ A가 더 유사
```

#### 주요 메트릭

| 메트릭 | 용도 | 비고 |
|--------|------|------|
| **BLEU** | 번역 | n-gram precision |
| **ROUGE** | 요약 | n-gram recall |
| **METEOR++** | 번역 | 개선된 BLEU |
| **TER** | 번역 | Translation Edit Rate |
| **CIDEr** | 이미지 캡션 | - |

**사용 벤치마크**: WMT, COCO Captions, GEMv2

#### ⚠️ 한계

1. **포괄적인 참조 세트 필요**
   - 좋은 응답이 참조에 없으면 낮은 점수
   - Fuyu 모델 사례: 정답인데 낮은 점수

2. **참조 자체가 틀릴 수 있음**
   - WMT 2023: 많은 잘못된 참조 번역 발견

3. **높은 점수 ≠ 좋은 응답**
   - HumanEval에서 BLEU 점수: 오답과 정답이 유사
   - BLEU 최적화 ≠ Functional Correctness 최적화

---

### Semantic Similarity (의미 유사도)

**정의**: 두 텍스트가 얼마나 **같은 의미**인지 측정

#### Lexical vs Semantic

```
Lexical 유사 but Semantic 다름:
"Let's eat, grandma" vs "Let's eat grandma"
→ 외형 유사, 의미 완전히 다름

Lexical 다름 but Semantic 유사:
"What's up?" vs "How are you?"
→ 단어 겹침 없음, 의미 유사
```

#### Embedding 기반 유사도

1. 텍스트를 **Embedding (수치 벡터)** 으로 변환
2. **Cosine Similarity**로 유사도 계산

```
"the cat sits on a mat" → [0.11, 0.02, 0.54, ...]
```

#### Cosine Similarity 계산

```
A = 생성된 응답의 embedding
B = 참조 응답의 embedding

Cosine Similarity = (A · B) / (||A|| × ||B||)

- A · B: 내적 (dot product)
- ||A||: L2 노름 (유클리드 노름)

점수 범위: -1 ~ 1
- 1: 완전히 동일
- 0: 관련 없음
- -1: 반대
```

#### 주요 메트릭

| 메트릭 | 임베딩 소스 |
|--------|------------|
| **BERTScore** | BERT |
| **MoverScore** | 혼합 알고리즘 |

#### 장점과 한계

**장점**:
- Lexical보다 포괄적인 참조 세트 불필요
- 의미적 동치성 포착

**한계**:
- 임베딩 알고리즘 품질에 의존
- 계산 비용 (비트리비얼한 컴퓨트와 시간)

> 💡 Semantic similarity는 정확 평가 카테고리에 넣었지만, 임베딩 알고리즘에 따라 결과가 달라지므로 주관적 요소도 있음

---

## 3.3.3 Introduction to Embedding

### Embedding이란?

**정의**: 원본 데이터의 의미를 포착하는 **수치적 표현**

```
"the cat sits on a mat" → [0.11, 0.02, 0.54, ...]
```

**특징**:
- 벡터 형태
- 일반적인 크기: 100 ~ 10,000 차원
- 원본 데이터보다 저차원

### 주요 Embedding 모델

| 모델 | Embedding 크기 |
|------|---------------|
| **BERT base** | 768 |
| **BERT large** | 1024 |
| **OpenAI CLIP** | 512 (이미지/텍스트) |
| **OpenAI text-embedding-3-small** | 1536 |
| **OpenAI text-embedding-3-large** | 3072 |
| **Cohere embed-english-v3.0** | 1024 |
| **Cohere embed-english-light-3.0** | 384 |

### 좋은 Embedding이란?

> 유사한 텍스트 → 가까운 임베딩

```
"the cat sits on a mat" 의 임베딩은

"the dog plays on the grass" 에 더 가깝고
"AI research is super fun" 에서 더 멀어야 함
```

**평가 방법**:
- 태스크 기반 평가 (분류, 추천, RAG 등)
- MTEB (Massive Text Embedding Benchmark)

### Embedding의 활용

| 용도 | 설명 |
|------|------|
| **유사도 평가** | 이 챕터의 주제 |
| **검색과 Retrieval** | RAG에서 관련 문서 찾기 |
| **순위 매기기** | 쿼리에 유사한 순서로 정렬 |
| **클러스터링** | 유사한 항목 그룹화 |
| **이상 탐지** | 다른 항목들과 다른 것 찾기 |
| **데이터 중복 제거** | 너무 유사한 항목 제거 |

### Multimodal Embedding

**CLIP (Contrastive Language-Image Pre-training)**:
- 텍스트와 이미지를 **공동 임베딩 공간**에 매핑
- (이미지, 텍스트) 쌍으로 학습

```
훈련 목표:
이미지 임베딩과 해당 텍스트 임베딩이 가깝도록

결과:
"a fisherman" 임베딩 ≈ 낚시하는 남자 이미지 임베딩
"fashion show" 임베딩 ≠ 낚시하는 남자 이미지 임베딩
```

**확장된 모델**:
- **ULIP**: 텍스트, 이미지, 3D 포인트 클라우드
- **ImageBind**: 6개 모달리티 (텍스트, 이미지, 오디오 등)

---

## 핵심 요약

### Exact Evaluation 비교

| 방법 | 자동화 | 적합한 태스크 | 한계 |
|------|--------|--------------|------|
| Functional Correctness | 일부 가능 | 코드, 게임, 측정 가능한 목표 | 많은 태스크에서 자동화 어려움 |
| Exact Match | 쉬움 | 짧은 정답 | 복잡한 태스크에서 실패 |
| Lexical Similarity | 쉬움 | 번역, 요약 | 포괄적 참조 필요, 의미 무시 |
| Semantic Similarity | 중간 | 의미 비교 필요 시 | 임베딩 품질 의존 |

### 언제 무엇을 사용할까?

```
코드 생성? → Functional Correctness (pass@k)
간단한 QA? → Exact Match
번역/요약? → BLEU/ROUGE (Lexical)
의미 비교 필요? → BERTScore (Semantic)
```

---

## 토론 질문

### 이해도 확인
1. Lexical similarity와 Semantic similarity의 차이를 예시로 설명해보세요.
2. pass@k에서 k가 커지면 점수가 어떻게 변하나요? 왜 그런가요?

### 실용적 적용
1. 우리 프로젝트에 가장 적합한 평가 방법은 무엇인가?
2. 참조 데이터를 만드는 비용을 어떻게 줄일 수 있을까?

### 심화 토론
1. BLEU 점수 최적화가 실제 품질 향상으로 이어지지 않는 이유는?
2. Embedding 품질이 평가 결과에 미치는 영향을 어떻게 측정할 수 있을까?

---

**다음**: [3.4 AI as a Judge →](./3.4-ai-as-judge.md)

[← 이전: 3.2 언어 모델 평가 지표](./3.2-language-modeling-metrics.md) | [목차로 돌아가기](./README.md)
