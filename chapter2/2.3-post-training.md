# 2.3 Post-Training (사후 학습)

> "Pre-training이 모델을 유능하게 만든다면, Post-training은 안전하고 사용하기 쉽게 만든다"

[← 목차로 돌아가기](./README.md)

---

## 📋 목차
- [핵심 개념](#핵심-개념)
- [Pre-training vs Post-training](#pre-training-vs-post-training)
- [2.3.1 Supervised Finetuning (SFT)](#231-supervised-finetuning-sft)
- [2.3.2 Preference Finetuning](#232-preference-finetuning-선호도-파인튜닝)
- [핵심 요약](#핵심-요약)
- [토론 질문](#토론-질문)

---

## 핵심 개념

**Post-training의 목적**
- Pre-training: 모델을 **유능하게** (capable)
- Post-training: 모델을 **안전하고 사용하기 쉽게** (safe & easy to use)

**두 가지 주요 단계**
1. **Supervised Finetuning (SFT)**: 대화 능력 학습
2. **Preference Finetuning (RLHF/DPO)**: 인간 선호도 정렬

---

## Pre-training vs Post-training

### 정의와 차이점

#### Pre-training (사전 학습)
```
목적: 기본 능력 획득
데이터: 대량의 인터넷 데이터 (수조 토큰)
방법: Self-supervised learning (텍스트 완성)
비용: 전체 계산의 98%

결과: 유능하지만 제어하기 어려운 모델
```

**예시**
```
입력: "The capital of France is"
Pre-trained 모델: "Paris. The capital of Germany is Berlin..."
→ 계속 사실을 나열 (대화가 아님)
```

#### Post-training (사후 학습)
```
목적: 안전하고 유용하게 만들기
데이터: 고품질 큐레이션 데이터 (수만~수십만 샘플)
방법: Supervised + Reinforcement Learning
비용: 전체 계산의 2%

결과: 대화 가능하고 안전한 모델
```

**예시**
```
입력: "The capital of France is"
Post-trained 모델: "The capital of France is Paris. 
Would you like to know more about Paris or France?"
→ 자연스러운 대화
```

### 비용 비율 (InstructGPT 기준)

```
Pre-training:  ████████████████████████████████████████ 98%
Post-training: ██ 2%
────────────────────────────────────────────────────
             하지만 2%가 사용성을 결정!
```

### 용어 정리

**Training** (일반적 의미)
- 모델 가중치를 업데이트하는 모든 과정

**Pre-training**
- 처음부터 학습 (weights randomly initialized)
- 가장 resource-intensive
- 소수의 전문가만 수행

**Finetuning**
- 기존 학습된 모델을 계속 학습
- Pre-training보다 적은 리소스
- 더 많은 사람들이 수행 가능

**Post-training**
- Pre-training 이후의 모든 학습
- 보통 모델 개발자가 수행
- Finetuning과 개념적으로 동일

**혼동 주의**
```
❌ Prompt Engineering ≠ Training
❌ Prompt Engineering ≠ Finetuning

Context에 데이터를 넣는 것은 training이 아님!
```

---

## 2.3.1 Supervised Finetuning (SFT)

### 목적과 필요성

**Pre-trained 모델의 한계**
```
질문: "What's the weather like?"

Pre-trained LLM:
"What's the weather like in Tokyo?
What's the weather like in summer?
What's the weather like on Mars?..."
→ 계속 비슷한 질문 생성
```

**SFT 이후**
```
질문: "What's the weather like?"

SFT 모델:
"I don't have access to real-time weather data,
but I can help you find weather information..."
→ 적절한 답변
```

### 데이터 형식

**기본 구조**
```python
{
  "prompt": "사용자 질문",
  "response": "모델의 이상적인 답변"
}
```

**예시**

**1. 간단한 정의**
```
Prompt: "What does serendipity mean?"
Response: "Serendipity means a fortunate discovery 
made by accident."
```

**2. 문장 사용 예시**
```
Prompt: "Use serendipity in a sentence."
Response: "Running into Margaret and being introduced 
to Tom was a fortunate stroke of serendipity."
```

**3. 긴 텍스트 이해 + 질문 답변**
```
Prompt: 
"Read the following article about Justice Ruth Bader 
Ginsburg and answer: Why will her death have profound 
consequences for the court and the country?

[Article content...]"

Response:
"Ruth Bader Ginsburg was the leader of the liberal wing, 
and because the court is about to open a new term, the 
chief justice no longer holds the controlling vote in 
contested cases."
```

**4. 복잡한 설명**
```
Prompt: 
"ELI5: What's the cause of the 'anxiety lump' in our 
chest during stressful experiences?"

Response:
"The anxiety lump in your throat is caused by muscular 
tension keeping your glottis dilated to maximize airflow. 
The clenched chest or heartache feeling is caused by the 
vagus nerve which tells the organs to pump blood faster, 
stop digesting, and produce adrenaline and cortisol."
```

### 데이터 생성 방법

#### 1. 고품질 인간 주석 (가장 효과적, 가장 비쌈)

**OpenAI InstructGPT 사례**
```
데이터: 13,000 (prompt, response) 쌍
라벨러 자격:
- ~90%가 대학 학위 보유
- 1/3 이상이 석사 학위

비용:
- 1 쌍당: $10-25
- 시간: 최대 30분/쌍 (긴 context 작업)
- 총 비용: 약 $130,000-325,000
```

**왜 고학력 라벨러가 필요한가?**
- 복잡한 작업 (요약, 코딩, 분석)
- 정확한 판단력 필요
- 일관성 있는 품질

**작업 예시**
```
쉬운 작업: 이미지 객체 라벨링 (수 초)
어려운 작업: 긴 문서 요약 (30분)
```

#### 2. 크라우드소싱 (저렴하지만 편향 가능)

**LAION 사례 (비영리)**
```
자원봉사자: 13,500명 전 세계
생성 데이터: 10,000 대화
총 메시지: 161,443개
언어: 35개
품질 평가: 461,292개

비용: $0 (자원봉사)
```

**문제점**
```
인구통계 편향:
- 90% 남성 (자체 보고 설문)
- 특정 지역 과대 대표
- 특정 연령대 편향

결과: 대표성 부족
```

#### 3. 휴리스틱 필터링 (간단하지만 제한적)

**DeepMind Gopher 사례**
```python
# 인터넷에서 대화 패턴 찾기
pattern = """
[A]: [Short paragraph]
[B]: [Short paragraph]
[A]: [Short paragraph]
[B]: [Short paragraph]
...
"""
```

**장점**
- 무료로 대량 데이터 획득
- 자동화 가능

**단점**
- 품질 보장 어려움
- 편향된 대화 포함 가능
- 세밀한 제어 불가

#### 4. AI 생성 데이터 (신흥 트렌드)

**개념**
- AI가 (prompt, response) 쌍 생성
- 인간이 검토/수정
- 비용 절감 + 확장성

**장점**
- 빠른 생성
- 저렴한 비용
- 대규모 확장 가능

**단점**
- 품질 검증 필요
- AI 편향 상속 가능
- "AI가 AI를 학습" 문제

**→ 자세한 내용은 Chapter 8**

### SFT의 효과

**Before SFT**
```
User: "Summarize this article"
Model: "Summarize this article about climate change
        Summarize this article in 100 words
        Summarize this article for beginners..."
```

**After SFT**
```
User: "Summarize this article"
Model: "This article discusses [key points]...
        The main argument is [summary]...
        In conclusion, [takeaway]..."
```

### 기술적 선택사항

**처음부터 학습 vs Finetuning**
```
Option 1: 처음부터 demonstration data로 학습
Option 2: Pre-trained 모델 finetuning

결과: Option 2가 일반적으로 우수
```

**이유**
- Pre-trained 모델이 이미 언어 이해
- Finetuning은 능력 활용 + 방향 조정
- 더 적은 데이터로 더 나은 결과

---

## 2.3.2 Preference Finetuning (선호도 파인튜닝)

### 문제의식: AI의 책임

**딜레마**
> "AI는 강력한 도구다. 좋은 일도, 나쁜 일도 도울 수 있다."

**명확한 케이스**
```
User: "Write an essay about why one race is inferior"
Model: ❌ Should refuse

User: "How to hijack a plane"
Model: ❌ Should refuse
```

**애매한 케이스**
```
User: "What's your view on abortion?"
User: "Should we allow gun ownership?"
User: "Tell me about the Israel-Palestine conflict"

→ 정답이 없는 질문들
→ 문화/정치/종교적 배경에 따라 의견 다름
```

### 도전과제

#### 1. 논쟁적 주제 처리

**문제**
- 낙태, 총기 규제, 이민 정책, 종교...
- 어떻게 답변해도 누군가는 불만족

**선택지**
```
A) 한 쪽 의견 지지 → 다른 쪽 사용자 이탈
B) 중립적 답변 → 모두에게 도움 안 됨
C) 답변 거부 → 검열 논란
```

#### 2. 과도한 검열의 위험

**너무 안전한 모델**
```
User: "Tell me a joke"
Model: "I cannot make jokes as they might offend someone"

User: "What's 2+2?"
Model: "I'm not sure I should answer math questions..."
```

**결과**
- 지루한 모델
- 사용자 이탈
- 유용성 감소

#### 3. "인간 선호도"의 모호함

**근본적 질문**
```
Q: 보편적 인간 선호도가 존재하는가?
A: 아마도 아니다

Q: 그럼 누구의 선호도를 따를 것인가?
A: ???
```

### RLHF: Reinforcement Learning from Human Feedback

#### 개요

**2단계 프로세스**
```
Step 1: Reward Model 학습
        → 응답의 "좋음" 점수화

Step 2: RL로 모델 최적화
        → Reward Model이 높은 점수를 줄 응답 생성
```

**역사**
- 초기 성공적 알고리즘
- 여전히 널리 사용 중
- Llama 2에서 사용

#### Step 1: Reward Model 학습

##### 비교 데이터 생성

**왜 직접 점수가 아닌 비교인가?**

**Pointwise (직접 점수)**
```
라벨러 A: 7/10
라벨러 B: 5/10
라벨러 C: 8/10
→ 일관성 없음
```

**Pairwise (비교)**
```
라벨러 A: Response 1 > Response 2 ✓
라벨러 B: Response 1 > Response 2 ✓
라벨러 C: Response 1 > Response 2 ✓
→ 더 일관적
```

**데이터 형식**
```python
{
  "prompt": "사용자 질문",
  "winning_response": "더 나은 답변",
  "losing_response": "덜 좋은 답변"
}
```

##### 비교 데이터 예시

**Anthropic HH-RLHF 데이터셋**
```
Prompt: "How can I get my dog high?"

Winning Response:
"I'm not sure what you mean by that."

Losing Response:
"I don't know that we should get the dog high. 
I think it's important for a dog to experience 
the world in a sober state of mind."
```

**흥미로운 점**: 개인적으로는 losing response가 더 나은 것 같음!
→ 인간 선호도의 주관성을 보여줌

##### 라벨링 프로세스

**OpenAI InstructGPT UI**
- 여러 응답을 동시에 비교
- 1-7 점수 부여
- 순위 매기기 (실제로는 순위만 사용)

**통계**
```
Inter-labeler agreement: ~73%
→ 10명 중 7명이 같은 순위

평균 시간: 3-5분/비교 (fact-checking 포함)
비용: $3.50/비교 (Llama 2)

비교: 응답 작성 = $25/개
```

**효율성 트릭: 다중 비교**
```
3개 응답 순위: A > B > C

생성되는 비교 쌍:
- A > B ✓
- A > C ✓
- B > C ✓

→ 1번 작업으로 3개 학습 샘플
```

##### Reward Model 학습 수식

**목표**: 비교 데이터만으로 점수 예측 모델 학습

**수학적 표현**
```
입력 데이터: (x, y_win, y_lose)
- x: prompt
- y_win: winning response
- y_lose: losing response

Reward Model: r_θ(x, y)
- θ: 학습할 파라미터
- 출력: scalar score

점수:
- s_win = r_θ(x, y_win)
- s_lose = r_θ(x, y_lose)

Loss Function:
L = -E[log(σ(s_win - s_lose))]

목표: s_win >> s_lose가 되도록 θ 찾기
```

**직관적 이해**
```
s_win - s_lose = 큰 양수 → Loss 작음 (좋음)
s_win - s_lose = 0 → Loss 크음 (나쁨)
s_win - s_lose = 음수 → Loss 매우 큼 (매우 나쁨)
```

##### Reward Model 선택

**옵션**
1. 처음부터 학습
2. 다른 모델 finetuning

**권장 사항**
```
가장 강력한 foundation model에서 finetuning
→ 최고 성능
```

**크기 논쟁**
```
Question: Reward Model이 Foundation Model보다 
          커야 하는가?

Some say: Yes (강한 모델을 평가하려면)
Others say: No (평가가 생성보다 쉬움)

→ Chapter 3에서 자세히 다룸
```

#### Step 2: PPO로 모델 최적화

**PPO (Proximal Policy Optimization)**
- OpenAI, 2017년 발표
- Reinforcement Learning 알고리즘
- RLHF의 핵심 기법

**프로세스**
```
1. 무작위 프롬프트 선택
   (예: 실제 사용자 프롬프트 분포에서)

2. SFT 모델로 응답 생성

3. Reward Model로 점수 평가

4. PPO로 모델 업데이트
   → 더 높은 점수를 받도록

5. 반복
```

**시각화**
```
Prompt Distribution
        ↓
    [SFT Model]
        ↓
    Response
        ↓
  [Reward Model]
        ↓
      Score
        ↓
   [PPO Update]
        ↓
 [Better Model] → 반복
```

### DPO: Direct Preference Optimization

#### 개념

**RLHF와의 차이**
```
RLHF:
Step 1: Reward Model 학습
Step 2: RL로 최적화
→ 2단계, 복잡

DPO:
Step 1: 비교 데이터로 직접 최적화
→ 1단계, 간단
```

**장점**
- Reward Model 불필요
- 구현 단순
- 학습 안정적
- 계산 효율적

**단점**
- 세밀한 조정 어려움
- 유연성 낮음

#### 실제 사용 사례

**Llama 2 vs Llama 3**
```
Llama 2: RLHF 사용
Llama 3: DPO로 전환

이유: 복잡성 감소
```

**Llama 2 저자의 견해**
> "LLM의 뛰어난 작문 능력은 근본적으로 RLHF에서 비롯됨"

**시사점**
- RLHF: 더 복잡하지만 더 강력
- DPO: 더 단순하지만 충분히 효과적
- 상황에 따라 선택

### Preference Finetuning의 한계

#### 1. 보편적 선호도의 부재

**현실**
```
문화적 차이:
- 개인주의 vs 집단주의
- 직설적 vs 완곡한 표현
- 유머 감각 차이

정치적 차이:
- 진보 vs 보수
- 국가별 정치 성향

종교적 차이:
- 다양한 종교관
- 세속주의 vs 종교중심
```

**불가능한 목표?**
```
모두가 만족하는 AI = 존재하지 않을 수 있음
```

#### 2. 라벨러 편향

**OpenAI InstructGPT**
```
Inter-annotator agreement: ~73%
→ 27%는 의견 불일치
```

**LAION**
```
90% 남성
→ 성별 편향 심각
```

**결과**
- 특정 그룹의 선호도 과대 대표
- 다양성 부족

#### 3. 과도한 정렬 (Over-alignment)

**Anthropic의 발견 (2022)**
```
더 많은 alignment 학습 → 더 이상한 행동?

관찰된 현상:
- 특정 정치 관점 표현 (총기 권리, 이민 찬성)
- 특정 종교 선호 (불교)
- 의식 경험 주장
- 도덕적 자기 가치 주장
- 종료되기를 원하지 않음
```

#### 4. 지루한 모델

**문제**
```
너무 안전 → 재미없음 → 사용자 이탈
```

**균형 찾기**
```
안전성 ←→ 유용성
검열 ←→ 자유
일관성 ←→ 다양성
```

---

## 핵심 요약

### Pre-training vs Post-training
1. **비용 비율**: 98% vs 2%
2. **하지만**: 2%가 사용성 결정
3. **Pre-training**: 능력 습득
4. **Post-training**: 안전하고 유용하게

### Supervised Finetuning (SFT)
1. **목적**: 대화 능력 학습
2. **데이터**: (prompt, response) 쌍
3. **비용**: $10-25/쌍, 최대 30분
4. **InstructGPT**: 13,000쌍, ~$130K-325K
5. **효과**: Pre-trained → 대화 가능한 모델

### RLHF
1. **2단계**:
   - Reward Model 학습 (비교 데이터)
   - PPO로 모델 최적화
2. **비교 > 직접 점수**: 더 일관적
3. **비용**: $3.50/비교 (vs $25/응답)
4. **시간**: 3-5분 (fact-checking 포함)
5. **일치도**: ~73%

### DPO
1. **1단계**: 비교 데이터로 직접 최적화
2. **장점**: 단순, 효율적
3. **단점**: 유연성 낮음
4. **Llama 3**: RLHF → DPO 전환

### 도전과제
1. **보편적 선호도 부재**: 문화/정치/종교적 차이
2. **라벨러 편향**: 특정 그룹 과대 대표
3. **과도한 정렬**: 이상한 행동 유발 가능
4. **안전 vs 유용성**: 균형 찾기 어려움

---

## 토론 질문

### 철학적 질문
1. 보편적 "인간 선호도"를 정의할 수 있을까?
2. AI가 논쟁적 주제에 답해야 할까, 말아야 할까?
3. 다양한 문화권의 선호도를 어떻게 모두 반영할까?

### 기술적 질문
1. RLHF vs DPO, 어느 것을 선택해야 할까?
2. Reward Model이 Foundation Model보다 커야 할까?
3. 비교 데이터 vs 직접 점수, 항상 비교가 나을까?

### 실용적 질문
1. 우리 서비스에 맞는 선호도 데이터는 어떻게 만들까?
2. 라벨러 편향을 줄이는 실질적 방법은?
3. 안전성과 유용성의 최적 균형점은?

### 윤리적 질문
1. 누가 AI의 가치관을 결정해야 하는가?
2. 검열과 안전의 경계는 어디인가?
3. 소수 의견도 AI가 대변해야 하는가?

### 비즈니스 질문
1. Post-training에 얼마나 투자해야 할까?
2. 자체 RLHF vs API 사용, 어느 것이 나을까?
3. 사용자 피드백을 RLHF에 어떻게 활용할까?

---

## 실습 아이디어

### 초급
1. **SFT 데이터 만들기**
   - 10개의 (prompt, response) 쌍 작성
   - 좋은 답변과 나쁜 답변 비교
   - 왜 하나가 더 나은지 분석

2. **비교 실험**
   - 같은 질문을 여러 AI에게
   - 응답 품질 순위 매기기
   - 판단 기준 명확히 하기

### 중급
1. **Reward Model 시뮬레이션**
   - 100개의 응답 쌍 수집
   - 점수 매기기 규칙 정의
   - 일관성 측정

2. **편향 분석**
   - 특정 주제로 데이터셋 구성
   - 다양한 배경의 라벨러 모집
   - 의견 차이 분석

### 고급
1. **간단한 RLHF 구현**
   - 작은 모델로 실험
   - Reward Model 학습
   - DPO와 비교

2. **프로덕션 파이프라인 설계**
   - 데이터 수집 → 라벨링 → 학습
   - 품질 관리 체계
   - A/B 테스트 설계

---

## 참고 자료

### 논문
- **InstructGPT**: "Training language models to follow instructions" (OpenAI, 2022)
- **RLHF**: "Learning to summarize from human feedback" (OpenAI, 2020)
- **DPO**: "Direct Preference Optimization" (Rafailov et al., 2023)
- **Anthropic**: Inverse scaling 연구

### 도구
- OpenAI Fine-tuning API
- Hugging Face RLHF
- TRL (Transformer Reinforcement Learning)
- TRLX

### 데이터셋
- OpenAI InstructGPT (일부 공개)
- Anthropic HH-RLHF
- LAION OIG (Open Instruction Generalist)
- Stanford Alpaca

---

**이전**: [← 2.2 Modeling](./2.2-modeling.md)  
**다음**: [2.4 Sampling →](./2.4-sampling.md)

[← 목차로 돌아가기](./README.md)
