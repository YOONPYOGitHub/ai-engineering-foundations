# 2.4 Sampling (샘플링)

> "AI에서 가장 과소평가된 개념 중 하나"

[← 목차로 돌아가기](./README.md)

---

## 📋 목차
- [중요성](#중요성)
- [2.4.1 Sampling Fundamentals](#241-sampling-fundamentals-기초)
- [2.4.2 Sampling Strategies](#242-sampling-strategies-전략들)
- [2.4.3 Test Time Compute](#243-test-time-compute-테스트-시간-컴퓨팅)
- [2.4.4 Structured Outputs](#244-structured-outputs-구조화된-출력)
- [2.4.5 확률적 특성](#245-ai의-확률적-특성)
- [핵심 요약](#핵심-요약)
- [토론 질문](#토론-질문)

---

## 중요성

### 왜 샘플링이 중요한가?

**샘플링은:**
1. **Hallucination을 설명**
   - 왜 AI가 사실이 아닌 것을 말하는가
   - 확률적 선택의 결과

2. **일관성 없는 동작을 설명**
   - 같은 질문에 다른 답변
   - 랜덤성의 역할

3. **성능을 크게 향상**
   - 올바른 샘플링 전략
   - 상대적으로 적은 노력으로 큰 효과

**저자의 말**
> "샘플링은 아마도 AI에서 가장 과소평가된 개념 중 하나입니다."

---

## 2.4.1 Sampling Fundamentals (기초)

### 언어 모델의 출력

**핵심 개념**
```
언어 모델은 텍스트를 생성하는 게 아니라
확률 분포를 생성한다
```

**예시**
```
입력: "The cat is on the"

모델 출력 (확률 분포):
┌─────────┬──────────┐
│ Token   │ 확률     │
├─────────┼──────────┤
│ mat     │ 30%      │
│ floor   │ 25%      │
│ roof    │ 15%      │
│ table   │ 10%      │
│ chair   │ 5%       │
│ bed     │ 5%       │
│ window  │ 3%       │
│ ...     │ ...      │
└─────────┴──────────┘
```

### 샘플링의 역할

**정의**
> 확률 분포에서 실제 토큰을 **선택**하는 과정

**과정**
```
Step 1: 모델이 확률 분포 생성
       [mat: 30%, floor: 25%, roof: 15%, ...]

Step 2: 샘플링 전략 적용
       어떤 토큰을 선택할 것인가?

Step 3: 선택된 토큰 출력
       "mat"
```

**왜 중요한가?**
```
다른 샘플링 전략 = 완전히 다른 출력
```

---

## 2.4.2 Sampling Strategies (전략들)

### 1. Greedy Decoding (탐욕적 디코딩)

#### 방법
```
항상 가장 높은 확률의 토큰 선택
```

**예시**
```
확률 분포:
mat: 30% ← 선택!
floor: 25%
roof: 15%
...

다음 단계:
"The cat is on the mat"
다시 확률 분포:
and: 40% ← 선택!
is: 20%
...

결과: "The cat is on the mat and..."
```

#### 특징

**장점**
- ✅ 결정적 (deterministic)
  - 같은 입력 → 항상 같은 출력
- ✅ 빠름
- ✅ 재현 가능

**단점**
- ❌ 반복적
- ❌ 지루함
- ❌ 창의성 없음
- ❌ 다양성 없음

**실제 예시**
```
질문: "Tell me a story"

Greedy:
"Once upon a time, there was a cat. 
The cat was very happy. The cat lived 
in a house. The cat liked to play..."
→ 매우 반복적!
```

#### 사용 사례
```
✓ 기계 번역 (정확성 중요)
✓ 요약 (일관성 중요)
✓ 사실 기반 질문 답변
✗ 창의적 글쓰기
✗ 대화
✗ 스토리텔링
```

### 2. Temperature Sampling

#### 개념

**Temperature (T)**: 확률 분포의 "온도"
```
T = 0:   완전히 결정적 (Greedy와 동일)
T < 1:   확률 차이 증폭 (보수적)
T = 1:   원본 확률 유지
T > 1:   확률 평탄화 (창의적)
T → ∞:  완전 무작위
```

#### 수학적 표현

**조정된 확률**
```
P'(token_i) = exp(logit_i / T) / Σ exp(logit_j / T)
```

#### 실제 효과

**원본 확률**
```
mat:    30% ██████
floor:  25% █████
roof:   15% ███
table:  10% ██
chair:   5% █
```

**T = 0.5 (보수적)**
```
mat:    50% ██████████
floor:  30% ██████
roof:   12% ██
table:   5% █
chair:   3% ▌
```

**T = 2.0 (창의적)**
```
mat:    22% ████
floor:  21% ████
roof:   18% ███
table:  14% ███
chair:  10% ██
```

#### 실전 가이드

**Temperature 선택 기준**

| Temperature | 특성 | 사용 사례 |
|-------------|------|----------|
| **0.0** | 완전 결정적 | 정확한 번역, 코드 실행 |
| **0.1-0.3** | 매우 보수적 | 사실 기반 QA, 기술 문서 |
| **0.4-0.6** | 약간 보수적 | 비즈니스 이메일, 요약 |
| **0.7-0.9** | 균형잡힌 | 대화, 일반 글쓰기 |
| **1.0-1.5** | 창의적 | 스토리텔링, 브레인스토밍 |
| **1.5+** | 매우 창의적 | 시, 실험적 콘텐츠 |

**예시: 코드 생성**
```python
# Temperature = 0.2 (안전)
def add(a, b):
    return a + b

# Temperature = 0.7 (균형)
def add_numbers(x, y):
    result = x + y
    return result

# Temperature = 1.5 (창의적)
def combine_values(first_num, second_num):
    # Let's add these together!
    total = first_num + second_num
    print(f"Adding {first_num} and {second_num}")
    return total
```

### 3. Top-k Sampling

#### 방법
```
1. 확률이 높은 상위 k개 토큰만 선택
2. 나머지는 확률 0으로 설정
3. 재정규화
4. 샘플링
```

#### 예시: k = 3

**Step 1: 원본 확률**
```
mat:    30%
floor:  25%
roof:   15%
table:  10% ← 여기까지만
chair:   5% ✗
bed:     5% ✗
window:  3% ✗
...
```

**Step 2: 필터링**
```
mat:    30%
floor:  25%
roof:   15%
[나머지 제거]
```

**Step 3: 재정규화**
```
총합: 30 + 25 + 15 = 70

mat:    30/70 = 42.9%
floor:  25/70 = 35.7%
roof:   15/70 = 21.4%
```

**Step 4: 샘플링**
```
이 세 개 중에서만 선택
```

#### 장단점

**장점**
- ✅ 품질 낮은 토큰 제거
- ✅ 일관성 있는 출력
- ✅ Greedy보다 다양함

**단점**
- ❌ 고정된 k가 항상 적절하지 않음
- ❌ 상황에 따라 k가 달라야 함

**문제 상황**
```
상황 1: 매우 확실한 경우
mat: 95%, floor: 2%, roof: 1%, ...
→ k=10이면 너무 많은 저품질 토큰 포함

상황 2: 불확실한 경우
mat: 12%, floor: 11%, roof: 10%, table: 9%, ...
→ k=3이면 너무 적음, 다양성 손실
```

### 4. Top-p (Nucleus) Sampling

#### 방법
```
누적 확률이 p를 초과할 때까지 토큰 선택
→ 동적으로 후보 수 조정
```

#### 예시: p = 0.9

**확률 분포**
```
mat:    30%
floor:  25%
roof:   15%
table:  10%
chair:   5%
bed:     5%
window:  3%
...
```

**누적 계산**
```
mat:    30%  (누적: 30%)  ← 90% 미만, 포함
floor:  25%  (누적: 55%)  ← 90% 미만, 포함
roof:   15%  (누적: 70%)  ← 90% 미만, 포함
table:  10%  (누적: 80%)  ← 90% 미만, 포함
chair:   5%  (누적: 85%)  ← 90% 미만, 포함
bed:     5%  (누적: 90%)  ← 90% 도달, 포함
window:  3%  (누적: 93%)  ← 90% 초과, ✗ 제외
...
```

**선택: mat, floor, roof, table, chair, bed**

#### Top-k vs Top-p 비교

**시나리오 1: 확률이 집중된 경우**
```
확률:
Token 1: 70%
Token 2: 20%
Token 3: 5%
Token 4-10: 1% 각각

Top-k (k=10): 10개 모두 포함 (low-quality 포함)
Top-p (p=0.9): 2개만 포함 (70% + 20% = 90%)
→ Top-p가 더 좋음
```

**시나리오 2: 확률이 분산된 경우**
```
확률:
Token 1-10: 각 10%

Top-k (k=3): 3개만 (너무 제한적)
Top-p (p=0.9): 9개 포함
→ Top-p가 더 좋음
```

#### 권장 설정

**일반적 조합**
```python
temperature = 0.7
top_p = 0.9
```

**이유**
- Temperature: 약간 보수적
- Top-p: 동적 필터링
- 함께 사용: 균형잡힌 결과

**상황별 조정**
```python
# 매우 정확해야 할 때
temperature = 0.2
top_p = 0.95

# 창의적이어야 할 때
temperature = 0.9
top_p = 0.85

# 최대 창의성
temperature = 1.2
top_p = 0.8
```

### 5. Best-of-N Sampling

#### 방법
```
1. N개의 응답 생성 (예: N=5)
2. 각 응답을 Reward Model로 평가
3. 가장 높은 점수의 응답 선택
```

#### 예시

**N = 5로 5개 생성**
```
Response 1: "Paris is the capital..." 
Score: 8.5

Response 2: "The capital of France is Paris..."
Score: 9.2 ← 최고!

Response 3: "France's capital city is..."
Score: 7.8

Response 4: "Paris, which is located..."
Score: 8.1

Response 5: "As the capital, Paris..."
Score: 7.5
```

**선택: Response 2 (9.2점)**

#### 특징

**장점**
- ✅ 품질 크게 향상
- ✅ 최고 응답 선택
- ✅ Hallucination 감소

**단점**
- ❌ 비용 N배 증가
- ❌ 레이턴시 증가 (순차 생성 시)
- ❌ Reward Model 필요

#### 변형: Parallel Sampling

**순차 생성**
```
Response 1 생성 (1초)
→ Response 2 생성 (1초)
→ Response 3 생성 (1초)
→ Response 4 생성 (1초)
→ Response 5 생성 (1초)

총 시간: 5초
```

**병렬 생성**
```
Response 1, 2, 3, 4, 5 동시 생성 (1초)

총 시간: 1초 (레이턴시)
처리량: 1/5로 감소
```

#### 사용 시기

**적합한 경우**
- 품질이 최우선
- 비용 감당 가능
- 레이턴시 여유 있음
- Reward Model 존재

**부적합한 경우**
- 실시간 응답 필요
- 비용 제약 큼
- 단순 작업

---

## 2.4.3 Test Time Compute (테스트 시간 컴퓨팅)

### 개념

**전통적 접근**
```
성능 향상 = 더 많은 학습
         = 더 큰 모델
         = 더 많은 데이터
```

**Test Time Compute**
```
성능 향상 = 추론 시 더 많은 계산
         = 더 깊은 사고
         = 더 나은 추론
```

**핵심 아이디어**
> "학습뿐 아니라 추론에도 자원 투입"

### 주요 기법

#### 1. Chain-of-Thought (CoT)

**기본 응답**
```
질문: "123 × 456은?"

모델: "56,088"
```

**CoT 응답**
```
질문: "123 × 456은?"

모델: 
"먼저 문제를 단계별로 풀어봅시다.

Step 1: 123 × 6 = 738
Step 2: 123 × 50 = 6,150
Step 3: 123 × 400 = 49,200
Step 4: 738 + 6,150 + 49,200 = 56,088

따라서 답은 56,088입니다."
```

**효과**
- ✅ 복잡한 문제 정확도 향상
- ✅ 추론 과정 투명
- ✅ 에러 추적 가능
- ❌ 더 많은 토큰 생성 (느리고 비쌈)

#### 2. Self-Consistency

**방법**
```
1. 같은 문제에 대해 여러 CoT 경로 생성
2. 각 경로의 답 수집
3. 가장 많이 나온 답 선택
```

**예시**
```
질문: "존은 사과 5개를 가지고 있었고, 
      3개를 먹었습니다. 남은 사과는?"

경로 1: "5 - 3 = 2" ✓
경로 2: "5개 중 3개 먹음, 2개 남음" ✓
경로 3: "3 + 2 = 5, 따라서 2개" ✓
경로 4: "5 - 3 = 2" ✓
경로 5: "5개에서 3개 빼면 8개" ✗

다수결: 2개 (4/5)
```

**장점**
- 일관성 있는 답 선택
- 잘못된 추론 필터링

#### 3. Tree-of-Thought

**개념**
```
Linear (CoT):
Start → Step 1 → Step 2 → Step 3 → Answer

Tree (ToT):
            ┌─ Step 1a → Step 2a → Answer A
Start ─────┼─ Step 1b → Step 2b → Answer B
            └─ Step 1c ✗ (막힘, 백트랙)
```

**특징**
- 여러 추론 경로 탐색
- 막힌 경로는 백트랙
- 가장 유망한 경로 선택

#### 4. Process Reward Models

**기존: Outcome Reward**
```
최종 답만 평가
"2" → 맞음/틀림
```

**Process Reward**
```
각 추론 단계 평가

Step 1: "5 - 3을 계산" → 올바름 ✓
Step 2: "2를 얻음" → 올바름 ✓
Step 3: "따라서 2개" → 올바름 ✓

vs

Step 1: "5 + 3을 계산" → 틀림 ✗
```

**장점**
- 올바른 추론 과정 유도
- 우연히 맞는 답 방지

### OpenAI o1 (2024년 9월)

#### 특징

**"Reasoning Tokens"**
```
사용자에게 보이지 않는 내부 추론

Input: 복잡한 수학 문제

[Internal Reasoning - 보이지 않음]
"이 문제는 미분방정식이다..."
"먼저 경계 조건을 확인..."
"치환을 통해..."
[수십~수백 개의 추론 토큰]

Output: 최종 답변만 표시
```

#### 성능

**수학**
```
AIME (수학 경시대회):
- GPT-4: 13.4%
- o1: 83.3%

IMO (국제 수학 올림피아드):
- 금메달 수준 문제 해결
```

**코딩**
```
Codeforces:
- 89 percentile
- 경쟁 프로그래밍 대회 수준
```

**과학**
```
PhD-level 과학 질문:
- 물리, 화학, 생물 벤치마크
- 전문가 수준 답변
```

#### 트레이드오프

**장점**
- 🚀 훨씬 높은 정확도
- 🧠 복잡한 추론 가능
- 🎯 전문가 수준 문제 해결

**단점**
- 🐌 느림 (더 많은 토큰)
- 💰 비쌈 (더 많은 계산)
- ⏱️ 레이턴시 높음

**사용 시기**
```
✓ 복잡한 수학/과학 문제
✓ 코딩 챌린지
✓ 다단계 추론 필요
✓ 정확도가 최우선
✓ 시간 여유 있음

✗ 단순 질문
✗ 실시간 응답 필요
✗ 비용 민감
```

### Test Time Compute의 미래

**트렌드**
```
Training Compute ↓ (상대적으로)
Test Time Compute ↑

이유:
- 모델 크기 한계
- 데이터 한계
- 추론이 더 유연
```

**의미**
- 더 작은 모델 + 더 많은 추론
- 동적 난이도 조절
- 사용자별 맞춤 계산

---

## 2.4.4 Structured Outputs (구조화된 출력)

### 문제: 유효하지 않은 출력

**요청**
```
"JSON 형식으로 답변해줘"
```

**응답 (문제)**
```json
{
  "name": "Alice"
  "age": 30  // ← 쉼표 누락!
}
```

**파싱 시도**
```python
import json
json.loads(response)
# JSONDecodeError!
```

**현실**
- 형식 오류 빈번
- 후처리 필요
- 불완전한 JSON/XML
- 문법 규칙 위반

### 해결책: Constrained Decoding

#### 개념
```
각 샘플링 단계에서
문법적으로 유효한 토큰만 허용
```

#### 작동 방식

**Step 1: 스키마 정의**
```json
{
  "type": "object",
  "properties": {
    "name": {"type": "string"},
    "age": {"type": "integer"}
  },
  "required": ["name", "age"]
}
```

**Step 2: 토큰별 검증**
```
현재: {
       "name": "Alice"

다음 토큰 후보:
- "," ✓ (유효)
- "}" ✗ (age 누락)
- "a" ✗ (문법 위반)

강제 선택: ","
```

**Step 3: 계속 제약**
```
현재: {
       "name": "Alice",
       
다음은 반드시:
- "age" (required field)

기타 토큰 확률 = 0
```

**결과: 100% 유효한 출력**
```json
{
  "name": "Alice",
  "age": 30
}
```

### 지원 형식

**1. JSON**
```json
{
  "user": {
    "id": 123,
    "email": "user@example.com"
  }
}
```

**2. XML**
```xml
<user>
  <id>123</id>
  <email>user@example.com</email>
</user>
```

**3. SQL**
```sql
SELECT name, age 
FROM users 
WHERE age > 18;
```

**4. 정규표현식**
```
패턴: \d{3}-\d{3}-\d{4}
출력: 123-456-7890 (전화번호)
```

**5. Context-Free Grammar (CFG)**
```
S → NP VP
NP → Det N
VP → V NP
...
```

### 도구

**Outlines**
```python
from outlines import generate

schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer"}
    }
}

generator = generate.json(model, schema)
result = generator(prompt)
# 항상 유효한 JSON
```

**Guidance**
```python
from guidance import models, gen

model = models.LlamaCpp(...)

result = model + f'''
{{
  "name": {gen("name", max_tokens=20)},
  "age": {gen("age", regex="[0-9]+")}
}}
'''
```

**LMQL**
```python
"Return JSON: " \
"""
{
    "name": "[STRING]",
    "age": "[INT]"
}
""" where INT >= 0 and INT <= 150
```

**jsonformer**
- JSON에 특화
- 매우 빠름
- 간단한 API

### 장단점

**장점**
- ✅ 100% 유효한 출력 보장
- ✅ 후처리 불필요
- ✅ 파싱 오류 제거
- ✅ API 통합 쉬움

**단점**
- ❌ 추가 계산 오버헤드
- ❌ 약간의 레이턴시 증가
- ❌ 복잡한 구현
- ❌ 모든 모델 지원 안 됨

### 사용 사례

**API 응답**
```python
# 보장된 JSON 구조
{
  "status": "success",
  "data": [...],
  "timestamp": "2026-01-09T..."
}
```

**데이터베이스 쿼리**
```sql
-- 항상 유효한 SQL
SELECT * FROM users WHERE id = 123;
```

**구조화된 데이터 추출**
```json
{
  "company": "Acme Corp",
  "revenue": 1000000,
  "year": 2025
}
```

---

## 2.4.5 AI의 확률적 특성

### 근본적 특성

**핵심 진실**
> "AI의 모든 출력은 본질적으로 확률적이다"

**의미**
```
같은 입력 → 다른 출력 (가능)

Temperature > 0일 때:
- 완벽한 재현 불가능
- 랜덤성이 특징, 버그 아님
```

### Hallucination의 본질

#### 왜 발생하는가?

**1. 학습 데이터에 없는 정보**
```
질문: "2025년 FIFA 월드컵 우승국은?"

모델: "브라질이 우승했습니다"
실제: 2025년에는 월드컵 없음

이유: 패턴 기반 예측, 사실 확인 안 함
```

**2. 낮은 확률이지만 선택됨**
```
확률:
- "I don't know": 60%
- "Brazil": 30%
- "Germany": 5%
- "Japan": 3%
- "Atlantis": 0.1% ← 샘플링으로 선택될 수 있음!
```

**3. 자기회귀적 오류 누적**
```
Token 1: 약간 잘못됨 (90% 확률)
   ↓
Token 2: Token 1 기반 (더 잘못됨)
   ↓
Token 3: Token 1+2 기반 (매우 잘못됨)
   ↓
완전히 허구의 이야기
```

#### 완화 방법

**1. Temperature 낮추기**
```python
temperature = 0.1  # 매우 보수적
# 가장 확률 높은 것만 선택
```

**2. Top-p 작게**
```python
top_p = 0.9  # 상위 90%만
# 저품질 토큰 제거
```

**3. RAG (Retrieval-Augmented Generation)**
```
1. 관련 문서 검색
2. 문서를 context로 제공
3. 문서 기반 답변 생성
→ 사실 기반 답변
```

**4. 여러 응답 + 검증**
```
1. N개 응답 생성
2. 사실 확인
3. 검증된 것만 사용
```

**5. 명시적 불확실성 표현**
```
"I don't have information about..."
"Based on my training data up to..."
"I'm not certain, but..."
```

### Randomness vs Determinism

#### Temperature = 0

**특성**
```
- 완전 결정적
- 재현 가능
- 같은 입력 → 같은 출력
- 창의성 없음
```

**사용 사례**
- 단위 테스트
- 벤치마크
- 디버깅
- 일관성 필요

#### Temperature > 0

**특성**
```
- 확률적
- 다양한 출력
- 완전 재현 불가능*
- 창의적
```

***주의**: Seed 고정해도 하드웨어 차이로 약간 다를 수 있음

**사용 사례**
- 창의적 작업
- 대화
- 브레인스토밍
- 다양한 아이디어

### 실무적 시사점

#### 1. 평가 시

**단일 실행 ≠ 정확한 평가**
```python
# 잘못된 방법
score = evaluate(model, test_set)

# 올바른 방법
scores = []
for _ in range(10):
    score = evaluate(model, test_set)
    scores.append(score)
    
average = mean(scores)
std_dev = std(scores)
```

**최악의 경우 고려**
```python
worst_case = min(scores)
# 이것도 발생할 수 있음!
```

#### 2. 프로덕션 시

**태스크별 Temperature**
```python
TEMPERATURES = {
    "translation": 0.1,      # 매우 정확
    "code_generation": 0.2,  # 보수적
    "email": 0.5,            # 약간 보수적
    "chat": 0.7,             # 균형
    "story": 1.0,            # 창의적
    "poetry": 1.3            # 매우 창의적
}
```

**항상 검증**
```python
response = model.generate(prompt)

# 검증 로직
if not is_valid(response):
    # 재시도 또는 fallback
    response = fallback_response
```

#### 3. 사용자 경험

**기대치 설정**
```
❌ "AI가 항상 완벽한 답을 줍니다"
✓ "AI가 대부분의 경우 도움이 될 수 있습니다"
```

**"다시 시도" 기능**
```python
if user_not_satisfied:
    regenerate_with_different_sample()
```

**신뢰도 표시**
```python
response = {
    "answer": "...",
    "confidence": 0.85  # 85% 확신
}
```

#### 4. 에러 처리

**Graceful Degradation**
```python
try:
    response = model.generate(prompt, temp=0.7)
    if not validate(response):
        # 더 보수적으로 재시도
        response = model.generate(prompt, temp=0.3)
except:
    # Fallback
    response = template_response
```

---

## 핵심 요약

### Sampling Fundamentals
1. **모델은 확률 분포 생성**
   - 텍스트가 아닌 확률
   - 샘플링이 실제 토큰 선택

2. **다른 전략 = 다른 출력**
   - 샘플링이 성능에 큰 영향
   - 상대적으로 쉬운 최적화

### Sampling Strategies

| 전략 | 특징 | 사용 사례 |
|------|------|----------|
| **Greedy** | 결정적, 반복적 | 번역, 요약 |
| **Temperature** | 창의성 조절 | 대부분의 경우 |
| **Top-k** | 고정된 후보 수 | 간단한 필터링 |
| **Top-p** | 동적 후보 수 | 권장 (일반) |
| **Best-of-N** | 최고 선택 | 품질 최우선 |

### 권장 설정
```python
# 일반 대화
temperature = 0.7
top_p = 0.9

# 정확성 중요
temperature = 0.2
top_p = 0.95

# 창의성 중요
temperature = 0.9
top_p = 0.85
```

### Test Time Compute
1. **추론에도 자원 투입**
   - CoT: 단계별 사고
   - Self-Consistency: 다수결
   - Tree-of-Thought: 경로 탐색
   - Process Reward: 단계별 평가

2. **OpenAI o1**
   - Reasoning tokens
   - 83.3% on AIME
   - 느리지만 정확

3. **트레이드오프**
   - 정확도 ↑, 속도 ↓, 비용 ↑

### Structured Outputs
1. **Constrained Decoding**
   - 문법 규칙 강제
   - 100% 유효한 출력
   - JSON, XML, SQL 등

2. **도구들**
   - Outlines, Guidance, LMQL, jsonformer

### 확률적 특성
1. **본질적으로 확률적**
   - Hallucination은 특성
   - 완벽한 일관성 불가능

2. **실무 대응**
   - Temperature 조절
   - 여러 번 평가
   - 검증 메커니즘
   - 사용자 기대치 관리

---

## 토론 질문

### 샘플링 전략
1. 실제 프로덕션에서 어떤 전략 조합이 최선인가?
2. Temperature와 Top-p, 하나만 써야 한다면?
3. Best-of-N의 비용 대비 효과는?

### Test Time Compute
1. Test Time Compute의 ROI는?
2. o1 같은 모델이 모든 걸 대체할까?
3. 어떤 태스크가 Test Time Compute에 적합한가?

### Hallucination
1. Hallucination을 완전히 없앨 수 있을까? 없애야 할까?
2. 사용자에게 불확실성을 어떻게 전달해야 할까?
3. RAG vs 파인튜닝, 어느 것이 더 효과적인가?

### 확률적 특성
1. AI의 확률적 특성을 사용자에게 어떻게 설명할까?
2. 결정적 출력이 필요한 경우는?
3. 재현 가능성 vs 다양성, 어떻게 균형을?

### 실무적 고려
1. 우리 서비스에 최적인 Temperature는?
2. 샘플링 파라미터를 사용자에게 노출해야 할까?
3. A/B 테스트로 샘플링 전략을 어떻게 평가할까?

---

## 실습 아이디어

### 초급
1. **Temperature 실험**
   - 같은 프롬프트, 다른 temperature
   - 0.1, 0.5, 1.0, 1.5로 각 10번 생성
   - 다양성과 품질 비교

2. **Top-k vs Top-p**
   - 같은 작업에 두 전략 적용
   - 출력 비교 분석

### 중급
1. **Best-of-N 구현**
   - 5개 응답 생성
   - 간단한 scoring 함수 작성
   - 최고 선택

2. **Hallucination 분석**
   - 사실 확인 가능한 질문 100개
   - 다양한 temperature로 테스트
   - Hallucination 빈도 측정

### 고급
1. **Constrained Decoding**
   - Outlines나 Guidance 사용
   - 복잡한 JSON 스키마 정의
   - 성공률 100% 검증

2. **최적 샘플링 찾기**
   - Grid search (temp, top_p)
   - 태스크별 최적 조합 발견
   - 프로덕션 적용

### 프로젝트
1. **Adaptive Sampling**
   - 질문 복잡도 분석
   - 자동으로 temperature 조절
   - 성능 평가

2. **Multi-Strategy Ensemble**
   - 여러 샘플링 전략 사용
   - 결과 통합 (투표, 평균 등)
   - 단일 전략과 비교

---

## 참고 자료

### 논문
- **Temperature**: "The Curious Case of Neural Text Degeneration"
- **Top-p/Nucleus**: "Nucleus Sampling" (Holtzman et al., 2019)
- **Test Time Compute**: OpenAI o1 System Card
- **Constrained Decoding**: Outlines paper

### 도구
- **Outlines**: https://github.com/outlines-dev/outlines
- **Guidance**: https://github.com/guidance-ai/guidance
- **LMQL**: https://lmql.ai
- **jsonformer**: https://github.com/1rgs/jsonformer

### 블로그/가이드
- OpenAI Best Practices for Sampling
- Anthropic Claude Sampling Guide
- Hugging Face Generation Parameters

---

**이전**: [← 2.3 Post-Training](./2.3-post-training.md)

[← 목차로 돌아가기](./README.md)

---

**🎉 Chapter 2 완료!**

이제 Foundation Models의 핵심을 모두 이해했습니다:
- Training Data: 무엇을 학습하는가
- Modeling: 어떻게 구성되는가
- Post-Training: 어떻게 안전하게 만드는가
- Sampling: 어떻게 출력을 생성하는가

다음 Chapter에서 만나요! 🚀
