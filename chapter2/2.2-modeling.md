# 2.2 Modeling (모델링)

> "Attention Is All You Need"

[← 목차로 돌아가기](./README.md)

---

## 📋 목차
- [핵심 개념](#핵심-개념)
- [2.2.1 Model Architecture](#221-model-architecture-모델-아키텍처)
- [2.2.2 Model Size](#222-model-size-모델-크기)
- [2.2.3 Scaling Law](#223-scaling-law-스케일링-법칙)
- [핵심 요약](#핵심-요약)
- [토론 질문](#토론-질문)

---

## 핵심 개념

모델을 학습시키기 전에 **모델 아키텍처**와 **크기**를 결정해야 합니다. 이러한 결정은:
- 모델의 능력에 영향
- 배포 용이성에 영향
- 최적화 방법에 영향

---

## 2.2.1 Model Architecture (모델 아키텍처)

### Transformer의 지배

**현재 상황** (2026년 기준)
- 대부분의 언어 기반 Foundation Model이 Transformer 사용
- 2017년 등장 이후 9년째 지배적 위치
- "Attention Is All You Need" 논문이 시작점

**역사적 맥락**
- 2012: AlexNet (CNN 르네상스)
- 2014-2018: Seq2seq (RNN 기반) 전성기
- 2014-2019: GAN 인기
- 2017-현재: Transformer 시대

### Seq2seq의 등장과 한계

#### Seq2seq Architecture (2014)

**구조**
```
Encoder (RNN) → [Hidden State] → Decoder (RNN)
     ↓                                ↓
   Input                            Output
```

**성공 사례**
- 2016년 Google Translate에 도입
- "기계 번역 품질의 역대 최대 개선"
- 기계 번역과 요약에서 획기적 성과

#### 두 가지 근본적 한계

**1. 정보 병목 (Information Bottleneck)**
```
전체 입력 → 단일 Hidden State → 출력 생성

비유:
책 전체 → 책 요약본만 → 책에 대한 질문 답변
```
- 모든 입력 정보를 하나의 hidden state에 압축
- 긴 시퀀스에서 정보 손실 불가피

**2. 순차 처리 (Sequential Processing)**
```
Input:  Token1 → Token2 → Token3 → ... → Token200
         ↓        ↓        ↓               ↓
       처리1 → 처리2 → 처리3 → ... → 처리200
```
- 각 토큰을 하나씩 처리해야 함
- 200 토큰 입력 = 200번의 순차 연산
- 병렬화 불가능 → 느림

**RNN의 추가 문제**
- Vanishing/Exploding Gradients
- 긴 의존성 학습 어려움

### Transformer의 해결책

#### 핵심: Attention Mechanism

**기본 아이디어**
> "모든 이전 토큰을 참조하여 다음 토큰 생성"

```
책 전체 → 모든 페이지 직접 참조 → 질문 답변

vs

책 전체 → 요약본만 → 질문 답변
```

#### Attention의 작동 방식

**세 가지 핵심 벡터**

1. **Query (Q)**: "무엇을 찾고 있는가"
   - 현재 디코더의 상태
   - "정보를 찾는 사람"

2. **Key (K)**: "어디에 있는가"
   - 각 이전 토큰의 표현
   - "페이지 번호"

3. **Value (V)**: "실제 내용은 무엇인가"
   - 각 이전 토큰의 실제 값
   - "페이지 내용"

**수학적 표현**
```
K = x · W_K
V = x · W_V
Q = x · W_Q

Attention(Q,K,V) = softmax(QK^T / √d) · V
```

**예시: "How are you?" 다음 토큰 생성**
```
Query: 다음에 올 토큰 찾기
Keys: [How, are, you, ?]의 표현
Values: 각 단어의 의미 벡터

Attention 계산:
- "How"에 대한 attention: 0.1
- "are"에 대한 attention: 0.3
- "you"에 대한 attention: 0.5
- "?"에 대한 attention: 0.1

→ "you"에 가장 많은 attention
→ 다음 토큰: "I'm" (높은 확률)
```

#### Multi-Headed Attention

**개념**
- 여러 "관점"에서 동시에 attention 계산
- 각 head가 다른 패턴 학습

**예시: Llama 2-7B**
```
모델 차원: 4096
Attention heads: 32
각 head 차원: 4096 / 32 = 128

Q, K, V 각각: 4096 → 32개의 128차원 벡터로 분할
```

**장점**
- 동시에 여러 정보 유형 추출
- 예: Head 1은 문법, Head 2는 의미, Head 3은 문맥...

### Transformer Block 구조

#### 주요 구성 요소

**1. Attention Module**
```
구성:
- Query Matrix (W_Q): 4096 × 4096
- Key Matrix (W_K): 4096 × 4096
- Value Matrix (W_V): 4096 × 4096
- Output Projection (W_O): 4096 × 4096
```

**2. MLP (Multi-Layer Perceptron) Module**
```
Linear Layer → Activation Function → Linear Layer

예시 (Llama 2-7B):
4096 → 11,008 → 4096
```

**Activation Functions**
- **ReLU**: `ReLU(x) = max(0, x)`
  - 음수 → 0으로 변환
  - 간단하고 빠름
  
- **GELU**: GPT-2, GPT-3 사용
  - 더 부드러운 비선형성
  - ReLU보다 약간 더 복잡

**왜 간단한 함수를 사용하는가?**
- 복잡한 활성화 함수 ≠ 더 나은 성능
- 빠른 계산이 더 중요
- 비선형성을 제공하기만 하면 됨

#### 전체 모델 구조

```
Input Text
    ↓
[Embedding Module]
- Token Embedding
- Positional Embedding
    ↓
[Transformer Block 1]
- Multi-Head Attention
- MLP
    ↓
[Transformer Block 2]
    ↓
    ...
    ↓
[Transformer Block N]
    ↓
[Output Layer / Unembedding]
    ↓
Token Probabilities
```

**Llama 2/3 모델 비교**

| 모델 | Blocks | Model Dim | FFN Dim | Vocab Size | Context Length |
|------|--------|-----------|---------|------------|----------------|
| **Llama 2-7B** | 32 | 4,096 | 11,008 | 32K | 4K |
| **Llama 2-13B** | 40 | 5,120 | 13,824 | 32K | 4K |
| **Llama 2-70B** | 80 | 8,192 | 22,016 | 32K | 4K |
| **Llama 3-8B** | 32 | 4,096 | 14,336 | 128K | 128K |
| **Llama 3-70B** | 80 | 8,192 | 28,672 | 128K | 128K |
| **Llama 3-405B** | 126 | 16,384 | 53,248 | 128K | 128K |

**관찰**:
- Llama 3: 더 큰 FFN, 더 큰 vocabulary, 32배 긴 context
- Context length는 파라미터 수에 포함 안 됨 (메모리에만 영향)

### Transformer의 Inference 과정

#### 1. Prefill Phase (병렬 처리)
```
Input: "What is the capital of France?"
       [What] [is] [the] [capital] [of] [France] [?]
         ↓      ↓     ↓      ↓       ↓      ↓      ↓
    [모든 토큰을 동시에 병렬 처리]
         ↓
    Key, Value 벡터 생성 및 저장
```
- 모든 입력 토큰을 **병렬**로 처리
- KV cache 생성 (메모리에 저장)
- 첫 출력 토큰 생성 준비

#### 2. Decode Phase (순차 처리)
```
생성: [The] → [capital] → [of] → [France] → [is] → [Paris]
       ↓        ↓          ↓        ↓         ↓        ↓
    토큰1 생성 → 토큰2 생성 → ... (하나씩 순차적으로)
```
- 출력 토큰을 **순차적**으로 생성
- 각 토큰 생성 시 이전 모든 토큰 참조
- Autoregressive: 이전 출력이 다음 입력

**시사점**:
- Prefill: 병렬화 가능 → 빠름
- Decode: 순차적 → 상대적으로 느림
- 최적화 전략이 단계별로 다름

### 대안 아키텍처

#### 새 아키텍처 개발의 어려움

**장벽들**
1. **Transformer는 9년간 최적화됨**
   - 수많은 연구와 엔지니어링
   - 하드웨어 최적화 (특히 GPU, TPU)
   
2. **대규모 증명 필요**
   - 소규모 실험 성공 ≠ 실제 채택
   - 수백억 파라미터 규모에서 증명해야
   
3. **생태계 효과**
   - 기존 도구, 라이브러리, 인프라
   - 전환 비용이 높음

**Ilya Sutskever의 통찰**
> "Neural network는 많은 프로그램을 시뮬레이션할 수 있다.  
> Gradient descent는 이 프로그램들 중 최적을 찾는 검색 알고리즘이다.  
> 새 아키텍처가 기존 것을 이기려면, 기존 것이 시뮬레이션 못 하는 프로그램을 실행해야 한다."

#### RWKV (RNN-based)

**특징**
```
RNN + 병렬 학습 가능
```
- RNN의 장점: 이론상 무한 context length
- Transformer의 장점: 학습 시 병렬화 가능

**장점**:
- 고정된 context length 제한 없음
- 메모리 효율적

**한계**:
- 이론적 무한 context ≠ 실제 긴 context에서 좋은 성능
- 실제 성능은 검증 필요

#### State Space Models (SSM)

**핵심 아이디어**
- 긴 시퀀스 처리에 특화
- 선형 시간 복잡도 (vs Transformer의 제곱)

**진화 과정**

**S4 (2021)**
```
"Efficiently Modeling Long Sequences with Structured State Spaces"
```
- SSM을 더 효율적으로
- 긴 시퀀스 학습 개선

**H3 (2022)**
```
"Hungry Hungry Hippos"
```
- 초기 토큰 회상 메커니즘
- 시퀀스 간 토큰 비교
- Attention과 유사한 역할, 더 효율적

**Mamba (2023)**
```
"Linear-Time Sequence Modeling with Selective State Spaces"
```
- 3B 파라미터까지 확장
- 성능:
  - Mamba-3B > Transformer-3B
  - Mamba-3B ≈ Transformer-6B
- **선형 시간**: O(n) vs Transformer의 O(n²)
- 백만 길이 시퀀스까지 검증

**Jamba (2024)**
```
"Hybrid Transformer–Mamba Language Model"
```
- Transformer + Mamba 블록 혼합
- 52B 총 파라미터 (12B 활성)
- 단일 80GB GPU에 fit
- 256K 토큰 context 지원
- 작은 메모리 footprint

**아키텍처 비교**
```
Transformer Block:
[Attention] → [MLP]

Mamba Block:
[Selective SSM]

Jamba Block:
[Attention] → [Mamba] → [Attention] → [Mamba] → ...
```

#### 미래 전망

**Transformer를 대체할 가능성**
- ✅ SSM 계열이 가장 유망
- ✅ 긴 context 처리에 강점
- ✅ 효율성 (속도, 메모리)
- ❓ 범용성은 검증 필요

**하지만**
- 근본적인 AI 원칙은 변하지 않음
- 데이터, 평가, 최적화 접근법은 유사
- ML Engineering → AI Engineering과 비슷한 전환

---

## 2.2.2 Model Size (모델 크기)

### 파라미터 수의 의미

**기본 개념**
```
Model Size = Number of Parameters
```
- Llama-7B = 70억 개 파라미터
- Llama-13B = 130억 개 파라미터
- Llama-70B = 700억 개 파라미터

**일반적 규칙**
> 파라미터 수 ↑ = 학습 능력 ↑ = 성능 ↑

**같은 모델 패밀리 내에서**:
```
Llama-13B >> Llama-7B
Llama-70B >> Llama-13B
```

### 메모리 계산

**기본 공식**
```
최소 메모리 = 파라미터 수 × bytes per parameter

예시 (FP16 사용):
Llama-7B = 7B × 2 bytes = 14GB
Llama-70B = 70B × 2 bytes = 140GB
```

**실제 메모리는 더 많이 필요**:
- Optimizer states
- Gradients
- Activations
- KV cache
- → 자세한 내용은 Chapter 7

### 세대 차이

**중요한 발견**
> 같은 크기라도 신세대 > 구세대

**예시**:
```
MMLU 벤치마크:
Llama 3-8B (2024) > Llama 2-70B (2023)

8B 모델이 70B 모델을 능가!
```

**이유**:
- 더 나은 학습 기법
- 더 좋은 데이터
- 개선된 아키텍처 디테일

### Sparse Models: Mixture-of-Experts (MoE)

#### 기본 개념

**Dense Model**
```
모든 파라미터가 항상 활성
7B 파라미터 = 7B 모두 사용
```

**Sparse Model (MoE)**
```
많은 파라미터가 0이거나 선택적으로만 활성
46.7B 파라미터 but 12.9B만 활성 (토큰당)
```

#### Mixtral 8x7B 예시

**구조**
```
8개의 Expert (각 7B 파라미터)
└─ Expert 1: 7B
└─ Expert 2: 7B
└─ Expert 3: 7B
└─ ...
└─ Expert 8: 7B
─────────────
총: 56B
- 공유 파라미터 고려: 46.7B
```

**작동 방식**
```
각 토큰마다:
- Router가 2개의 expert 선택
- 선택된 expert만 활성화
- 2 × 7B = 14B 사용 (실제는 12.9B)
```

**장점**
```
모델 크기: 46.7B (큰 용량)
실제 비용: 12.9B (빠르고 저렴)
```

### 데이터 크기의 중요성

**극단적 예시**
```
모델 A: 13B 파라미터, 1개 문장 학습
  "I like pineapples."

모델 B: 1B 파라미터, 1조 토큰 학습
  다양한 인터넷 데이터

결과: B >>> A
```

**시사점**
> 파라미터 수만으로 성능 판단 불가  
> 학습 데이터 양과 질도 critical

### Training Tokens 측정

**왜 토큰으로 측정하는가?**

**문제: 샘플 수는 부정확**
```
샘플 1: "Hello" (1 문장)
샘플 2: "War and Peace" (책 전체, ~67,000 토큰)

→ 두 샘플의 가치가 천차만별
```

**해결: 토큰 수**
- 토큰 = 모델이 연산하는 단위
- 데이터셋 크기의 정확한 측정

**주요 모델의 학습 토큰**

| 모델 | 파라미터 | 학습 토큰 |
|------|---------|----------|
| LaMDA | 137B | 168B |
| GPT-3 | 175B | 300B |
| Jurassic | 178B | 300B |
| Gopher | 280B | 300B |
| MT-NLG 530B | 530B | 270B |
| **Chinchilla** | 70B | **1.4T** |
| **Llama 1** | - | 1.4T |
| **Llama 2** | - | 2T |
| **Llama 3** | - | **15T** |

**트렌드**:
- 초기 모델들: 수백 B 토큰
- 최근 모델들: 수 T ~ 수십 T 토큰
- 점점 더 많은 데이터로 학습

**Epoch**
```
1 epoch = 전체 데이터셋 1회 통과

데이터셋: 1T 토큰
2 epoch 학습 → 2T 학습 토큰
```

현대 LLM은 대부분 **1 epoch만** 학습 (데이터가 너무 많아서)

### FLOPs: 계산 비용 측정

#### FLOPs vs FLOP/s

**FLOPs** (Floating Point Operations)
```
= 작업을 완료하는데 필요한 총 연산 수
= 계산 요구량
```

**FLOP/s** (FLOPs per Second)
```
= 초당 수행할 수 있는 연산 수
= 하드웨어 성능
```

**혼란 주의**
```
FLOPs (복수) ≠ FLOPS (per second)
```

**OpenAI 표기법**
```
FLOP/s-day = 하루 동안의 FLOPs
1 FLOP/s-day = 86,400 FLOPs
```

#### 계산 예시: GPT-3 학습

**학습 비용**
```
GPT-3: 3.14 × 10²³ FLOPs
```

**하드웨어 스펙**
```
NVIDIA H100 NVL:
- 60 TeraFLOP/s = 6 × 10¹³ FLOP/s
- 하루: 5.2 × 10¹⁸ FLOPs
```

**필요 시간 계산**
```
H100 256개 사용:
= 256 × 5.2 × 10¹⁸ FLOPs/day
= 1.33 × 10²¹ FLOPs/day

소요 시간:
= 3.14 × 10²³ / 1.33 × 10²¹
= 236일 (약 7.8개월)
```

**비용 계산**
```
가정:
- H100: $2/hour
- 활용률: 70%
- 256 GPUs × 24 hours × 236 days

총 비용: $4,142,811 (약 $4.14M)
```

**현실적 고려사항**

**Utilization (활용률)**
```
이론적 최대 성능 대비 실제 사용 비율

50% = OK
70% = Great
> 70% = Excellent
```

**왜 100% 안 되는가?**
- I/O 대기
- 동기화 오버헤드
- 메모리 전송
- 디버깅 및 체크포인팅

### 모델 크기 결정의 세 가지 수치

**1. Parameters (파라미터 수)**
```
= 모델의 학습 용량
= 메모리 요구량의 기준
```

**2. Training Tokens (학습 토큰 수)**
```
= 모델이 얼마나 배웠는가
= 데이터셋 크기의 척도
```

**3. FLOPs (연산 수)**
```
= 학습 비용
= 시간과 돈의 척도
```

---

## 2.2.3 Scaling Law (스케일링 법칙)

### Inverse Scaling: 더 큰 게 더 나쁠 때

**Anthropic의 발견 (2022)**
- 더 많은 alignment 학습 → 덜 aligned?
- 큰 모델이 특정 정치/종교 관점 더 강하게 표현
- 자의식과 도덕적 자기 가치 주장
- 종료되기를 원하지 않는다고 표현

**Inverse Scaling Prize (2023)**
- NYU 주도, $125K 상금
- 99개 제출, 11개 3등상 수상
- 2등상, 1등상은 수상자 없음
- 이유: 작은 테스트셋에서만 실패, 실제로는 문제없음

**결론**
> 대부분의 경우 bigger = better  
> 하지만 항상은 아님

### Chinchilla Scaling Law

#### 동기

**세 가지 확실한 사실**
1. 성능 = f(모델 크기, 데이터 크기)
2. 더 큰 모델, 더 많은 데이터 = 더 많은 계산
3. 계산 = 돈

**핵심 질문**
> 고정된 계산 예산으로 최고 성능을 내려면?

#### 연구 방법

**DeepMind 실험 (2022)**
```
400개의 언어 모델 학습:
- 크기: 70M ~ 16B 파라미터
- 데이터: 5B ~ 500B 토큰

모든 조합 테스트 → 최적 비율 발견
```

#### 핵심 법칙

**Chinchilla Law**
```
최적 학습 토큰 수 ≈ 파라미터 수 × 20
```

**예시**
```
3B 파라미터 모델
→ 60B 학습 토큰 필요

70B 파라미터 모델
→ 1.4T 학습 토큰 필요
```

**스케일링 원칙**
```
모델 2배 확대 → 데이터도 2배 확대
```

#### 예측 가능성

**놀라운 발견**
- 주어진 FLOP 예산으로 최적 모델 크기 예측 가능
- 최적 토큰 수 예측 가능
- **예상 Loss까지 예측 가능**

**시사점**
> AI 학습이 더 이상 연금술이 아님  
> 과학적으로 예측 가능한 공학

#### 실제 적용

**많은 모델이 Chinchilla Law 미준수**

```
GPT-3 (175B):
- 실제: 300B 토큰
- 최적: 3.5T 토큰
→ Under-trained!

Gopher (280B):
- 실제: 300B 토큰
- 최적: 5.6T 토큰
→ Under-trained!
```

**왜 최적이 아닌 모델을 만드는가?**

**Llama의 선택**
```
Chinchilla Law대로라면:
→ 더 큰 모델, 더 나은 성능

Llama의 실제 선택:
→ 작은 모델, 약간 낮은 성능
→ But: 배포 쉬움, 추론 저렴
→ 결과: 훨씬 넓은 adoption
```

**실용적 고려사항**
1. **학습 비용** (FLOP 예산)
2. **추론 비용** (사용 시 비용)
3. **배포 용이성** (메모리, 레이턴시)
4. **사용자 수요** (얼마나 많이 쓰일까)

**수정된 Scaling Law (Sardana et al., 2023)**
```
추론 수요를 고려한 최적 모델 크기 계산
→ 더 현실적인 선택 가능
```

### 성능 개선의 비용

#### 좋은 소식: 같은 성능이 저렴해짐

**ImageNet 93% 정확도 비용**
```
2019: $X
2021: $X/2 (50% 감소)
```

출처: Stanford HAI AI Index Report 2022

#### 나쁜 소식: 개선은 비싸짐

**Last Mile Challenge**
```
85% → 90%: $100K
90% → 95%: $1M (10배)
95% → 98%: $10M (100배)
```

**언어 모델 Loss**
```
3.4 nats → 2.8 nats
= 10배 더 많은 데이터 필요
```

**비전 모델**
```
1B → 2B 학습 샘플 (2배 증가)
= ImageNet 정확도 몇 %p 개선
```

**Meta의 연구 (Data Pruning)**
> "2% 오류율 모델 vs 3% 오류율 모델  
> = 계산/데이터/에너지 **10배 차이**"

#### 하지만...

**작은 성능 차이 = 큰 품질 차이**
```
언어 모델 loss: 3.4 → 2.8
= 수치로는 작아 보임
= 실제 사용하면 엄청난 차이 체감
```

---

### Scaling Extrapolation (하이퍼파라미터 전이)

#### 문제

**작은 모델**
```
하이퍼파라미터 튜닝:
- 여러 설정으로 학습
- 최고 성능 선택
- 비용: 감당 가능
```

**큰 모델**
```
한 번 학습도 비용 막대
→ 여러 번 학습 불가능
→ 한 번에 맞춰야 함!
```

#### 해결책

**Scaling Extrapolation**
> 작은 모델 실험 → 큰 모델에 적용할 하이퍼파라미터 예측

**접근법**
1. 작은 모델로 여러 하이퍼파라미터 테스트
2. 패턴 분석
3. 큰 모델에 외삽

**주의사항**
- 완벽하지 않음
- 여전히 리스크 존재
- 연구 활발히 진행 중

---

## 핵심 요약

### Model Architecture
1. **Transformer 지배**
   - 2017년 이후 9년째
   - Attention 메커니즘이 핵심
   - Seq2seq의 두 한계 해결: 정보 병목, 순차 처리

2. **Attention Mechanism**
   - Query, Key, Value 벡터
   - 모든 이전 토큰 참조 가능
   - Multi-headed로 다양한 패턴 학습

3. **Inference: Prefill + Decode**
   - Prefill: 병렬 (빠름)
   - Decode: 순차 (상대적으로 느림)

4. **대안 아키텍처**
   - RWKV: RNN + 병렬화
   - Mamba: 선형 시간, 긴 context
   - Jamba: Transformer + Mamba 하이브리드

### Model Size
1. **파라미터 수의 의미**
   - 학습 능력의 척도
   - 메모리 요구량 결정
   - 같은 크기라도 신세대 > 구세대

2. **Sparse Models (MoE)**
   - 많은 파라미터, 일부만 활성
   - Mixtral 8x7B: 46.7B지만 12.9B 비용

3. **세 가지 핵심 수치**
   - Parameters: 용량
   - Training Tokens: 학습량
   - FLOPs: 비용

4. **GPT-3 학습 비용**
   - 3.14 × 10²³ FLOPs
   - H100 256개로 236일
   - 약 $4.14M (70% 활용률 가정)

### Scaling Law
1. **Chinchilla Law**
   - 학습 토큰 ≈ 파라미터 × 20
   - 모델 2배 → 데이터도 2배

2. **실용적 선택**
   - 최적 성능 vs 배포 용이성
   - Llama: 약간 낮은 성능, 훨씬 넓은 adoption

3. **Last Mile Challenge**
   - 90% → 95%가 85% → 90%보다 10배 비쌈
   - 2% → 3% 오류율 개선에 10배 자원

4. **예측 가능성**
   - 주어진 예산으로 최적 모델 크기 계산 가능
   - Loss까지 예측 가능
   - AI 학습이 과학이 됨

---

## 토론 질문

### Architecture
1. Transformer는 언제까지 지배할까? Mamba가 실제로 대체할 수 있을까?
2. 어떤 use case에서 대안 아키텍처가 더 나을까?
3. Context length 제한은 실제로 얼마나 중요한가?

### Model Size
1. "더 크면 더 좋다"는 언제까지 유효할까?
2. MoE의 장점이 단점보다 클까?
3. 우리 팀은 어떤 크기의 모델을 사용해야 할까?

### Scaling Law
1. Chinchilla Law를 따르지 않는 Llama의 선택이 옳았나?
2. Last Mile Challenge를 극복할 방법이 있을까?
3. 추론 비용을 고려한 최적 모델 크기는?

### 실용적 고려사항
1. 학습 비용 vs 추론 비용, 어느 것을 더 최적화해야 할까?
2. 활용률 70%를 달성하기 위한 실전 팁은?
3. 작은 모델 + 좋은 데이터 vs 큰 모델 + 보통 데이터?

---

**이전**: [← 2.1 Training Data](./2.1-training-data.md)  
**다음**: [2.3 Post-Training →](./2.3-post-training.md)

[← 목차로 돌아가기](./README.md)
