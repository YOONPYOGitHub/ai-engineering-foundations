# 4.3 Design Your Evaluation Pipeline

## 📚 개요

평가 기준과 모델 선택 방법을 학습했으니, 이제 실제 **평가 파이프라인을 설계하고 운영**하는 방법을 다룹니다. 3단계 접근법으로 체계적인 평가 시스템을 구축합니다.

---

## 🏗️ 3단계 평가 파이프라인 설계

```
┌─────────────────────────────────────────────────────────────┐
│                    평가 파이프라인 설계                       │
├─────────────────────────────────────────────────────────────┤
│  Step 1: 평가할 구성요소 및 기준 파악                         │
│     ↓                                                        │
│  Step 2: 각 기준별 점수 기준(Rubric) 및 가이드라인 생성       │
│     ↓                                                        │
│  Step 3: 평가 방법 및 평가 데이터 구축                        │
└─────────────────────────────────────────────────────────────┘
```

---

## 1️⃣ Step 1: 구성요소 및 기준 파악

### 평가 대상 분해

복잡한 AI 시스템을 평가 가능한 구성요소로 분해합니다.

```
AI Application
├── Input Processing
│   └── Query understanding, Intent detection
├── Core Model
│   └── Response generation
├── Retrieval (RAG의 경우)
│   └── Document retrieval quality
├── Post-processing
│   └── Formatting, Safety filtering
└── End-to-end
    └── 전체 시스템 성능
```

### 평가 유형

| 유형 | 설명 | 예시 |
|------|------|------|
| **Component-level** | 개별 구성요소 평가 | Retriever의 recall@k |
| **Turn-based** | 단일 대화 턴 평가 | 응답 품질 |
| **Task-based** | 전체 태스크 완료 평가 | 예약 완료율 |
| **Session-based** | 전체 세션 평가 | 사용자 만족도 |

### 기준별 구성요소 매핑

```python
evaluation_matrix = {
    "component": {
        "retriever": ["relevance", "recall"],
        "generator": ["fluency", "faithfulness"],
        "safety_filter": ["precision", "recall"]
    },
    "criteria": {
        "factual_consistency": ["generator", "retriever"],
        "safety": ["generator", "safety_filter"],
        "latency": ["all_components"]
    }
}
```

---

## 2️⃣ Step 2: Scoring Rubric 생성

### Rubric이란?
평가 점수를 부여하기 위한 **명확하고 일관된 기준**입니다.

### Rubric 설계 원칙

1. **명확성**: 누가 평가해도 같은 결과가 나오도록
2. **구체성**: 추상적 표현 대신 구체적 기준
3. **포괄성**: 가능한 모든 경우 커버
4. **계층성**: 점수 간 명확한 구분

### Rubric 예시: 응답 품질 (5점 척도)

| 점수 | 기준 | 예시 |
|------|------|------|
| **5** | 완벽한 응답. 정확하고 완전하며 잘 구조화됨 | 모든 세부사항 포함, 명확한 설명 |
| **4** | 좋은 응답. 대부분 정확하나 사소한 개선 여지 | 핵심 정보 포함, 일부 세부사항 누락 |
| **3** | 적절한 응답. 기본적으로 정확하나 개선 필요 | 질문에 답하지만 불완전 |
| **2** | 미흡한 응답. 부분적으로만 정확하거나 불완전 | 주요 정보 누락 또는 오류 포함 |
| **1** | 부적절한 응답. 잘못되었거나 무관함 | 질문과 무관한 답변 |

### Binary vs Likert Scale

| 방식 | 장점 | 단점 | 적합한 경우 |
|------|------|------|------------|
| **Binary (0/1)** | 명확, 일관성 높음 | 미묘한 차이 포착 불가 | Pass/Fail 판단 |
| **Likert (1-5)** | 세분화된 평가 | 평가자 간 불일치 | 품질 순위화 |

### Annotation Guideline 작성

```markdown
## 평가 가이드라인: 사실 일관성

### 정의
응답이 제공된 컨텍스트의 정보와 일치하는 정도

### 평가 절차
1. 컨텍스트를 먼저 읽고 핵심 사실 파악
2. 응답의 각 주장이 컨텍스트에서 지원되는지 확인
3. 지원되지 않는 주장이 있으면 감점

### 주의사항
- 일반 상식은 컨텍스트 없이도 허용
- 추론은 논리적이면 허용
- 명확한 모순만 오류로 처리

### 예시
[컨텍스트]: "2024년 1분기 매출은 100억원이었다."
[응답]: "2024년 1분기 매출은 120억원을 달성했다."
→ 점수: 1 (명확한 사실 오류)
```

---

## 3️⃣ Step 3: 평가 방법 및 데이터 구축

### 평가 방법 선택

| 방법 | 비용 | 속도 | 신뢰도 | 적합한 경우 |
|------|------|------|--------|------------|
| **Exact Match** | 낮음 | 빠름 | 높음 | 정답이 명확할 때 |
| **Lexical Metrics** | 낮음 | 빠름 | 중간 | 참조 텍스트 존재 시 |
| **Embedding Similarity** | 중간 | 빠름 | 중간 | 의미 유사도 필요 시 |
| **AI Judge** | 높음 | 느림 | 중-높 | 복잡한 기준 평가 시 |
| **Human Evaluation** | 매우 높음 | 매우 느림 | 높음 | 최종 검증, 주관적 기준 |

### 평가 데이터 구축

#### 데이터 소스

```
평가 데이터 소스
├── Production Data (가장 현실적)
│   ├── 실제 사용자 쿼리
│   └── 자연 발생 라벨 (클릭, 피드백)
├── Synthetic Data (확장 용이)
│   ├── 템플릿 기반 생성
│   └── AI 생성 데이터
└── Curated Data (고품질)
    ├── 전문가 작성
    └── 기존 벤치마크 활용
```

#### 데이터 슬라이싱

데이터를 서브셋으로 분리하여 세분화된 분석을 수행합니다.

```python
slices = {
    "user_tier": ["free", "premium", "enterprise"],
    "query_length": ["short (<50 tokens)", "medium", "long (>200 tokens)"],
    "topic": ["technical", "general", "sensitive"],
    "source": ["mobile", "web", "api"],
    "language": ["en", "ko", "ja"]
}

# 각 슬라이스별 성능 분석
for slice_name, categories in slices.items():
    for category in categories:
        evaluate_on_slice(data, slice_name, category)
```

#### Simpson's Paradox 주의

| 그룹 | 모델 A | 모델 B |
|------|--------|--------|
| Group 1 | 93% (81/87) | 87% (234/270) |
| Group 2 | 73% (192/263) | 69% (55/80) |
| **Overall** | **78%** (273/350) | **83%** (289/350) |

> 모델 A가 각 그룹에서 더 좋지만, 전체 집계에서는 모델 B가 더 좋음!
> → 슬라이스별 분석이 필수인 이유

### 필요한 샘플 수 추정

#### OpenAI 가이드라인 (95% 신뢰도)

| 감지할 점수 차이 | 필요 샘플 수 |
|-----------------|-------------|
| 30% | ~10 |
| 10% | ~100 |
| 3% | ~1,000 |
| 1% | ~10,000 |

> **규칙**: 점수 차이가 3배 감소하면, 필요 샘플 수는 10배 증가

#### Bootstrap을 통한 신뢰도 검증

```python
import numpy as np

def bootstrap_evaluation(data, model, n_bootstraps=100):
    """평가 결과의 안정성을 부트스트랩으로 검증"""
    scores = []
    
    for _ in range(n_bootstraps):
        # 복원 추출로 샘플링
        sample = np.random.choice(data, size=len(data), replace=True)
        score = evaluate(model, sample)
        scores.append(score)
    
    return {
        "mean": np.mean(scores),
        "std": np.std(scores),
        "ci_95": (np.percentile(scores, 2.5), np.percentile(scores, 97.5))
    }

# 결과 해석
# std가 크면 → 평가 세트 크기 증가 필요
# ci_95 범위가 넓으면 → 결과 신뢰도 낮음
```

---

## 🔄 파이프라인 평가 및 반복

### 파이프라인 자체를 평가하기

| 질문 | 확인 방법 |
|------|----------|
| 올바른 신호를 주는가? | 좋은 응답 = 높은 점수인지 검증 |
| 신뢰할 수 있는가? | 동일 데이터에 반복 실행 시 결과 일관성 |
| 재현 가능한가? | Temperature=0 설정, 버전 고정 |
| 비용 효율적인가? | 평가당 비용 추적 |

### 지표 간 상관관계 분석

```python
# 상관관계가 높은 지표 = 중복
# 상관관계가 없는 지표 = 다른 측면 측정 또는 신뢰도 문제

correlations = {
    ("fluency", "coherence"): 0.85,  # 높음 → 하나만 사용 가능
    ("factual", "safety"): 0.15,     # 낮음 → 둘 다 필요
    ("metric_A", "metric_B"): -0.3,  # 음수 → 검토 필요
}
```

### 반복 개선 (Iteration)

```
┌────────────────────────────────────────────────────────┐
│                     반복 개선 사이클                     │
├────────────────────────────────────────────────────────┤
│  1. 평가 실행                                           │
│     ↓                                                   │
│  2. 결과 분석 (슬라이스별, 오류 패턴)                    │
│     ↓                                                   │
│  3. 문제점 식별                                         │
│     ↓                                                   │
│  4. 개선 방안 도출                                      │
│     ├─ 모델/프롬프트 개선                               │
│     ├─ 평가 기준 조정                                   │
│     └─ 평가 데이터 보강                                 │
│     ↓                                                   │
│  5. 재평가                                              │
└────────────────────────────────────────────────────────┘
```

### 실험 추적 (Experiment Tracking)

```python
experiment_log = {
    "experiment_id": "exp-2024-0115-001",
    "timestamp": "2024-01-15T10:30:00Z",
    "evaluation_data": {
        "version": "v3.2",
        "size": 1000,
        "slices": ["premium", "mobile"]
    },
    "rubric_version": "v2.1",
    "judge_config": {
        "model": "gpt-4-turbo",
        "temperature": 0,
        "prompt_version": "judge-v5"
    },
    "results": {
        "overall": 0.82,
        "by_slice": {"premium": 0.87, "mobile": 0.78}
    }
}
```

---

## 👥 User Feedback 통합

### 피드백 유형

| 유형 | 수집 방법 | 활용 |
|------|----------|------|
| **Explicit** | 👍/👎, 별점 | 직접적 품질 지표 |
| **Implicit** | 재시도, 이탈, 체류시간 | 간접적 만족도 |
| **Textual** | 코멘트, 불만 접수 | 질적 분석 |

### 피드백과 자동 평가의 상관관계

```
목표: 자동 평가 점수 ↔ 사용자 피드백 상관관계 분석

상관관계 높음 → 자동 평가 신뢰도 높음
상관관계 낮음 → 평가 기준 재검토 필요
```

---

## 💡 핵심 요약

1. **3단계 접근법**: 구성요소 파악 → Rubric 생성 → 평가 방법/데이터 구축
2. **슬라이싱 필수**: Simpson's Paradox 방지, 세분화된 인사이트
3. **샘플 수 중요**: 1% 차이 감지하려면 ~10,000개 필요
4. **파이프라인도 평가 대상**: 신뢰도, 비용, 상관관계 지속 모니터링
5. **반복 개선**: 평가는 일회성이 아닌 지속적 프로세스

---

## 📋 체크리스트: 평가 파이프라인 설계

- [ ] 평가할 구성요소를 모두 식별했는가?
- [ ] 각 기준에 대한 명확한 Rubric이 있는가?
- [ ] 평가 방법이 기준에 적합한가?
- [ ] 평가 데이터가 충분한가? (부트스트랩 검증)
- [ ] 데이터 슬라이싱을 수행하는가?
- [ ] 평가 결과의 재현성이 보장되는가?
- [ ] 실험 추적이 구현되어 있는가?
- [ ] 사용자 피드백과의 상관관계를 분석하는가?

---

## ❓ 토론 질문

1. 평가 파이프라인 자체의 신뢰도를 어떻게 정량화할 수 있을까요?

2. 평가 비용과 신뢰도 사이에서 최적의 균형점을 찾는 방법은?

3. AI Judge의 일관성을 높이기 위한 Rubric 설계 전략은?

4. 사용자 피드백이 자동 평가 점수와 상관관계가 낮을 때, 어느 쪽을 신뢰해야 할까요?
