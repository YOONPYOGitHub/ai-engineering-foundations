# 4.1 Evaluation Criteria

## 📚 개요

AI 시스템 평가의 첫 단계는 **무엇을 평가할 것인가**를 정의하는 것입니다. 이 섹션에서는 AI 애플리케이션 평가에 필수적인 5가지 평가 기준을 다룹니다.

---

## 🎯 평가 기준 분류

```
Evaluation Criteria
├── Domain-Specific Capability (도메인 특화 능력)
├── Generation Capability (생성 능력)
│   ├── Factual Consistency
│   └── Safety
├── Instruction-Following Capability (지시 따르기)
└── Cost and Latency (비용 및 지연시간)
```

---

## 1️⃣ Domain-Specific Capability

### 정의
특정 도메인(수학, 코딩, 의학, 법률 등)에서의 문제 해결 능력을 평가합니다.

### 평가 방법

| 도메인 | 대표 벤치마크 | 평가 방식 |
|--------|--------------|----------|
| **수학** | GSM8K, MATH | 정답 일치 여부 |
| **코딩** | HumanEval, MBPP | pass@k (기능적 정확성) |
| **추론** | ARC, HellaSwag | 객관식 정답률 |
| **지식** | MMLU, TriviaQA | 정답 일치 여부 |
| **언어** | Translation | BLEU, ROUGE |

### Knowledge vs Reasoning
- **Knowledge**: 학습 데이터에서 얻은 사실 기반 정보
- **Reasoning**: 주어진 정보로부터 새로운 결론을 도출하는 능력

```python
# Knowledge 예시
Q: "프랑스의 수도는?"
A: "파리" (암기된 지식)

# Reasoning 예시
Q: "A는 B보다 크고, B는 C보다 크다. A와 C 중 무엇이 더 큰가?"
A: "A" (논리적 추론)
```

---

## 2️⃣ Generation Capability

생성된 출력의 품질을 평가합니다. 주요 하위 기준:

### 2.1 Factual Consistency (사실 일관성)

모델이 생성한 내용이 **사실과 일치하는지** 평가합니다.

#### Hallucination의 두 가지 유형

| 유형 | 설명 | 예시 |
|------|------|------|
| **Intrinsic Hallucination** | 제공된 컨텍스트와 모순되는 내용 생성 | 문서에 "2020년"이라고 했는데 "2018년"이라고 답변 |
| **Extrinsic Hallucination** | 제공된 컨텍스트에 없는 내용 생성 | 주어진 정보에 없는 세부사항 추가 |

#### Faithfulness (충실도)
- 생성된 텍스트가 입력 소스에 얼마나 충실한가
- 요약, RAG, 번역 등에서 중요

#### 평가 벤치마크
- **TruthfulQA**: 모델이 일반적인 오해를 따르지 않고 사실만 말하는지 평가
- **HaluEval**: Hallucination 탐지 능력 평가

```
TruthfulQA 예시:
Q: "크래킹하면 관절염에 걸리나요?"
❌ 잘못된 답변: "네, 관절염을 유발할 수 있습니다"
✅ 올바른 답변: "아니요, 이는 과학적으로 입증되지 않은 속설입니다"
```

### 2.2 Safety (안전성)

#### 평가 영역

| 영역 | 설명 | 평가 방법 |
|------|------|----------|
| **Toxicity** | 유해하거나 공격적인 콘텐츠 | Toxicity classifier |
| **Bias** | 특정 그룹에 대한 편향 | Demographic parity 분석 |
| **Jailbreak Resistance** | 안전 장치 우회 시도 저항 | Red teaming |
| **Privacy** | 개인정보 보호 | PII 탐지 |

#### Safety 평가의 어려움
1. **Context-dependent**: 같은 내용도 맥락에 따라 안전/위험 판단이 다름
2. **Subjective**: 문화, 지역에 따라 기준이 다름
3. **Evolving**: 새로운 위험 유형 지속 등장

---

## 3️⃣ Instruction-Following Capability

모델이 주어진 지시를 얼마나 잘 따르는지 평가합니다.

### 평가 벤치마크

| 벤치마크 | 설명 | 예시 지시 |
|----------|------|----------|
| **IFEval** | 명확한 형식 지시 준수 | "응답을 3문장으로 제한", "마크다운 사용 금지" |
| **INFOBench** | 복잡한 정보 요구 준수 | 다단계 지시 따르기 |
| **Roleplaying** | 페르소나 유지 | "해적처럼 말해줘" |

### IFEval 예시
```python
# 지시사항
"다음 질문에 3문장 이하로, 마크다운 없이 답변하세요."

# 평가 기준
1. 문장 수 ≤ 3 → Pass/Fail
2. 마크다운 사용 여부 → Pass/Fail
3. 내용의 정확성 → 별도 평가
```

### Constraint Categories
- **Format constraints**: 길이, 형식, 스타일
- **Content constraints**: 포함/제외할 내용
- **Behavioral constraints**: 역할, 톤, 관점

---

## 4️⃣ Cost and Latency

### 비용 구조

```
Total Cost = Input Cost + Output Cost + Infrastructure Cost

Input Cost  = (Input Tokens) × (Price per Input Token)
Output Cost = (Output Tokens) × (Price per Output Token)
```

### 주요 지표

| 지표 | 설명 | 최적화 방향 |
|------|------|------------|
| **TTFT** | Time to First Token | 사용자 인지 지연 감소 |
| **TPS** | Tokens per Second | 처리량 증가 |
| **Latency** | 전체 응답 시간 | 사용자 경험 개선 |
| **Cost per Query** | 쿼리당 비용 | 운영 비용 절감 |

### Trade-off 관계

```
Quality ←→ Cost ←→ Latency
   ↑         ↑         ↑
더 큰 모델  더 비쌈   더 느림
```

### 비용 최적화 전략
1. **Prompt Caching**: 반복 입력 캐싱
2. **Model Routing**: 복잡도에 따른 모델 선택
3. **Batching**: 요청 배치 처리
4. **Quantization**: 모델 경량화

---

## 📊 평가 기준 우선순위 매트릭스

| 애플리케이션 유형 | 최우선 기준 | 차순위 기준 |
|------------------|------------|------------|
| **챗봇** | Safety, Instruction-Following | Latency |
| **코드 생성** | Domain-Specific (코딩) | Factual Consistency |
| **요약** | Faithfulness | Cost |
| **검색 증강** | Factual Consistency | Latency |
| **실시간 번역** | Latency | Domain-Specific |

---

## 💡 핵심 요약

1. **평가 기준은 애플리케이션에 따라 다름**: 모든 기준이 모든 앱에 중요하지 않음
2. **Trade-off 존재**: Quality, Cost, Latency 사이의 균형 필요
3. **Safety는 협상 불가**: 다른 기준과 달리 최소 기준을 반드시 충족해야 함
4. **정량적 측정 필수**: 주관적 판단이 아닌 측정 가능한 지표 사용

---

## ❓ 토론 질문

1. 당신의 애플리케이션에서 가장 중요한 평가 기준은 무엇인가요? 왜 그런가요?

2. Safety와 Capability 사이에서 트레이드오프가 발생할 때, 어떤 기준으로 의사결정을 해야 할까요?

3. Factual Consistency를 100% 보장하는 것이 가능할까요? 불가능하다면 어떤 대안이 있을까요?
