# 4.2 Model Selection

## 📚 개요

수많은 Foundation Model 중에서 애플리케이션에 적합한 모델을 선택하는 방법을 다룹니다. Build vs Buy 의사결정과 Public Benchmark 활용법을 학습합니다.

---

## 🔄 Model Selection Workflow

### 단계별 접근법

```
1. 요구사항 정의
      ↓
2. 후보 모델 리스트업
      ↓
3. Public Benchmark로 초기 필터링
      ↓
4. 자체 평가 데이터로 테스트
      ↓
5. 비용/지연시간 분석
      ↓
6. 최종 선택 및 모니터링
```

### 핵심 원칙

> **"Public benchmarks can help you weed out bad models, but they won't help you find the best models for your applications."**

---

## 🏗️ Model Build vs Buy

### 7가지 의사결정 축

| 축 | Build (Self-host) | Buy (API) |
|----|-------------------|-----------|
| **Data Privacy** | 데이터가 외부로 나가지 않음 | 제3자에게 데이터 전송 |
| **Data Lineage** | 학습 데이터 추적 가능 | 불투명 |
| **Performance** | 커스터마이징으로 최적화 가능 | 즉시 사용 가능한 성능 |
| **Functionality** | 완전한 커스터마이징 | 제공 기능에 제한 |
| **Control** | 모델/인프라 완전 통제 | 벤더 종속성 |
| **Cost** | 높은 초기 투자, 장기 비용 절감 가능 | 낮은 시작 비용, 사용량 기반 과금 |
| **Speed to Market** | 긴 개발 시간 | 빠른 출시 |

### 1️⃣ Data Privacy

#### API 사용 시 우려사항
- 입력 데이터가 외부 서버로 전송
- 학습 데이터로 사용될 가능성
- 규제 준수 문제 (GDPR, HIPAA 등)

```
규제 산업에서의 고려사항:
┌──────────────┬─────────────────────────────────────┐
│ 금융 (FINRA) │ 고객 데이터 외부 전송 제한         │
│ 의료 (HIPAA) │ PHI 보호 요구                       │
│ 법률         │ Attorney-client privilege 유지     │
└──────────────┴─────────────────────────────────────┘
```

#### 대응 방안
- On-premise 모델 배포
- Private cloud 환경
- Zero data retention 계약

### 2️⃣ Data Lineage

#### 중요성
- 학습 데이터 출처 파악
- 저작권/라이선스 문제 확인
- 편향 원인 추적

```python
# 데이터 계보 추적의 이점
data_lineage = {
    "source": "위키피디아, 뉴스, 책",
    "license": "CC-BY, Public Domain",
    "bias_check": "Demographic 분석 완료",
    "version": "v2.3",
    "date": "2024-01"
}
```

### 3️⃣ Performance

#### Build의 장점
- 도메인 특화 파인튜닝
- Prompt 최적화 자유도
- 지속적 개선 가능

#### Buy의 장점
- 즉시 SOTA 성능 사용
- 자동 업데이트
- 전문 팀의 최적화 혜택

### 4️⃣ Functionality

| 기능 | Build | Buy |
|------|-------|-----|
| 커스텀 토크나이저 | ✅ | ❌ |
| 아키텍처 수정 | ✅ | ❌ |
| 출력 형식 제어 | ✅ | ⚠️ |
| 새로운 기능 추가 | ✅ | 벤더 의존 |

### 5️⃣ Control

#### Build 시 통제권
- 모델 버전 관리
- 서비스 가용성
- 업데이트 시점 결정

#### Buy 시 리스크
- API 사용 중단
- 가격 변경
- 모델 변경 (성능 변화)
- Rate limiting

```
⚠️ Vendor Lock-in 리스크:
- OpenAI API → Azure OpenAI 마이그레이션 필요
- 특정 API 기능에 의존 → 대안 부재
- 프롬프트 최적화 → 다른 모델에서 재작업 필요
```

### 6️⃣ Cost Analysis

#### Build 비용 구조
```
초기 비용:
├── GPU 하드웨어 또는 클라우드 예약
├── 엔지니어링 인력
├── 파인튜닝 비용
└── 인프라 구축

운영 비용:
├── 컴퓨팅 (GPU 시간)
├── 스토리지
├── 네트워크
└── 모니터링/유지보수
```

#### Buy 비용 구조
```
운영 비용:
├── Input tokens × 단가
├── Output tokens × 단가
└── 추가 기능 요금 (embedding, fine-tuning 등)
```

#### 손익분기점 분석
```python
# 간단한 비용 비교 예시
def compare_costs(monthly_queries, avg_tokens_per_query):
    # API 비용 (예: GPT-4)
    api_cost_per_token = 0.00003  # $30/1M tokens
    api_monthly = monthly_queries * avg_tokens_per_query * api_cost_per_token
    
    # Self-host 비용 (예: A100 GPU)
    gpu_monthly = 2000  # GPU 인스턴스 월 비용
    setup_amortized = 500  # 초기 설정 비용 분할
    self_host_monthly = gpu_monthly + setup_amortized
    
    return {
        "api": api_monthly,
        "self_host": self_host_monthly,
        "breakeven_queries": self_host_monthly / (avg_tokens_per_query * api_cost_per_token)
    }
```

### 7️⃣ Speed to Market

| 시나리오 | 권장 접근법 |
|----------|------------|
| MVP/프로토타입 | Buy (API) |
| 빠른 검증 필요 | Buy (API) |
| 장기 운영 계획 | Build 검토 |
| 규제 산업 | Build 우선 고려 |

---

## 📊 Navigate Public Benchmarks

### Benchmark 선택 기준

1. **관련성**: 애플리케이션 도메인과 일치하는가?
2. **최신성**: 최근에 만들어졌는가?
3. **신뢰성**: 평가 방법이 검증되었는가?
4. **오염도**: Data Contamination 우려는?

### 주요 Benchmark 카테고리

| 카테고리 | 벤치마크 | 측정 대상 |
|----------|----------|-----------|
| **General Knowledge** | MMLU | 다분야 지식 |
| **Reasoning** | ARC, HellaSwag | 추론 능력 |
| **Coding** | HumanEval, MBPP | 코드 생성 |
| **Math** | GSM8K, MATH | 수학적 추론 |
| **Safety** | TruthfulQA | 진실성 |
| **Instruction** | IFEval | 지시 따르기 |

### Benchmark Aggregation

#### 리더보드의 집계 방식

```
HELM 예시:
┌─────────────────────────────────────────────┐
│ 1. 각 벤치마크별 점수 계산                   │
│ 2. 정규화 (0-1 스케일)                       │
│ 3. 가중 평균 또는 단순 평균                  │
│ 4. 최종 순위 산출                            │
└─────────────────────────────────────────────┘
```

#### 문제점
- 벤치마크 선택 기준 불투명
- 가중치 설정의 주관성
- 특정 능력에 과도한 비중

### Correlation Analysis

> **"If two benchmarks are highly correlated, you might not need both."**

```python
# 벤치마크 간 상관관계 분석
import numpy as np

# 예시: MMLU와 ARC 점수
mmlu_scores = [0.85, 0.72, 0.91, 0.68, 0.79]
arc_scores = [0.82, 0.75, 0.88, 0.71, 0.77]

correlation = np.corrcoef(mmlu_scores, arc_scores)[0, 1]
# correlation ≈ 0.95 → 높은 상관관계 → 하나만 사용 가능
```

---

## ⚠️ Data Contamination

### 정의
벤치마크 데이터가 모델의 학습 데이터에 포함되어 평가 결과가 부풀려지는 현상

### 발생 원인
```
Training Data Sources
├── Web scraping (Common Crawl)
├── GitHub repositories
├── Books & papers
└── Wikipedia
     ↓
벤치마크 데이터도 같은 소스에서 유래
     ↓
벤치마크 ⊆ Training Data
```

### 탐지 방법

| 방법 | 설명 |
|------|------|
| **Canary strings** | 학습 데이터에 고유 문자열 삽입 후 모델이 기억하는지 테스트 |
| **Perplexity analysis** | 벤치마크 데이터에 대해 비정상적으로 낮은 perplexity 확인 |
| **N-gram overlap** | 학습 데이터와 벤치마크의 n-gram 중복 측정 |
| **Holdout sets** | 절대 공개하지 않은 평가 데이터 사용 |

### 대응 방안

1. **Private Evaluation Sets**: 자체 비공개 평가 데이터 구축
2. **Dynamic Benchmarks**: 주기적으로 업데이트되는 벤치마크 사용
3. **Multiple Benchmarks**: 단일 벤치마크에 의존하지 않음
4. **Production Data**: 실제 사용자 데이터로 평가

---

## 🏆 Public Leaderboards

### 주요 리더보드

| 리더보드 | 특징 | 주의점 |
|----------|------|--------|
| **Chatbot Arena** | 실제 사용자 투표 기반 Elo | 주관적 선호 반영 |
| **Open LLM Leaderboard** | 오픈소스 모델 중심 | 자동 평가만 |
| **HELM** | 종합적 평가 | 업데이트 주기 |
| **AlpacaEval** | 대화 능력 평가 | GPT-4 판정 의존 |

### 리더보드 활용 전략

```
1단계: 리더보드로 후보 모델 10개 선정
        ↓
2단계: 관련 벤치마크 점수 비교
        ↓
3단계: 상위 3-5개 모델 심층 평가
        ↓
4단계: 자체 데이터로 최종 검증
```

---

## 💡 핵심 요약

1. **Build vs Buy는 상황에 따라**: 정답이 없으며 7가지 축으로 분석 필요
2. **Public Benchmark는 시작점일 뿐**: 나쁜 모델 필터링에 유용, 최선 선택에는 불충분
3. **Data Contamination 주의**: 벤치마크 점수를 맹신하지 말 것
4. **자체 평가 데이터 필수**: 애플리케이션 특화 평가 세트 구축 필요

---

## ❓ 토론 질문

1. 스타트업과 대기업에서 Build vs Buy 결정이 어떻게 달라질까요?

2. Data Contamination 문제가 심각해지면 Public Benchmark의 미래는 어떻게 될까요?

3. 여러 벤치마크 점수를 하나의 숫자로 집계하는 것의 장단점은 무엇인가요?
